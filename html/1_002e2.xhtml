<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<!-- Created by GNU Texinfo 7.1, https://www.gnu.org/software/texinfo/ -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>1.2 (Structure and Interpretation of Computer Programs, 2e)</title>

<meta name="description" content="1.2 (Structure and Interpretation of Computer Programs, 2e)" />
<meta name="keywords" content="1.2 (Structure and Interpretation of Computer Programs, 2e)" />
<meta name="resource-type" content="document" />
<meta name="distribution" content="global" />
<meta name="Generator" content="texi2any" />
<meta name="viewport" content="width=device-width,initial-scale=1" />

<link href="index.xhtml" rel="start" title="Top" />
<link href="Term-Index.xhtml" rel="index" title="Term Index" />
<link href="index.xhtml#SEC_Contents" rel="contents" title="Table of Contents" />
<link href="Chapter-1.xhtml" rel="up" title="Chapter 1" />
<link href="1_002e3.xhtml#g_t1_002e3" rel="next" title="1.3" />
<link href="1_002e1.xhtml#g_t1_002e1" rel="prev" title="1.1" />
<style type="text/css">
<!--
a.copiable-link {visibility: hidden; text-decoration: none; line-height: 0em}
div.center {text-align:center}
div.example {margin-left: 3.2em}
span:hover a.copiable-link {visibility: visible}
ul.mark-bullet {list-style-type: disc}
-->
</style>

<script src="js/jquery.min.js" type="text/javascript"></script>
<script src="js/footnotes.js" type="text/javascript"></script>
<script src="js/browsertest.js" type="text/javascript"></script>
</head>

<body lang="en">
<div class="section-level-extent" id="g_t1_002e2">
<div class="nav-panel">
<p>
Next: <a href="1_002e3.xhtml#g_t1_002e3" accesskey="n" rel="next">Formulating Abstractions with Higher-Order Procedures</a>, Previous: <a href="1_002e1.xhtml#g_t1_002e1" accesskey="p" rel="prev">The Elements of Programming</a>, Up: <a href="Chapter-1.xhtml" accesskey="u" rel="up">Building Abstractions with Procedures</a> &#160; [<a href="index.xhtml#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="Term-Index.xhtml" title="Index" rel="index">Index</a>]</p>
</div>
<h3 class="section" id="Procedures-and-the-Processes-They-Generate"><span>1.2 Procedures and the Processes They Generate<a class="copiable-link" href="#Procedures-and-the-Processes-They-Generate"> &#182;</a></span></h3>

<p>We have now considered the elements of programming: We have used primitive
arithmetic operations, we have combined these operations, and we have
abstracted these composite operations by defining them as compound procedures.
But that is not enough to enable us to say that we know how to program.  Our
situation is analogous to that of someone who has learned the rules for how the
pieces move in chess but knows nothing of typical openings, tactics, or
strategy.  Like the novice chess player, we don&#8217;t yet know the common patterns
of usage in the domain.  We lack the knowledge of which moves are worth making
(which procedures are worth defining).  We lack the experience to predict the
consequences of making a move (executing a procedure).
</p>
<p>The ability to visualize the consequences of the actions under consideration is
crucial to becoming an expert programmer, just as it is in any synthetic,
creative activity.  In becoming an expert photographer, for example, one must
learn how to look at a scene and know how dark each region will appear on a
print for each possible choice of exposure and development conditions.  Only
then can one reason backward, planning framing, lighting, exposure, and
development to obtain the desired effects.  So it is with programming, where we
are planning the course of action to be taken by a process and where we control
the process by means of a program.  To become experts, we must learn to
visualize the processes generated by various types of procedures.  Only after
we have developed such a skill can we learn to reliably construct programs that
exhibit the desired behavior.
</p>
<p>A procedure is a pattern for the <a class="index-entry-id" id="index-local-evolution"></a>
<em class="dfn">local evolution</em> of a computational
process.  It specifies how each stage of the process is built upon the previous
stage.  We would like to be able to make statements about the overall, or
<a class="index-entry-id" id="index-global"></a>
<em class="dfn">global</em>, behavior of a process whose local evolution has been
specified by a procedure.  This is very difficult to do in general, but we can
at least try to describe some typical patterns of process evolution.
</p>
<p>In this section we will examine some common &#8220;shapes&#8221; for processes generated
by simple procedures.  We will also investigate the rates at which these
processes consume the important computational resources of time and space.  The
procedures we will consider are very simple.  Their role is like that played by
test patterns in photography: as oversimplified prototypical patterns, rather
than practical examples in their own right.
</p>

<hr />
<div class="subsection-level-extent" id="g_t1_002e2_002e1">
<h4 class="subsection" id="Linear-Recursion-and-Iteration"><span>1.2.1 Linear Recursion and Iteration<a class="copiable-link" href="#Linear-Recursion-and-Iteration"> &#182;</a></span></h4>

<p>We begin by considering the factorial function, defined by

\[ % :5:
 
n! \,=\, {n \cdot (n - 1)} \cdot {(n - 2)} \cdots {3 \cdot 2 \cdot 1.}
\]

There are many ways to compute factorials.  One way is to make use of the
observation that <em class="math">{n!}</em> is equal to <em class="math">n</em> times <em class="math">{(n - 1)!}</em> for any positive
integer <em class="math">n</em>:

\[ % :6:
 
n! \,=\, {n \cdot [(n - 1)} \cdot {(n - 2)} \cdots {3 \cdot 2 \cdot 1]} \,=\, {n \cdot (n - 1)!.}
\]

Thus, we can compute <em class="math">{n!}</em> by computing <em class="math">{(n - 1)!}</em> and multiplying the
result by <em class="math">n</em>.  If we add the stipulation that 1! is equal to 1, this
observation translates directly into a procedure:
</p>
<div class="example">
<pre class="example-preformatted">fn factorial(n: u64) -&gt; u64 {
    if n == 1 {
        1
    } else {
        n * factorial(n - 1)
    }
}
</pre></div>

<p>We can use the substitution model of <a class="ref" href="1_002e1.xhtml#g_t1_002e1_002e5">The Substitution Model for Procedure Application</a> to watch this
procedure in action computing 6!, as shown in <a class="ref" href="#Figure-1_002e3">Figure 1.3</a>.
</p>
<div class="float">
<a class="anchor" id="Figure-1_002e3"></a><img class="image" src="fig/chap1/Fig1.3d.std.svg" alt="fig/chap1/Fig1.3d" />
<div class="caption"><p><strong class="strong">Figure 1.3:</strong> A linear recursive process for computing 6!.</p></div></div>
<p>Now let&#8217;s take a different perspective on computing factorials.  We could
describe a rule for computing <em class="math">{n!}</em> by specifying that we first multiply 1 by
2, then multiply the result by 3, then by 4, and so on until we reach <em class="math">n</em>.
More formally, we maintain a running product, together with a counter that
counts from 1 up to <em class="math">n</em>.  We can describe the computation by saying that the
counter and the product simultaneously change from one step to the next
according to the rule
</p>
<div class="example">
<pre class="example-preformatted">product <em class="math">\gets</em> counter * product
counter <em class="math">\gets</em> counter + 1
</pre></div>

<p>and stipulating that <em class="math">{n!}</em> is the value of the product when the counter
exceeds <em class="math">n</em>.
</p>
<p>Once again, we can recast our description as a procedure for computing
factorials:<a class="footnote" id="DOCF24" href="#FOOT24"><sup>24</sup></a>
</p>
<div class="example">
<pre class="example-preformatted">fn factorial(n: u64) -&gt; u64 {
    fact_iter(1, 1, n)
}

fn fact_iter(product: u64, counter: u64, max_count: u64) -&gt; u64 {
    if counter &gt; max_count {
        product
    } else {
        fact_iter(counter * product, counter + 1, max_count)
    }
}
</pre></div>

<p>As before, we can use the substitution model to visualize the process of
computing 6!, as shown in <a class="ref" href="#Figure-1_002e4">Figure 1.4</a>.
</p>
<div class="float">
<a class="anchor" id="Figure-1_002e4"></a><img class="image" src="fig/chap1/Fig1.4d.std.svg" alt="fig/chap1/Fig1.4d" />
<div class="caption"><p><strong class="strong">Figure 1.4:</strong> A linear iterative process for computing 6!.</p></div></div>
<p>Compare the two processes.  From one point of view, they seem hardly different
at all.  Both compute the same mathematical function on the same domain, and
each requires a number of steps proportional to <em class="math">n</em> to compute <em class="math">{n!}</em>.
Indeed, both processes even carry out the same sequence of multiplications,
obtaining the same sequence of partial products.  On the other hand, when we
consider the &#8220;shapes&#8221; of the two processes, we find that they evolve quite
differently.
</p>
<p>Consider the first process.  The substitution model reveals a shape of
expansion followed by contraction, indicated by the arrow in <a class="ref" href="#Figure-1_002e3">Figure 1.3</a>.
The expansion occurs as the process builds up a chain of <a class="index-entry-id" id="index-deferred-operations"></a>
<em class="dfn">deferred operations</em> 
(in this case, a chain of multiplications).  The contraction occurs
as the operations are actually performed.  This type of process, characterized
by a chain of deferred operations, is called a <a class="index-entry-id" id="index-recursive-process"></a>
<em class="dfn">recursive process</em>.
Carrying out this process requires that the interpreter keep track of the
operations to be performed later on.  In the computation of <em class="math">{n!}</em>, the length
of the chain of deferred multiplications, and hence the amount of information
needed to keep track of it, grows linearly with <em class="math">n</em> (is proportional to
<em class="math">n</em>), just like the number of steps.  Such a process is called a
<a class="index-entry-id" id="index-linear-recursive-process"></a>
<em class="dfn">linear recursive process</em>.
</p>
<p>By contrast, the second process does not grow and shrink.  At each step, all we
need to keep track of, for any <em class="math">n</em>, are the current values of the variables
<code class="code">product</code>, <code class="code">counter</code>, and <code class="code">max_count</code>.  We call this an
<a class="index-entry-id" id="index-iterative-process"></a>
<em class="dfn">iterative process</em>.  In general, an iterative process is one whose
state can be summarized by a fixed number of <a class="index-entry-id" id="index-state-variables"></a>
<em class="dfn">state variables</em>,
together with a fixed rule that describes how the state variables should be
updated as the process moves from state to state and an (optional) end test
that specifies conditions under which the process should terminate.  In
computing <em class="math">{n!}</em>, the number of steps required grows linearly with <em class="math">n</em>.  Such
a process is called a <a class="index-entry-id" id="index-linear-iterative-process"></a>
<em class="dfn">linear iterative process</em>.
</p>
<p>The contrast between the two processes can be seen in another way.  In the
iterative case, the program variables provide a complete description of the
state of the process at any point.  If we stopped the computation between
steps, all we would need to do to resume the computation is to supply the
interpreter with the values of the three program variables.  Not so with the
recursive process.  In this case there is some additional &#8220;hidden&#8221;
information, maintained by the interpreter and not contained in the program
variables, which indicates &#8220;where the process is&#8221; in negotiating the chain of
deferred operations.  The longer the chain, the more information must be
maintained.<a class="footnote" id="DOCF25" href="#FOOT25"><sup>25</sup></a>
</p>
<p>In contrasting iteration and recursion, we must be careful not to confuse the
notion of a recursive <a class="index-entry-id" id="index-process"></a>
<em class="dfn">process</em> with the notion of a recursive
<a class="index-entry-id" id="index-procedure"></a>
<em class="dfn">procedure</em>.  When we describe a procedure as recursive, we are
referring to the syntactic fact that the procedure definition refers (either
directly or indirectly) to the procedure itself.  But when we describe a
process as following a pattern that is, say, linearly recursive, we are
speaking about how the process evolves, not about the syntax of how a procedure
is written.  It may seem disturbing that we refer to a recursive procedure such
as <code class="code">fact_iter</code> as generating an iterative process.  However, the process
really is iterative: Its state is captured completely by its three state
variables, and an interpreter need keep track of only three variables in order
to execute the process.
</p>
<p>One reason that the distinction between process and procedure may be confusing
is that most implementations of common languages (including Ada, Pascal, C, and
Rust) are designed in such a way that the interpretation of any recursive
procedure consumes an amount of memory that grows with the number of procedure
calls, even when the process described is, in principle, iterative.  As a
consequence, these languages can describe iterative processes only by resorting
to special-purpose &#8220;looping constructs&#8221; such as <code class="code">do</code>, <code class="code">repeat</code>,
<code class="code">until</code>, <code class="code">for</code>, and <code class="code">while</code>.  In Rust, while the LLVM compiler
backend may optimize tail calls in some cases, tail-call optimization is not
guaranteed by the language specification.  Therefore, for truly constant-space
iterative processes, Rust programmers typically use <code class="code">loop</code>, <code class="code">while</code>,
or <code class="code">for</code> constructs rather than relying on recursive function calls.  The
implementation of Scheme we shall consider in <a class="ref" href="Chapter-5.xhtml">Computing with Register Machines</a> does not share
this defect.  It will execute an iterative process in constant space, even if
the iterative process is described by a recursive procedure.  An implementation
with this property is called <a class="index-entry-id" id="index-tail_002drecursive"></a>
<em class="dfn">tail-recursive</em>.  With a tail-recursive
implementation, iteration can be expressed using the ordinary procedure call
mechanism, so that special iteration constructs are useful only as syntactic
sugar.<a class="footnote" id="DOCF26" href="#FOOT26"><sup>26</sup></a>
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e9"></a>Exercise 1.9:</strong> Each of the following two
procedures defines a method for adding two positive integers in terms of the
procedures <code class="code">inc</code>, which increments its argument by 1, and <code class="code">dec</code>,
which decrements its argument by 1.
</p>
<div class="example">
<pre class="example-preformatted">fn add(a: u64, b: u64) -&gt; u64 {
    if a == 0 {
        b
    } else {
        inc(add(dec(a), b))
    }
}

fn add(a: u64, b: u64) -&gt; u64 {
    if a == 0 {
        b
    } else {
        add(dec(a), inc(b))
    }
}
</pre></div>

<p>Using the substitution model, illustrate the process generated by each
procedure in evaluating <code class="code">add(4, 5)</code>.  Are these processes iterative or
recursive?
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e10"></a>Exercise 1.10:</strong> The following procedure computes
a mathematical function called Ackermann&#8217;s function.
</p>
<div class="example">
<pre class="example-preformatted">fn a(x: u64, y: u64) -&gt; u64 {
    if y == 0 {
        0
    } else if x == 0 {
        2 * y
    } else if y == 1 {
        2
    } else {
        a(x - 1, a(x, y - 1))
    }
}
</pre></div>

<p>What are the values of the following expressions?
</p>
<div class="example">
<pre class="example-preformatted">a(1, 10)
a(2, 4)
a(3, 3)
</pre></div>

<p>Consider the following procedures, where <code class="code">a</code> is the procedure
defined above:
</p>
<div class="example">
<pre class="example-preformatted">fn f(n: u64) -&gt; u64 { a(0, n) }
fn g(n: u64) -&gt; u64 { a(1, n) }
fn h(n: u64) -&gt; u64 { a(2, n) }
fn k(n: u64) -&gt; u64 { 5 * n * n }
</pre></div>

<p>Give concise mathematical definitions for the functions computed by the
procedures <code class="code">f</code>, <code class="code">g</code>, and <code class="code">h</code> for positive integer values of
<em class="math">n</em>.  For example, <code class="code">k(n)</code> computes <em class="math">{5n^2}</em>.
</p></blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t1_002e2_002e2">
<h4 class="subsection" id="Tree-Recursion"><span>1.2.2 Tree Recursion<a class="copiable-link" href="#Tree-Recursion"> &#182;</a></span></h4>

<p>Another common pattern of computation is called <a class="index-entry-id" id="index-tree-recursion"></a>
<em class="dfn">tree recursion</em>.  As
an example, consider computing the sequence of Fibonacci numbers, in which each
number is the sum of the preceding two:
</p>
<div class="center">0, 1, 1, 2, 3, 5, 8, 13, 21, &#8230;.
</div>
<p>In general, the Fibonacci numbers can be defined by the rule

\[ % :7:
  
\text{Fib}(n) \; = \;
  \left\{ 
    \begin{array}{ll}	 	 
      0 &amp; \;\text{if} \;\; n = 0, \\
      1 &amp; \;\text{if} \;\; n = 1, \\
      \text{Fib}(n-1) + \text{Fib}(n-2) &amp; \;\text{otherwise}. 
    \end{array} 
  \right. 
\]

We can immediately translate this definition into a recursive procedure for
computing Fibonacci numbers:
</p>
<div class="example">
<pre class="example-preformatted">fn fib(n: u64) -&gt; u64 {
    if n == 0 {
        0
    } else if n == 1 {
        1
    } else {
        fib(n - 1) + fib(n - 2)
    }
}
</pre></div>

<p>Consider the pattern of this computation.  To compute <code class="code">fib(5)</code>, we
compute <code class="code">fib(4)</code> and <code class="code">fib(3)</code>.  To compute <code class="code">fib(4)</code>, we
compute <code class="code">fib(3)</code> and <code class="code">fib(2)</code>.  In general, the evolved process
looks like a tree, as shown in <a class="ref" href="#Figure-1_002e5">Figure 1.5</a>.  Notice that the branches
split into two at each level (except at the bottom); this reflects the fact
that the <code class="code">fib</code> procedure calls itself twice each time it is invoked.
</p>
<div class="float">
<a class="anchor" id="Figure-1_002e5"></a><img class="image" src="fig/chap1/Fig1.5d.std.svg" alt="fig/chap1/Fig1.5d" />
<div class="caption"><p><strong class="strong">Figure 1.5:</strong> The tree-recursive process generated in computing <code class="code">fib(5)</code>.</p></div></div>
<p>This procedure is instructive as a prototypical tree recursion, but it is a
terrible way to compute Fibonacci numbers because it does so much redundant
computation.  Notice in <a class="ref" href="#Figure-1_002e5">Figure 1.5</a> that the entire computation of
<code class="code">fib(3)</code>&#8212;almost half the work&#8212;is duplicated.  In fact, it is not hard
to show that the number of times the procedure will compute <code class="code">fib(1)</code> or
<code class="code">fib(0)</code> (the number of leaves in the above tree, in general) is
precisely <em class="math">{\text{Fib}(n+1)}</em>.  To get an idea of how bad this is, one can
show that the value of <em class="math">{\text{Fib}(n)}</em> grows exponentially with <em class="math">n</em>.  More
precisely (see <a class="ref" href="#Exercise-1_002e13">Exercise 1.13</a>), <em class="math">{\text{Fib}(n)}</em> is the closest integer
to <em class="math">{\varphi^n / \sqrt{5}}</em>, where

\[ % :8:

\varphi \,=\, \frac{1 + \sqrt{5}}{2} \,\approx\, 1.6180
\]

is the <a class="index-entry-id" id="index-golden-ratio"></a>
<em class="dfn">golden ratio</em>, which satisfies the equation

\[ % :9:
 
\varphi^2 \,=\, {\varphi + 1.}
\]

Thus, the process uses a number of steps that grows exponentially with the
input.  On the other hand, the space required grows only linearly with the
input, because we need keep track only of which nodes are above us in the tree
at any point in the computation.  In general, the number of steps required by a
tree-recursive process will be proportional to the number of nodes in the tree,
while the space required will be proportional to the maximum depth of the tree.
</p>
<p>We can also formulate an iterative process for computing the Fibonacci numbers.
The idea is to use a pair of integers <em class="math">a</em> and <em class="math">b</em>, initialized to
<em class="math">{\text{Fib(1) = 1}}</em> and <em class="math">{\text{Fib(0) = 0}}</em>, and to repeatedly apply the
simultaneous transformations
</p>

\[ % :10:
 
\begin{array}{l}
  a \;\leftarrow\; a + b, \\ 
  b \;\leftarrow\; a. 
\end{array}
\]

<p>It is not hard to show that, after applying this transformation <em class="math">n</em> times,
<em class="math">a</em> and <em class="math">b</em> will be equal, respectively, to <em class="math">{\text{Fib}(n+1)}</em> and
<em class="math">{\text{Fib}(n)}</em>.  Thus, we can compute Fibonacci numbers iteratively using
the procedure
</p>
<div class="example">
<pre class="example-preformatted">fn fib(n: u64) -&gt; u64 {
    fib_iter(1, 0, n)
}

fn fib_iter(a: u64, b: u64, count: u64) -&gt; u64 {
    if count == 0 {
        b
    } else {
        fib_iter(a + b, a, count - 1)
    }
}
</pre></div>

<p>This second method for computing <em class="math">{\text{Fib}(n)}</em> is a linear iteration.  The
difference in number of steps required by the two methods&#8212;one linear in
<em class="math">n</em>, one growing as fast as <em class="math">{\text{Fib}(n)}</em> itself&#8212;is enormous, even for
small inputs.
</p>
<p>One should not conclude from this that tree-recursive processes are useless.
When we consider processes that operate on hierarchically structured data
rather than numbers, we will find that tree recursion is a natural and powerful
tool.<a class="footnote" id="DOCF27" href="#FOOT27"><sup>27</sup></a> But
even in numerical operations, tree-recursive processes can be useful in helping
us to understand and design programs.  For instance, although the first
<code class="code">fib</code> procedure is much less efficient than the second one, it is more
straightforward, being little more than a translation into Lisp of the
definition of the Fibonacci sequence.  To formulate the iterative algorithm
required noticing that the computation could be recast as an iteration with
three state variables.
</p>
<h4 class="subsubheading" id="Example_003a-Counting-change"><span>Example: Counting change<a class="copiable-link" href="#Example_003a-Counting-change"> &#182;</a></span></h4>

<p>It takes only a bit of cleverness to come up with the iterative Fibonacci
algorithm.  In contrast, consider the following problem: How many different
ways can we make change of $1.00, given half-dollars, quarters, dimes,
nickels, and pennies?  More generally, can we write a procedure to compute the
number of ways to change any given amount of money?
</p>
<p>This problem has a simple solution as a recursive procedure.  Suppose we think
of the types of coins available as arranged in some order.  Then the following
relation holds:
</p>
<p>The number of ways to change amount <em class="math">a</em> using <em class="math">n</em> kinds of coins equals
</p>
<ul class="itemize mark-bullet">
<li>the number of ways to change amount <em class="math">a</em> using all but the first kind of coin,
plus

</li><li>the number of ways to change amount <em class="math">{a - d}</em> using all <em class="math">n</em> kinds of
coins, where <em class="math">d</em> is the denomination of the first kind of coin.

</li></ul>

<p>To see why this is true, observe that the ways to make change can be divided
into two groups: those that do not use any of the first kind of coin, and those
that do.  Therefore, the total number of ways to make change for some amount is
equal to the number of ways to make change for the amount without using any of
the first kind of coin, plus the number of ways to make change assuming that we
do use the first kind of coin.  But the latter number is equal to the number of
ways to make change for the amount that remains after using a coin of the first
kind.
</p>
<p>Thus, we can recursively reduce the problem of changing a given amount to the
problem of changing smaller amounts using fewer kinds of coins.  Consider this
reduction rule carefully, and convince yourself that we can use it to describe
an algorithm if we specify the following degenerate cases:<a class="footnote" id="DOCF28" href="#FOOT28"><sup>28</sup></a>
</p>
<ul class="itemize mark-bullet">
<li>If <em class="math">a</em> is exactly 0, we should count that as 1 way to make change.

</li><li>If <em class="math">a</em> is less than 0, we should count that as 0 ways to make change.

</li><li>If <em class="math">n</em> is 0, we should count that as 0 ways to make change.

</li></ul>

<p>We can easily translate this description into a recursive procedure:
</p>
<div class="example">
<pre class="example-preformatted">fn count_change(amount: i64) -&gt; u64 {
    cc(amount, 5)
}

fn cc(amount: i64, kinds_of_coins: u64) -&gt; u64 {
    if amount == 0 {
        1
    } else if amount &lt; 0 || kinds_of_coins == 0 {
        0
    } else {
        cc(amount, kinds_of_coins - 1) +
        cc(amount - first_denomination(kinds_of_coins), kinds_of_coins)
    }
}

fn first_denomination(kinds_of_coins: u64) -&gt; i64 {
    match kinds_of_coins {
        1 =&gt; 1,
        2 =&gt; 5,
        3 =&gt; 10,
        4 =&gt; 25,
        5 =&gt; 50,
        _ =&gt; 0,
    }
}
</pre></div>

<p>(The <code class="code">first_denomination</code> procedure takes as input the number of kinds of
coins available and returns the denomination of the first kind.  Here we are
thinking of the coins as arranged in order from largest to smallest, but any
order would do as well.)  We can now answer our original question about
changing a dollar:
</p>
<div class="example">
<pre class="example-preformatted">count_change(100)
<i class="i">292</i>
</pre></div>

<p><code class="code">Count-change</code> generates a tree-recursive process with redundancies
similar to those in our first implementation of <code class="code">fib</code>.  (It will take
quite a while for that 292 to be computed.)  On the other hand, it is not
obvious how to design a better algorithm for computing the result, and we leave
this problem as a challenge.  The observation that a tree-recursive process may
be highly inefficient but often easy to specify and understand has led people
to propose that one could get the best of both worlds by designing a &#8220;smart
compiler&#8221; that could transform tree-recursive procedures into more efficient
procedures that compute the same result.<a class="footnote" id="DOCF29" href="#FOOT29"><sup>29</sup></a>
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e11"></a>Exercise 1.11:</strong> A function <em class="math">f</em> is defined by
the rule that <em class="math">{f(n) = n}</em> if <em class="math">{n &lt; 3}</em> and <em class="math">{f(n)} = {f(n-1)} + {2f(n-2)} + {3f(n-3)}</em> if <em class="math">{n \ge 3}</em>.  
Write a procedure that computes <em class="math">f</em> by means of a recursive process.  Write a procedure that
computes <em class="math">f</em> by means of an iterative process.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e12"></a>Exercise 1.12:</strong> The following pattern of numbers
is called <a class="index-entry-id" id="index-Pascal_0027s-triangle"></a>
<em class="dfn">Pascal&#8217;s triangle</em>.
</p>
<div class="example">
<pre class="example-preformatted">         1
       1   1
     1   2   1
   1   3   3   1
 1   4   6   4   1
       . . .
</pre></div>

<p>The numbers at the edge of the triangle are all 1, and each number inside the
triangle is the sum of the two numbers above it.<a class="footnote" id="DOCF30" href="#FOOT30"><sup>30</sup></a> Write a procedure that computes elements of Pascal&#8217;s triangle by
means of a recursive process.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e13"></a>Exercise 1.13:</strong> Prove that <em class="math">{\text{Fib}(n)}</em> is
the closest integer to <em class="math">{\varphi^n / \sqrt{5}}</em>, where <em class="math">\varphi = {(1 + \sqrt{5}) / 2}</em>.  
Hint: Let <em class="math">\psi = {(1 - \sqrt{5}) / 2}</em>.
Use induction and the definition of the Fibonacci numbers (see <a class="ref" href="#g_t1_002e2_002e2">Tree Recursion</a>) 
to prove that <em class="math">{\text{Fib}(n)} = {(\varphi^n - \psi^n) / \sqrt{5}}</em>.
</p></blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t1_002e2_002e3">
<h4 class="subsection" id="Orders-of-Growth"><span>1.2.3 Orders of Growth<a class="copiable-link" href="#Orders-of-Growth"> &#182;</a></span></h4>

<p>The previous examples illustrate that processes can differ considerably in the
rates at which they consume computational resources.  One convenient way to
describe this difference is to use the notion of <a class="index-entry-id" id="index-order-of-growth"></a>
<em class="dfn">order of growth</em> to
obtain a gross measure of the resources required by a process as the inputs
become larger.
</p>
<p>Let <em class="math">n</em> be a parameter that measures the size of the problem, and let
<em class="math">{R(n)}</em> be the amount of resources the process requires for a problem of
size <em class="math">n</em>.  In our previous examples we took <em class="math">n</em> to be the number for which
a given function is to be computed, but there are other possibilities.  For
instance, if our goal is to compute an approximation to the square root of a
number, we might take <em class="math">n</em> to be the number of digits accuracy required.  For
matrix multiplication we might take <em class="math">n</em> to be the number of rows in the
matrices.  In general there are a number of properties of the problem with
respect to which it will be desirable to analyze a given process.  Similarly,
<em class="math">{R(n)}</em> might measure the number of internal storage registers used, the
number of elementary machine operations performed, and so on.  In computers
that do only a fixed number of operations at a time, the time required will be
proportional to the number of elementary machine operations performed.
</p>
<p>We say that <em class="math">{R(n)}</em> has order of growth <em class="math">{\Theta(f(n))}</em>, written
<em class="math">{R(n)} = {\Theta(f(n))}</em> (pronounced &#8220;theta of
<em class="math">{f(n)}</em>&#8221;), if there are positive constants <em class="math">k_1</em> and <em class="math">k_2</em>
independent of <em class="math">n</em> such that <em class="math">{k_1 f(n)} \le {R(n)} \le {k_2 f(n)}</em> 
for any sufficiently large value of <em class="math">n</em>.  (In other words, for large <em class="math">n</em>,
the value <em class="math">{R(n)}</em> is sandwiched between <em class="math">{k_1 f(n)}</em> and
<em class="math">{k_2 f(n)}</em>.)
</p>
<p>For instance, with the linear recursive process for computing factorial
described in <a class="ref" href="#g_t1_002e2_002e1">Linear Recursion and Iteration</a> the number of steps grows proportionally to
the input <em class="math">n</em>.  Thus, the steps required for this process grows as
<em class="math">{\Theta(n)}</em>.  We also saw that the space required grows as
<em class="math">{\Theta(n)}</em>.  For the iterative factorial, the number of steps is still
<em class="math">{\Theta(n)}</em> but the space is <em class="math">{\Theta(1)}</em>&#8212;that is,
constant.<a class="footnote" id="DOCF31" href="#FOOT31"><sup>31</sup></a> The
tree-recursive Fibonacci computation requires <em class="math">{\Theta(\varphi^n)}</em>
steps and space <em class="math">{\Theta(n)}</em>, where <em class="math">\varphi</em> is the golden ratio
described in <a class="ref" href="#g_t1_002e2_002e2">Tree Recursion</a>.
</p>
<p>Orders of growth provide only a crude description of the behavior of a process.
For example, a process requiring <em class="math">n^2</em> steps and a process requiring
<em class="math">{1000n^2}</em> steps and a process requiring <em class="math">{3n^2} + {10n} + 17</em> steps all
have <em class="math">{\Theta(n^2)}</em> order of growth.  On the other hand, order of growth
provides a useful indication of how we may expect the behavior of the process
to change as we change the size of the problem.  For a <em class="math">{\Theta(n)}</em>
(linear) process, doubling the size will roughly double the amount of resources
used.  For an exponential process, each increment in problem size will multiply
the resource utilization by a constant factor.  In the remainder of 
<a class="ref" href="#g_t1_002e2">Procedures and the Processes They Generate</a> we will examine two algorithms whose order of growth is logarithmic,
so that doubling the problem size increases the resource requirement by a
constant amount.
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e14"></a>Exercise 1.14:</strong> Draw the tree illustrating the
process generated by the <code class="code">count_change</code> procedure of <a class="ref" href="#g_t1_002e2_002e2">Tree Recursion</a>
in making change for 11 cents.  What are the orders of growth of the space and
number of steps used by this process as the amount to be changed increases?
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e15"></a>Exercise 1.15:</strong> The sine of an angle (specified
in radians) can be computed by making use of the approximation 
<em class="math">{\sin x  \approx x}</em> if <em class="math">x</em> is sufficiently small, and the trigonometric
identity

\[ % :12:
 
{\sin x} \,=\, {3\sin \frac{x}{3}} \,-\, {4\sin^3 \frac{x}{3}}
\]

to reduce the size of the argument of sin.  (For purposes of this
exercise an angle is considered &#8220;sufficiently small&#8221; if its magnitude is not
greater than 0.1 radians.) These ideas are incorporated in the following
procedures:
</p>
<div class="example">
<pre class="example-preformatted">fn cube(x: f64) -&gt; f64 { x * x * x }
fn p(x: f64) -&gt; f64 { 3.0 * x - 4.0 * cube(x) }
fn sine(angle: f64) -&gt; f64 {
    if angle.abs() &lt;= 0.1 {
        angle
    } else {
        p(sine(angle / 3.0))
    }
}
</pre></div>

<ol class="enumerate" type="a" start="1">
<li> How many times is the procedure <code class="code">p</code> applied when <code class="code">sine(12.15)</code> is
evaluated?

</li><li> What is the order of growth in space and number of steps (as a function of
<em class="math">a</em>) used by the process generated by the <code class="code">sine</code> procedure when
<code class="code">sine(a)</code> is evaluated?

</li></ol>
</blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t1_002e2_002e4">
<h4 class="subsection" id="Exponentiation"><span>1.2.4 Exponentiation<a class="copiable-link" href="#Exponentiation"> &#182;</a></span></h4>

<p>Consider the problem of computing the exponential of a given number.  We would
like a procedure that takes as arguments a base <em class="math">b</em> and a positive integer
exponent <em class="math">n</em> and computes <em class="math">b^n</em>.  One way to do this is via the
recursive definition

\[ % :13:
 
\begin{array}{l}
  b^n \,=\, b\cdot b^{n-1}, \\ 
  b^0 \,=\, 1, 
\end{array}
\]

which translates readily into the procedure
</p>
<div class="example">
<pre class="example-preformatted">fn expt(b: u64, n: u64) -&gt; u64 {
    if n == 0 {
        1
    } else {
        b * expt(b, n - 1)
    }
}
</pre></div>

<p>This is a linear recursive process, which requires <em class="math">{\Theta(n)}</em> steps and
<em class="math">{\Theta(n)}</em> space.  Just as with factorial, we can readily formulate an
equivalent linear iteration:
</p>
<div class="example">
<pre class="example-preformatted">fn expt(b: u64, n: u64) -&gt; u64 {
    expt_iter(b, n, 1)
}

fn expt_iter(b: u64, counter: u64, product: u64) -&gt; u64 {
    if counter == 0 {
        product
    } else {
        expt_iter(b, counter - 1, b * product)
    }
}
</pre></div>

<p>This version requires <em class="math">{\Theta(n)}</em> steps and <em class="math">{\Theta(1)}</em> space.
</p>
<p>We can compute exponentials in fewer steps by using successive squaring.  For
instance, rather than computing <em class="math">b^8</em> as

\[ % :14:
  
{b\cdot (b\cdot (b}\cdot {(b\cdot (b\cdot (b}\cdot {(b\cdot b)))))),}
\]

we can compute it using three multiplications:

\[ % :15:
 
\begin{array}{l}
  b^2 \,=\, b\cdot b, \\ 
  b^4 \,=\, b^2\cdot b^2, \\
  b^8 \,=\, b^4\cdot b^4.
\end{array}
\]

This method works fine for exponents that are powers of 2.  We can also take
advantage of successive squaring in computing exponentials in general if we use
the rule

\[ % :16:
 
\begin{array}{ll}
  b^n \,=\, (b^{n / 2})^2   &amp; \text{if} \; n \; \text{is even}, \\
  b^n \,=\, b\cdot b^{n-1}  &amp; \text{if} \; n \; \text{is odd}.
\end{array}
\]

We can express this method as a procedure:
</p>
<div class="example">
<pre class="example-preformatted">fn fast_expt(b: u64, n: u64) -&gt; u64 {
    if n == 0 {
        1
    } else if n % 2 == 0 {
        let half = fast_expt(b, n / 2);
        half * half
    } else {
        b * fast_expt(b, n - 1)
    }
}
</pre></div>

<p>where the predicate to test whether an integer is even is defined in terms of
the primitive procedure <code class="code">remainder</code> by
</p>
<div class="example">
<pre class="example-preformatted">fn is_even(n: u64) -&gt; bool {
    n % 2 == 0
}
</pre></div>

<p>The process evolved by <code class="code">fast-expt</code> grows logarithmically with <em class="math">n</em> in
both space and number of steps.  To see this, observe that computing
<em class="math">b^{2n}</em> using <code class="code">fast-expt</code> requires only one more multiplication
than computing <em class="math">b^n</em>.  The size of the exponent we can compute therefore
doubles (approximately) with every new multiplication we are allowed.  Thus,
the number of multiplications required for an exponent of <em class="math">n</em> grows about as
fast as the logarithm of <em class="math">n</em> to the base 2.  The process has
<em class="math">{\Theta(\log n)}</em> growth.<a class="footnote" id="DOCF32" href="#FOOT32"><sup>32</sup></a>
</p>
<p>The difference between <em class="math">{\Theta(\log n)}</em> growth and
<em class="math">{\Theta(n)}</em> growth becomes striking as <em class="math">n</em> becomes large.  For
example, <code class="code">fast-expt</code> for <em class="math">n</em> = 1000 requires only 14
multiplications.<a class="footnote" id="DOCF33" href="#FOOT33"><sup>33</sup></a> It is also possible to
use the idea of successive squaring to devise an iterative algorithm that
computes exponentials with a logarithmic number of steps (see <a class="ref" href="#Exercise-1_002e16">Exercise 1.16</a>), 
although, as is often the case with iterative algorithms, this is not
written down so straightforwardly as the recursive algorithm.<a class="footnote" id="DOCF34" href="#FOOT34"><sup>34</sup></a>
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e16"></a>Exercise 1.16:</strong> Design a procedure that evolves
an iterative exponentiation process that uses successive squaring and uses a
logarithmic number of steps, as does <code class="code">fast-expt</code>.  (Hint: Using the
observation that <em class="math">{(b^{n / 2})^2} = {(b^2)^{n / 2}}</em>, keep, along with
the exponent <em class="math">n</em> and the base <em class="math">b</em>, an additional state variable <em class="math">a</em>, and
define the state transformation in such a way that the product <em class="math">{ab^n}</em> 
is unchanged from state to state.  At the beginning of the process
<em class="math">a</em> is taken to be 1, and the answer is given by the value of <em class="math">a</em> at the
end of the process.  In general, the technique of defining an
<a class="index-entry-id" id="index-invariant-quantity"></a>
<em class="dfn">invariant quantity</em> that remains unchanged from state to state is a
powerful way to think about the design of iterative algorithms.)
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e17"></a>Exercise 1.17:</strong> The exponentiation algorithms in
this section are based on performing exponentiation by means of repeated
multiplication.  In a similar way, one can perform integer multiplication by
means of repeated addition.  The following multiplication procedure (in which
it is assumed that our language can only add, not multiply) is analogous to the
<code class="code">expt</code> procedure:
</p>
<div class="example">
<pre class="example-preformatted">fn multiply(a: u64, b: u64) -&gt; u64 {
    if b == 0 {
        0
    } else {
        a + multiply(a, b - 1)
    }
}
</pre></div>

<p>This algorithm takes a number of steps that is linear in <code class="code">b</code>.  Now suppose
we include, together with addition, operations <code class="code">double</code>, which doubles an
integer, and <code class="code">halve</code>, which divides an (even) integer by 2.  Using these,
design a multiplication procedure analogous to <code class="code">fast-expt</code> that uses a
logarithmic number of steps.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e18"></a>Exercise 1.18:</strong> Using the results of
<a class="ref" href="#Exercise-1_002e16">Exercise 1.16</a> and <a class="ref" href="#Exercise-1_002e17">Exercise 1.17</a>, devise a procedure that generates
an iterative process for multiplying two integers in terms of adding, doubling,
and halving and uses a logarithmic number of steps.<a class="footnote" id="DOCF35" href="#FOOT35"><sup>35</sup></a>
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e19"></a>Exercise 1.19:</strong> There is a clever algorithm for
computing the Fibonacci numbers in a logarithmic number of steps.  Recall the
transformation of the state variables <em class="math">a</em> and <em class="math">b</em> in the <code class="code">fib-iter</code>
process of <a class="ref" href="#g_t1_002e2_002e2">Tree Recursion</a>: <em class="math">a \gets a + b</em> and <em class="math">b \gets a</em>.
Call this transformation <em class="math">T</em>, and observe that applying <em class="math">T</em> over and over
again <em class="math">n</em> times, starting with 1 and 0, produces the pair <em class="math">{\text{Fib}(n+1)}</em> and 
<em class="math">{\text{Fib}(n)}</em>.  In other words, the Fibonacci numbers are produced
by applying <em class="math">T^n</em>, the <em class="math">n^{\text{th}}</em> power of the transformation <em class="math">T</em>,
starting with the pair (1, 0).  Now consider <em class="math">T</em> to be the special case of
<em class="math">{p=0}</em> and <em class="math">{q=1}</em> in a family of transformations <em class="math">T_{pq}</em>,
where <em class="math">T_{pq}</em> transforms the pair <em class="math">{(a, b)}</em> according to 
<em class="math">a \gets {bq} + {aq} + {ap}</em> and <em class="math">b \gets {bp} + {aq}</em>.
Show that if we apply such a transformation <em class="math">T_{pq}</em> twice, the
effect is the same as using a single transformation <em class="math">T_{p'q'}</em> of the
same form, and compute <em class="math">p'\!</em> and <em class="math">q'\!</em> in terms of <em class="math">p</em> and <em class="math">q</em>.  This
gives us an explicit way to square these transformations, and thus we can
compute <em class="math">T^n</em> using successive squaring, as in the <code class="code">fast-expt</code>
procedure.  Put this all together to complete the following procedure, which
runs in a logarithmic number of steps:<a class="footnote" id="DOCF36" href="#FOOT36"><sup>36</sup></a>
</p>
<div class="example">
<pre class="example-preformatted">fn fib(n: u64) -&gt; u64 {
    fib_iter(1, 0, 0, 1, n)
}

fn fib_iter(a: u64, b: u64, p: u64, q: u64, count: u64) -&gt; u64 {
    if count == 0 {
        b
    } else if count % 2 == 0 {
        fib_iter(a,
                 b,
                 ⟨??⟩,  // compute p'
                 ⟨??⟩,  // compute q'
                 count / 2)
    } else {
        fib_iter(b * q + a * q + a * p,
                 b * p + a * q,
                 p,
                 q,
                 count - 1)
    }
}
</pre></div>
</blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t1_002e2_002e4a">
<h4 class="subsection" id="Const-Evaluation"><span>1.2.5 Const Evaluation<a class="copiable-link" href="#Const-Evaluation"> &#182;</a></span></h4>

<p>One of Rust&#8217;s most powerful features for achieving zero-cost abstraction is
<a class="index-entry-id" id="index-compile_002dtime-computation"></a>
<em class="dfn">compile-time computation</em> through const evaluation.  While the
procedures we&#8217;ve seen so far are evaluated at runtime, Rust allows us to
evaluate certain computations during compilation, eliminating all runtime
overhead for those operations.
</p>
<p>Consider the factorial function from <a class="ref" href="#g_t1_002e2_002e1">Linear Recursion and Iteration</a>.  When we write
<code class="code">factorial(5)</code>, the computation happens when the program runs.  But if we
know the argument at compile time, why should we pay for this computation
every time the program executes?  Rust&#8217;s <code class="code">const fn</code> feature allows the
compiler to perform such calculations once, at compile time.
</p>
<div class="example">
<pre class="example-preformatted">const fn factorial(n: u64) -&gt; u64 {
    if n == 0 {
        1
    } else {
        n * factorial(n - 1)
    }
}

const FACT_5: u64 = factorial(5);  // Computed at compile time
const FACT_10: u64 = factorial(10); // Also at compile time
</pre></div>

<p>When we declare <code class="code">FACT_5</code>, the compiler evaluates <code class="code">factorial(5)</code>
during compilation and replaces <code class="code">FACT_5</code> with the literal value
<code class="code">120</code> in the generated code.  There is no function call at runtime, no
stack frames, no computation&#8212;just the final value.  This is what we mean by
<a class="index-entry-id" id="index-zero_002dcost-abstraction"></a>
<em class="dfn">zero-cost abstraction</em>: we write high-level code (a factorial
function), but pay no runtime cost for the abstraction.
</p>
<p>The <code class="code">const</code> keyword before <code class="code">fn</code> indicates that this function can be
evaluated at compile time.  However, this imposes certain restrictions: const
functions cannot perform arbitrary operations.  They must be <a class="index-entry-id" id="index-pure"></a>
functions
<em class="dfn">pure
functions</em>&#8212;functions that depend only on their arguments and have no side
effects.  They cannot allocate heap memory, perform I/O, or call non-const
functions.
</p>
<p>Let&#8217;s examine the difference between compile-time and runtime evaluation more
closely by considering the Fibonacci sequence from <a class="ref" href="#g_t1_002e2_002e2">Tree Recursion</a>:
</p>
<div class="example">
<pre class="example-preformatted">const fn fib(n: u64) -&gt; u64 {
    if n &lt; 2 {
        n
    } else {
        fib(n - 1) + fib(n - 2)
    }
}

// Compile-time evaluation
const FIB_10: u64 = fib(10);

// Runtime evaluation
fn compute_fib_at_runtime(n: u64) -&gt; u64 {
    fib(n)  // Computed when program runs
}
</pre></div>

<p>When we write <code class="code">const FIB_10 = fib(10)</code>, the compiler must know the value
before the program runs.  It evaluates the recursive calls during compilation,
producing the value <code class="code">55</code>.  The compiled program contains only this
literal value.  In contrast, <code class="code">compute_fib_at_runtime(10)</code> performs the
computation every time it&#8217;s called, even though the argument is the same.
</p>
<p>This distinction becomes critical for performance-sensitive code.  Consider
computing powers of two for a lookup table:
</p>
<div class="example">
<pre class="example-preformatted">const fn pow2(n: u32) -&gt; u64 {
    1 &lt;&lt; n
}

const POWERS_OF_TWO: [u64; 64] = {
    let mut arr = [0; 64];
    let mut i = 0;
    while i &lt; 64 {
        arr[i] = pow2(i as u32);
        i += 1;
    }
    arr
};
</pre></div>

<p>This example introduces <a class="index-entry-id" id="index-const-blocks"></a>
<em class="dfn">const blocks</em>&#8212;blocks of code that execute
at compile time.  The <code class="code">const</code> context allows us to use imperative loops
and mutable bindings (within the block), but all computation happens during
compilation.  The resulting array <code class="code">POWERS_OF_TWO</code> is embedded directly in
the program&#8217;s data section, with all 64 values pre-computed.
</p>
<p>The compiler&#8217;s const evaluation engine is quite sophisticated.  It can handle:
</p>
<ul class="itemize mark-bullet">
<li>Arithmetic operations and bitwise operations
</li><li>Conditionals (<code class="code">if</code> expressions)
</li><li>Loops (<code class="code">while</code> and <code class="code">loop</code>, but not <code class="code">for</code> in all contexts)
</li><li>Pattern matching
</li><li>Function calls to other const functions
</li><li>Tuple and array operations
</li></ul>

<p>However, the compiler must be able to determine when evaluation finishes.
Infinite loops or unbounded recursion in const contexts will cause compilation
to fail:
</p>
<div class="example">
<pre class="example-preformatted">const fn infinite() -&gt; u64 {
    infinite()  // Compilation error: infinite recursion
}
</pre></div>

<h4 class="subheading" id="Const-Generics"><span>Const Generics<a class="copiable-link" href="#Const-Generics"> &#182;</a></span></h4>

<p>Const evaluation becomes even more powerful when combined with <a class="index-entry-id" id="index-const"></a>
generics
<em class="dfn">const
generics</em>&#8212;generic parameters that are values rather than types.  This allows
us to write functions that are specialized for specific constant values at
compile time.
</p>
<div class="example">
<pre class="example-preformatted">fn create_array&lt;const N: usize&gt;() -&gt; [u64; N] {
    let mut arr = [0; N];
    let mut i = 0;
    while i &lt; N {
        arr[i] = (i * i) as u64;
        i += 1;
    }
    arr
}

const SQUARES_5: [u64; 5] = create_array::&lt;5&gt;();
const SQUARES_100: [u64; 100] = create_array::&lt;100&gt;();
</pre></div>

<p>Here, <code class="code">N</code> is a const generic parameter&#8212;a compile-time constant value.
The compiler generates specialized versions of <code class="code">create_array</code> for each
value of <code class="code">N</code> we use, and if called in a const context, it evaluates the
entire function at compile time.
</p>
<p>This feature is particularly valuable for fixed-size data structures.  The
fast exponentiation algorithm from <a class="ref" href="#g_t1_002e2_002e4">Exponentiation</a> can be adapted to create
compile-time lookup tables:
</p>
<div class="example">
<pre class="example-preformatted">const fn fast_expt(b: u64, n: u64) -&gt; u64 {
    if n == 0 {
        1
    } else if n % 2 == 0 {
        let half = fast_expt(b, n / 2);
        half * half
    } else {
        b * fast_expt(b, n - 1)
    }
}

const fn create_power_table&lt;const BASE: u64, const SIZE: usize&gt;()
    -&gt; [u64; SIZE]
{
    let mut table = [0; SIZE];
    let mut i = 0;
    while i &lt; SIZE {
        table[i] = fast_expt(BASE, i as u64);
        i += 1;
    }
    table
}

// Pre-compute powers of 2 from 2^0 to 2^10
const POWERS_OF_2: [u64; 11] = create_power_table::&lt;2, 11&gt;();

// Pre-compute powers of 3 from 3^0 to 3^10
const POWERS_OF_3: [u64; 11] = create_power_table::&lt;3, 11&gt;();
</pre></div>

<p>The compiler generates all 11 powers for each base at compile time.  Looking
up <code class="code">POWERS_OF_2[5]</code> at runtime is a simple array indexing operation&#8212;no
exponentiation computation occurs.
</p>
<h4 class="subheading" id="When-Does-the-Compiler-Evaluate_003f"><span>When Does the Compiler Evaluate?<a class="copiable-link" href="#When-Does-the-Compiler-Evaluate_003f"> &#182;</a></span></h4>

<p>Understanding when the compiler chooses to perform const evaluation versus
deferring to runtime is crucial for writing efficient code.  The rules are:
</p>
<ol class="enumerate">
<li> <strong class="strong">Must evaluate:</strong> In const contexts (<code class="code">const</code> items, array lengths,
const generic arguments), the compiler must evaluate the expression at compile
time.  If it cannot, compilation fails.

</li><li> <strong class="strong">May evaluate:</strong> When a const function is called with constant arguments
in a non-const context, the compiler may evaluate it at compile time as an
optimization, but is not required to.

</li><li> <strong class="strong">Cannot evaluate:</strong> When a const function is called with runtime values,
evaluation must happen at runtime.
</li></ol>

<div class="example">
<pre class="example-preformatted">const fn square(x: u64) -&gt; u64 {
    x * x
}

const A: u64 = square(5);           // Must evaluate at compile time

fn example() {
    let b = square(5);              // May evaluate at compile time
    let n = read_input();           // Runtime value
    let c = square(n);              // Must evaluate at runtime
}
</pre></div>

<p>In this example, <code class="code">A</code> is guaranteed to contain the value <code class="code">25</code> with no
runtime computation.  The value of <code class="code">b</code> might be computed at compile time
(an optimization), but <code class="code">c</code> must be computed at runtime because <code class="code">n</code>
is not known until the program runs.
</p>
<p>Modern Rust compilers are quite aggressive about compile-time evaluation when
possible.  Simple const functions called with literal arguments are almost
always evaluated at compile time, even in non-const contexts.  This is part of
the zero-cost abstraction philosophy: abstractions should impose no runtime
penalty.
</p>
<h4 class="subheading" id="Limits-of-Const-Evaluation"><span>Limits of Const Evaluation<a class="copiable-link" href="#Limits-of-Const-Evaluation"> &#182;</a></span></h4>

<p>While powerful, const evaluation has limitations rooted in the halting
problem.  The compiler must determine whether evaluation will complete, which
is undecidable in general.  Therefore, Rust imposes practical limits:
</p>
<ul class="itemize mark-bullet">
<li><strong class="strong">Recursion depth:</strong> There&#8217;s a limit on how deeply recursive calls can
nest during const evaluation (typically around 128 levels, but
implementation-specific).

</li><li><strong class="strong">Instruction count:</strong> The compiler limits the total number of
instructions executed during const evaluation to prevent compilation from
hanging on infinite loops.

</li><li><strong class="strong">Available operations:</strong> Only a subset of Rust&#8217;s operations are
permitted in const contexts.  For example, heap allocation through <code class="code">Box</code>
or <code class="code">Vec</code> is not allowed in const functions (as of Rust 1.83).
</li></ul>

<p>When these limits are exceeded, you&#8217;ll receive a compilation error indicating
that const evaluation failed.  The solution is typically to either reduce the
complexity of the computation or perform it at runtime.
</p>
<h4 class="subheading" id="The-Philosophy-of-Zero_002dCost-Abstraction"><span>The Philosophy of Zero-Cost Abstraction<a class="copiable-link" href="#The-Philosophy-of-Zero_002dCost-Abstraction"> &#182;</a></span></h4>

<p>Const evaluation exemplifies Rust&#8217;s commitment to zero-cost abstraction.  We
can write expressive, high-level code (recursive functions, loops, complex
data structures) while maintaining the performance of hand-written, optimized
machine code.  The abstraction cost is paid at compile time, not runtime.
</p>
<p>This philosophy pervades Rust&#8217;s design.  Later, we&#8217;ll see how it extends to
iterators (<a class="ref" href="1_002e3.xhtml#g_t1_002e3_002e5">Iterator Combinators</a>), generic functions, and trait implementations.  In
each case, the goal is the same: write code that is easy to understand and
maintain, while generating machine code as efficient as if you&#8217;d written
low-level optimized code by hand.
</p>
<p>The key insight is that many programs contain computations whose inputs are
known at compile time.  Configuration values, mathematical constants, lookup
tables&#8212;all can be pre-computed once during compilation rather than
recomputed every execution.  Const evaluation makes this optimization
automatic and transparent.
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e24a"></a>Exercise 1.24a:</strong> The iterative factorial from
<a class="ref" href="#g_t1_002e2_002e1">Linear Recursion and Iteration</a> can also be made const.  Define a const version of the iterative
factorial procedure.  Then create a const array containing the factorials of
numbers 0 through 12.  (Note: 13! exceeds <code class="code">u64::MAX</code>, so stop at 12.)
</p>
<p>Verify that your array is correct by checking that <code class="code">FACTORIALS[5]</code> equals
120 and <code class="code">FACTORIALS[12]</code> equals 479001600.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e24b"></a>Exercise 1.24b:</strong> Consider the <code class="code">fast_expt</code>
procedure from <a class="ref" href="#g_t1_002e2_002e4">Exponentiation</a>.  Make it a const function, then use it to compute
<em class="math">2^{64} - 1</em> at compile time (this is the maximum value representable in
a <code class="code">u64</code>, also available as <code class="code">u64::MAX</code>).
</p>
<p>Explain why <code class="code">const MAX: u64 = fast_expt(2, 64);</code> would cause a
compilation error (hint: overflow), and show how to correctly compute
<em class="math">2^{64} - 1</em> using const evaluation.
</p>
<p>Now consider: if you call <code class="code">fast_expt(2, 10)</code> in a regular function body
(not a const context), can you guarantee it will be evaluated at compile time?
Why or why not?
</p></blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t1_002e2_002e5">
<h4 class="subsection" id="Greatest-Common-Divisors"><span>1.2.6 Greatest Common Divisors<a class="copiable-link" href="#Greatest-Common-Divisors"> &#182;</a></span></h4>

<p>The greatest common divisor (<abbr class="abbr">GCD</abbr>) of two integers <em class="math">a</em> and <em class="math">b</em> is
defined to be the largest integer that divides both <em class="math">a</em> and <em class="math">b</em> with no
remainder.  For example, the <abbr class="abbr">GCD</abbr> of 16 and 28 is 4.  In <a class="ref" href="Chapter-2.xhtml">Building Abstractions with Data</a>, when we investigate how to implement rational-number arithmetic, we will
need to be able to compute <abbr class="abbr">GCD</abbr>s in order to reduce rational numbers
to lowest terms.  (To reduce a rational number to lowest terms, we must divide
both the numerator and the denominator by their <abbr class="abbr">GCD</abbr>.  For example,
16/28 reduces to 4/7.)  One way to find the <abbr class="abbr">GCD</abbr> of two integers is to
factor them and search for common factors, but there is a famous algorithm that
is much more efficient.
</p>
<p>The idea of the algorithm is based on the observation that, if <em class="math">r</em> is the
remainder when <em class="math">a</em> is divided by <em class="math">b</em>, then the common divisors of <em class="math">a</em> and
<em class="math">b</em> are precisely the same as the common divisors of <em class="math">b</em> and <em class="math">r</em>.  Thus,
we can use the equation
</p>
<div class="example">
<pre class="example-preformatted">GCD(a,b) = GCD(b,r)
</pre></div>

<p>to successively reduce the problem of computing a <abbr class="abbr">GCD</abbr> to the problem
of computing the <abbr class="abbr">GCD</abbr> of smaller and smaller pairs of integers.  For
example,
</p>
<div class="example">
<pre class="example-preformatted">GCD(206,40) = GCD(40,6)
            = GCD(6,4)
            = GCD(4,2)
            = GCD(2,0) = 2
</pre></div>

<p>reduces <abbr class="abbr">GCD</abbr>(206, 40) to <abbr class="abbr">GCD</abbr>(2, 0), which is 2.  It is
possible to show that starting with any two positive integers and performing
repeated reductions will always eventually produce a pair where the second
number is 0.  Then the <abbr class="abbr">GCD</abbr> is the other number in the pair.  This
method for computing the <abbr class="abbr">GCD</abbr> is known as 
<a class="index-entry-id" id="index-Euclid_0027s-Algorithm"></a>
<em class="dfn">Euclid&#8217;s Algorithm</em>.<a class="footnote" id="DOCF37" href="#FOOT37"><sup>37</sup></a>
</p>
<p>It is easy to express Euclid&#8217;s Algorithm as a function:
</p>
<div class="example">
<pre class="example-preformatted">fn gcd(a: u64, b: u64) -&gt; u64 {
    if b == 0 {
        a
    } else {
        gcd(b, a % b)
    }
}
</pre></div>

<p>This generates an iterative process, whose number of steps grows as the
logarithm of the numbers involved.
</p>
<p>The fact that the number of steps required by Euclid&#8217;s Algorithm has
logarithmic growth bears an interesting relation to the Fibonacci numbers:
</p>
<blockquote class="quotation">
<p><strong class="strong">Lam&#233;&#8217;s Theorem:</strong> If Euclid&#8217;s Algorithm requires <em class="math">k</em> steps to
compute the <abbr class="abbr">GCD</abbr> of some pair, then the smaller number in the pair
must be greater than or equal to the <em class="math">k^{\text{th}}</em> Fibonacci number.<a class="footnote" id="DOCF38" href="#FOOT38"><sup>38</sup></a>
</p></blockquote>

<p>We can use this theorem to get an order-of-growth estimate for Euclid&#8217;s
Algorithm.  Let <em class="math">n</em> be the smaller of the two inputs to the procedure.  If
the process takes <em class="math">k</em> steps, then we must have 
<em class="math">n \ge {\text{Fib}(k)} \approx {\varphi^k / \sqrt{5}}</em>.  Therefore the number of steps <em class="math">k</em>
grows as the logarithm (to the base <em class="math">\varphi</em>) of <em class="math">n</em>.  Hence, the order of
growth is <em class="math">{\Theta(\log n)}</em>.
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e20"></a>Exercise 1.20:</strong> The process that a procedure
generates is of course dependent on the rules used by the interpreter.  As an
example, consider the iterative <code class="code">gcd</code> procedure given above.  Suppose we
were to interpret this procedure using normal-order evaluation, as discussed in
<a class="ref" href="1_002e1.xhtml#g_t1_002e1_002e5">The Substitution Model for Procedure Application</a>.  (The normal-order-evaluation rule for <code class="code">if</code> is
described in <a class="ref" href="1_002e1.xhtml#Exercise-1_002e5">Exercise 1.5</a>.)  Using the substitution method (for normal
order), illustrate the process generated in evaluating <code class="code">gcd(206, 40)</code> and
indicate the <code class="code">remainder</code> operations that are actually performed.  How many
<code class="code">remainder</code> operations are actually performed in the normal-order
evaluation of <code class="code">gcd(206, 40)</code>?  In the applicative-order evaluation?
</p></blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t1_002e2_002e6">
<h4 class="subsection" id="Example_003a-Testing-for-Primality"><span>1.2.7 Example: Testing for Primality<a class="copiable-link" href="#Example_003a-Testing-for-Primality"> &#182;</a></span></h4>

<p>This section describes two methods for checking the primality of an integer
<em class="math">n</em>, one with order of growth <em class="math">{\Theta(\sqrt{n})}</em>, and a
&#8220;probabilistic&#8221; algorithm with order of growth <em class="math">{\Theta(\log n)}</em>.
The exercises at the end of this section suggest programming projects based on
these algorithms.
</p>
<h4 class="subsubheading" id="Searching-for-divisors"><span>Searching for divisors<a class="copiable-link" href="#Searching-for-divisors"> &#182;</a></span></h4>

<p>Since ancient times, mathematicians have been fascinated by problems concerning
prime numbers, and many people have worked on the problem of determining ways
to test if numbers are prime.  One way to test if a number is prime is to find
the number&#8217;s divisors.  The following program finds the smallest integral
divisor (greater than 1) of a given number <em class="math">n</em>.  It does this in a
straightforward way, by testing <em class="math">n</em> for divisibility by successive integers
starting with 2.
</p>
<div class="example">
<pre class="example-preformatted">fn smallest_divisor(n: u64) -&gt; u64 {
    find_divisor(n, 2)
}

fn find_divisor(n: u64, test_divisor: u64) -&gt; u64 {
    if square(test_divisor) &gt; n {
        n
    } else if divides(test_divisor, n) {
        test_divisor
    } else {
        find_divisor(n, test_divisor + 1)
    }
}

fn divides(a: u64, b: u64) -&gt; bool {
    b % a == 0
}
</pre></div>

<p>We can test whether a number is prime as follows: <em class="math">n</em> is prime if and only if
<em class="math">n</em> is its own smallest divisor.
</p>
<div class="example">
<pre class="example-preformatted">fn is_prime(n: u64) -&gt; bool {
    n == smallest_divisor(n)
}
</pre></div>

<p>The end test for <code class="code">find_divisor</code> is based on the fact that if <em class="math">n</em> is not
prime it must have a divisor less than or equal to
<em class="math">\sqrt{n}</em>.<a class="footnote" id="DOCF39" href="#FOOT39"><sup>39</sup></a>  This means that the algorithm need only test divisors
between 1 and <em class="math">\sqrt{n}</em>.  Consequently, the number of steps required
to identify <em class="math">n</em> as prime will have order of growth
<em class="math">{\Theta(\sqrt{n})}</em>.
</p>
<h4 class="subsubheading" id="The-Fermat-test"><span>The Fermat test<a class="copiable-link" href="#The-Fermat-test"> &#182;</a></span></h4>

<p>The <em class="math">{\Theta(\log n)}</em> primality test is based on a result from
number theory known as Fermat&#8217;s Little Theorem.<a class="footnote" id="DOCF40" href="#FOOT40"><sup>40</sup></a>
</p>
<blockquote class="quotation">
<p><strong class="strong">Fermat&#8217;s Little Theorem:</strong> If <em class="math">n</em> is a prime number and <em class="math">a</em> is any
positive integer less than <em class="math">n</em>, then <em class="math">a</em> raised to the <em class="math">n^{\text{th}}</em> power is
congruent to <em class="math">a</em> modulo <em class="math">n</em>.
</p></blockquote>

<p>(Two numbers are said to be <a class="index-entry-id" id="index-congruent-modulo"></a>
<em class="dfn">congruent modulo</em> <em class="math">n</em> if they both have
the same remainder when divided by <em class="math">n</em>.  The remainder of a number <em class="math">a</em> when
divided by <em class="math">n</em> is also referred to as the <a class="index-entry-id" id="index-remainder-of"></a>
<em class="dfn">remainder of</em> <em class="math">a</em>
<a class="index-entry-id" id="index-modulo"></a>
<em class="dfn">modulo</em> <em class="math">n</em>, or simply as <em class="math">a</em> <a class="index-entry-id" id="index-modulo-1"></a>
<em class="dfn">modulo</em> <em class="math">n</em>.)
</p>
<p>If <em class="math">n</em> is not prime, then, in general, most of the numbers <em class="math">{a &lt; n}</em> will
not satisfy the above relation.  This leads to the following algorithm for
testing primality: Given a number <em class="math">n</em>, pick a random number <em class="math">{a &lt; n}</em> and
compute the remainder of <em class="math">a^n</em> modulo <em class="math">n</em>.  If the result is not equal
to <em class="math">a</em>, then <em class="math">n</em> is certainly not prime.  If it is <em class="math">a</em>, then chances are
good that <em class="math">n</em> is prime.  Now pick another random number <em class="math">a</em> and test it
with the same method.  If it also satisfies the equation, then we can be even
more confident that <em class="math">n</em> is prime.  By trying more and more values of <em class="math">a</em>,
we can increase our confidence in the result.  This algorithm is known as the
Fermat test.
</p>
<p>To implement the Fermat test, we need a procedure that computes the exponential
of a number modulo another number:
</p>
<div class="example">
<pre class="example-preformatted">fn expmod(base: u64, exp: u64, m: u64) -&gt; u64 {
    if exp == 0 {
        1
    } else if is_even(exp) {
        square(expmod(base, exp / 2, m)) % m
    } else {
        (base * expmod(base, exp - 1, m)) % m
    }
}
</pre></div>

<p>This is very similar to the <code class="code">fast-expt</code> procedure of <a class="ref" href="#g_t1_002e2_002e4">Exponentiation</a>.
It uses successive squaring, so that the number of steps grows logarithmically
with the exponent.<a class="footnote" id="DOCF41" href="#FOOT41"><sup>41</sup></a>
</p>
<p>The Fermat test is performed by choosing at random a number <em class="math">a</em> between 1 and
<em class="math">{n-1}</em> inclusive and checking whether the remainder modulo <em class="math">n</em> of the
<em class="math">n^{\text{th}}</em> power of <em class="math">a</em> is equal to <em class="math">a</em>.  The random number <em class="math">a</em> is chosen
using a procedure <code class="code">random</code>, which we assume is available in our environment
(in Rust, this would typically come from the <code class="code">rand</code> crate).
<code class="code">Random</code> returns a nonnegative integer less than its integer input.  Hence, to obtain a random number between 1 and <em class="math">{n-1}</em>, we call
<code class="code">random</code> with an input of <em class="math">{n-1}</em> and add 1 to the result:
</p>
<div class="example">
<pre class="example-preformatted">fn fermat_test(n: u64) -&gt; bool {
    fn try_it(a: u64, n: u64) -&gt; bool {
        expmod(a, n, n) == a
    }
    // Note: Using rand crate's random() function
    // In Rust: use rand::Rng; let a = rand::thread_rng().gen_range(1..n);
    let a = 1 + (rand::random::&lt;u64&gt;() % (n - 1));
    try_it(a, n)
}
</pre></div>

<p>The following procedure runs the test a given number of times, as specified by
a parameter.  Its value is true if the test succeeds every time, and false
otherwise.
</p>
<div class="example">
<pre class="example-preformatted">fn fast_prime(n: u64, times: u64) -&gt; bool {
    if times == 0 {
        true
    } else if fermat_test(n) {
        fast_prime(n, times - 1)
    } else {
        false
    }
}
</pre></div>

<h4 class="subsubheading" id="Probabilistic-methods"><span>Probabilistic methods<a class="copiable-link" href="#Probabilistic-methods"> &#182;</a></span></h4>

<p>The Fermat test differs in character from most familiar algorithms, in which
one computes an answer that is guaranteed to be correct.  Here, the answer
obtained is only probably correct.  More precisely, if <em class="math">n</em> ever fails the
Fermat test, we can be certain that <em class="math">n</em> is not prime.  But the fact that
<em class="math">n</em> passes the test, while an extremely strong indication, is still not a
guarantee that <em class="math">n</em> is prime.  What we would like to say is that for any
number <em class="math">n</em>, if we perform the test enough times and find that <em class="math">n</em> always
passes the test, then the probability of error in our primality test can be
made as small as we like.
</p>
<p>Unfortunately, this assertion is not quite correct.  There do exist numbers
that fool the Fermat test: numbers <em class="math">n</em> that are not prime and yet have the
property that <em class="math">a^n</em> is congruent to <em class="math">a</em> modulo <em class="math">n</em> for all integers
<em class="math">{a &lt; n}</em>.  Such numbers are extremely rare, so the Fermat test is quite
reliable in practice.<a class="footnote" id="DOCF42" href="#FOOT42"><sup>42</sup></a>
</p>
<p>There are variations of the Fermat test that cannot be fooled.  In these tests,
as with the Fermat method, one tests the primality of an integer <em class="math">n</em> by
choosing a random integer <em class="math">{a &lt; n}</em> and checking some condition that depends
upon <em class="math">n</em> and <em class="math">a</em>.  (See <a class="ref" href="#Exercise-1_002e28">Exercise 1.28</a> for an example of such a test.)
On the other hand, in contrast to the Fermat test, one can prove that, for any
<em class="math">n</em>, the condition does not hold for most of the integers <em class="math">{a &lt; n}</em> unless
<em class="math">n</em> is prime.  Thus, if <em class="math">n</em> passes the test for some random choice of
<em class="math">a</em>, the chances are better than even that <em class="math">n</em> is prime.  If <em class="math">n</em> passes
the test for two random choices of <em class="math">a</em>, the chances are better than 3 out of
4 that <em class="math">n</em> is prime. By running the test with more and more randomly chosen
values of <em class="math">a</em> we can make the probability of error as small as we like.
</p>
<p>The existence of tests for which one can prove that the chance of error becomes
arbitrarily small has sparked interest in algorithms of this type, which have
come to be known as <a class="index-entry-id" id="index-probabilistic-algorithms"></a>
<em class="dfn">probabilistic algorithms</em>.  There is a great deal
of research activity in this area, and probabilistic algorithms have been
fruitfully applied to many fields.<a class="footnote" id="DOCF43" href="#FOOT43"><sup>43</sup></a>
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e21"></a>Exercise 1.21:</strong> Use the <code class="code">smallest_divisor</code>
procedure to find the smallest divisor of each of the following numbers: 199,
1999, 19999.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e22"></a>Exercise 1.22:</strong> Most Lisp implementations include
a primitive called <code class="code">runtime</code> that returns an integer that specifies the
amount of time the system has been running (measured, for example, in
microseconds).  The following <code class="code">timed_prime_test</code> procedure, when called
with an integer <em class="math">n</em>, prints <em class="math">n</em> and checks to see if <em class="math">n</em> is prime.  If
<em class="math">n</em> is prime, the procedure prints three asterisks followed by the amount of
time used in performing the test.
</p>
<div class="example">
<pre class="example-preformatted">fn timed_prime_test(n: u64) {
    println!();
    print!(&quot;{}&quot;, n);
    start_prime_test(n, Instant::now());
}
</pre></div>

<div class="example">
<pre class="example-preformatted">fn start_prime_test(n: u64, start_time: Instant) {
    if is_prime(n) {
        report_prime(start_time.elapsed());
    }
}
</pre></div>

<div class="example">
<pre class="example-preformatted">fn report_prime(elapsed_time: Duration) {
    print!(&quot; *** &quot;);
    print!(&quot;{:?}&quot;, elapsed_time);
}
</pre></div>

<p>Using this procedure, write a procedure <code class="code">search_for_primes</code> that checks
the primality of consecutive odd integers in a specified range.  Use your
procedure to find the three smallest primes larger than 1000; larger than
10,000; larger than 100,000; larger than 1,000,000.  Note the time needed to
test each prime.  Since the testing algorithm has order of growth of
<em class="math">{\Theta(\sqrt{n})}</em>, you should expect that testing for primes
around 10,000 should take about <em class="math">\sqrt{10}</em> times as long as testing for
primes around 1000.  Do your timing data bear this out?  How well do the data
for 100,000 and 1,000,000 support the <em class="math">{\Theta(\sqrt{n})}</em> prediction?  Is your
result compatible with the notion that programs on your machine run in time
proportional to the number of steps required for the computation?
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e23"></a>Exercise 1.23:</strong> The <code class="code">smallest_divisor</code>
procedure shown at the start of this section does lots of needless testing:
After it checks to see if the number is divisible by 2 there is no point in
checking to see if it is divisible by any larger even numbers.  This suggests
that the values used for <code class="code">test-divisor</code> should not be 2, 3, 4, 5, 6,
&#8230;, but rather 2, 3, 5, 7, 9, &#8230;.  To implement this change, define a
procedure <code class="code">next</code> that returns 3 if its input is equal to 2 and otherwise
returns its input plus 2.  Modify the <code class="code">smallest_divisor</code> procedure to use
<code class="code">(next test-divisor)</code> instead of <code class="code">(+ test-divisor 1)</code>.  With
<code class="code">timed_prime_test</code> incorporating this modified version of
<code class="code">smallest_divisor</code>, run the test for each of the 12 primes found in
<a class="ref" href="#Exercise-1_002e22">Exercise 1.22</a>.  Since this modification halves the number of test steps,
you should expect it to run about twice as fast.  Is this expectation
confirmed?  If not, what is the observed ratio of the speeds of the two
algorithms, and how do you explain the fact that it is different from 2?
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e24"></a>Exercise 1.24:</strong> Modify the
<code class="code">timed_prime_test</code> procedure of <a class="ref" href="#Exercise-1_002e22">Exercise 1.22</a> to use
<code class="code">fast_prime</code> (the Fermat method), and test each of the 12 primes you
found in that exercise.  Since the Fermat test has <em class="math">{\Theta(\log n)}</em>
growth, how would you expect the time to test primes near 1,000,000 to
compare with the time needed to test primes near 1000?  Do your data bear this
out?  Can you explain any discrepancy you find?
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e25"></a>Exercise 1.25:</strong> Alyssa P. Hacker complains that
we went to a lot of extra work in writing <code class="code">expmod</code>.  After all, she says,
since we already know how to compute exponentials, we could have simply written
</p>
<div class="example">
<pre class="example-preformatted">fn expmod(base: u64, exp: u64, m: u64) -&gt; u64 {
    fast_expt(base, exp) % m
}
</pre></div>

<p>Is she correct?  Would this procedure serve as well for our fast prime tester?
Explain.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e26"></a>Exercise 1.26:</strong> Louis Reasoner is having great
difficulty doing <a class="ref" href="#Exercise-1_002e24">Exercise 1.24</a>.  His <code class="code">fast_prime</code> test seems to run
more slowly than his <code class="code">is_prime</code> test.  Louis calls his friend Eva Lu Ator
over to help.  When they examine Louis&#8217;s code, they find that he has rewritten
the <code class="code">expmod</code> procedure to use an explicit multiplication, rather than
calling <code class="code">square</code>:
</p>
<div class="example">
<pre class="example-preformatted">fn expmod(base: u64, exp: u64, m: u64) -&gt; u64 {
    if exp == 0 {
        1
    } else if is_even(exp) {
        (expmod(base, exp / 2, m) * expmod(base, exp / 2, m)) % m
    } else {
        (base * expmod(base, exp - 1, m)) % m
    }
}
</pre></div>

<p>&#8220;I don&#8217;t see what difference that could make,&#8221; says Louis.  &#8220;I&#160;do.&#8221;<!-- /@w -->  says
Eva.  &#8220;By writing the procedure like that, you have transformed the
<em class="math">{\Theta(\log n)}</em> process into a <em class="math">{\Theta(n)}</em> process.&#8221;
Explain.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e27"></a>Exercise 1.27:</strong> Demonstrate that the Carmichael
numbers listed in <a class="ref" href="#Footnote-47">Footnote 47</a> really do fool the Fermat test.  That is,
write a procedure that takes an integer <em class="math">n</em> and tests whether <em class="math">a^n</em> is
congruent to <em class="math">a</em> modulo <em class="math">n</em> for every <em class="math">{a &lt; n}</em>, and try your procedure
on the given Carmichael numbers.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-1_002e28"></a>Exercise 1.28:</strong> One variant of the Fermat test
that cannot be fooled is called the <a class="index-entry-id" id="index-Miller_002dRabin-test"></a>
<em class="dfn">Miller-Rabin test</em> (<a class="ref" href="References.xhtml#Miller-1976">Miller 1976</a>;
<a class="ref" href="References.xhtml#Rabin-1980">Rabin 1980</a>).  This starts from an alternate form of Fermat&#8217;s Little Theorem,
which states that if <em class="math">n</em> is a prime number and <em class="math">a</em> is any positive integer
less than <em class="math">n</em>, then <em class="math">a</em> raised to the <em class="math">{(n-1)}</em>-st power is congruent to 1
modulo <em class="math">n</em>.  To test the primality of a number <em class="math">n</em> by the Miller-Rabin
test, we pick a random number <em class="math">{a &lt; n}</em> and raise <em class="math">a</em> to the <em class="math">{(n-1)}</em>-st
power modulo <em class="math">n</em> using the <code class="code">expmod</code> procedure.  However, whenever we
perform the squaring step in <code class="code">expmod</code>, we check to see if we have
discovered a &#8220;nontrivial square root of 1 modulo <em class="math">n</em>,&#8221; that is, a number
not equal to 1 or <em class="math">{n-1}</em> whose square is equal to 1 modulo <em class="math">n</em>.  It is
possible to prove that if such a nontrivial square root of 1 exists, then <em class="math">n</em>
is not prime.  It is also possible to prove that if <em class="math">n</em> is an odd number that
is not prime, then, for at least half the numbers <em class="math">{a &lt; n}</em>, computing
<em class="math">a^{n-1}</em> in this way will reveal a nontrivial square root of 1 modulo
<em class="math">n</em>.  (This is why the Miller-Rabin test cannot be fooled.)  Modify the
<code class="code">expmod</code> procedure to signal if it discovers a nontrivial square root of
1, and use this to implement the Miller-Rabin test with a procedure analogous
to <code class="code">fermat-test</code>.  Check your procedure by testing various known primes
and non-primes.  Hint: One convenient way to make <code class="code">expmod</code> signal is to
have it return 0.
</p></blockquote>

</div>
</div>
<div class="footnotes-segment">
<hr />
<h4 class="footnotes-heading">Footnotes</h4>

<h5 class="footnote-body-heading"><a id="FOOT24" href="#DOCF24">(24)</a></h5>
<p>In a real program we would probably use the block
structure introduced in the last section to hide the definition of
<code class="code">fact_iter</code>:
</p>
<div class="example">
<pre class="example-preformatted">fn factorial(n: u64) -&gt; u64 {
    fn iter(product: u64, counter: u64, max_count: u64) -&gt; u64 {
        if counter &gt; max_count {
            product
        } else {
            iter(counter * product, counter + 1, max_count)
        }
    }
    iter(1, 1, n)
}
</pre></div>

<p>We avoided doing this here so as to minimize the number of things to think
about at once.</p>
<h5 class="footnote-body-heading"><a id="FOOT25" href="#DOCF25">(25)</a></h5>
<p>When we discuss the implementation of procedures on
register machines in <a class="ref" href="Chapter-5.xhtml">Computing with Register Machines</a>, we will see that any iterative process
can be realized &#8220;in hardware&#8221; as a machine that has a fixed set of registers
and no auxiliary memory.  In contrast, realizing a recursive process requires a
machine that uses an auxiliary data structure known as a <a class="index-entry-id" id="index-stack"></a>
<em class="dfn">stack</em>.</p>
<h5 class="footnote-body-heading"><a id="FOOT26" href="#DOCF26">(26)</a></h5>
<p>Tail
recursion has long been known as a compiler optimization trick.  A coherent
semantic basis for tail recursion was provided by Carl <a class="ref" href="References.xhtml#Hewitt-_00281977_0029">Hewitt (1977)</a>, who
explained it in terms of the &#8220;message-passing&#8221; model of computation that we
shall discuss in <a class="ref" href="Chapter-3.xhtml">Modularity, Objects, and State</a>.  Inspired by this, Gerald Jay Sussman and Guy
Lewis Steele Jr. (see <a class="ref" href="References.xhtml#Steele-and-Sussman-1975">Steele and Sussman 1975</a>) constructed a tail-recursive interpreter for
Scheme.  Steele later showed how tail recursion is a consequence of the natural
way to compile procedure calls (<a class="ref" href="References.xhtml#Steele-1977">Steele 1977</a>).  The <abbr class="abbr">IEEE</abbr> standard for
Scheme requires that Scheme implementations be tail-recursive.</p>
<h5 class="footnote-body-heading"><a id="FOOT27" href="#DOCF27">(27)</a></h5>
<p>An example of this was hinted at in <a class="ref" href="1_002e1.xhtml#g_t1_002e1_002e3">Evaluating Combinations</a>. The
interpreter itself evaluates expressions using a tree-recursive process.</p>
<h5 class="footnote-body-heading"><a id="FOOT28" href="#DOCF28">(28)</a></h5>
<p>For
example, work through in detail how the reduction rule applies to the problem
of making change for 10 cents using pennies and nickels.</p>
<h5 class="footnote-body-heading"><a id="FOOT29" href="#DOCF29">(29)</a></h5>
<p>One approach to coping with
redundant computations is to arrange matters so that we automatically construct
a table of values as they are computed.  Each time we are asked to apply the
procedure to some argument, we first look to see if the value is already stored
in the table, in which case we avoid performing the redundant computation.
This strategy, known as <a class="index-entry-id" id="index-tabulation"></a>
<em class="dfn">tabulation</em> or <a class="index-entry-id" id="index-memoization"></a>
<em class="dfn">memoization</em>, can be
implemented in a straightforward way.  Tabulation can sometimes be used to
transform processes that require an exponential number of steps (such as
<code class="code">count_change</code>) into processes whose space and time requirements grow
linearly with the input.  See <a class="ref" href="3_002e3.xhtml#Exercise-3_002e27">Exercise 3.27</a>.</p>
<h5 class="footnote-body-heading"><a id="FOOT30" href="#DOCF30">(30)</a></h5>
<p>The elements of
Pascal&#8217;s triangle are called the <a class="index-entry-id" id="index-binomial-coefficients"></a>
<em class="dfn">binomial coefficients</em>, because the
<em class="math">{n^{\text{th}}}</em> row consists of the coefficients of the terms in the expansion of
<em class="math">{(x + y)^n}</em>.  This pattern for computing the coefficients appeared in
Blaise Pascal&#8217;s 1653 seminal work on probability theory, <cite class="cite">Trait&#233; du
triangle arithm&#233;tique</cite>.  According to <a class="ref" href="References.xhtml#Knuth-_00281973_0029">Knuth (1973)</a>, the same pattern appears
in the <cite class="cite">Szu-yuen Y&#252;-chien</cite> (&#8220;The Precious Mirror of the Four
Elements&#8221;), published by the Chinese mathematician Chu Shih-chieh in 1303, in
the works of the twelfth-century Persian poet and mathematician Omar Khayyam,
and in the works of the twelfth-century Hindu mathematician Bh&#225;scara
&#193;ch&#225;rya.</p>
<h5 class="footnote-body-heading"><a id="FOOT31" href="#DOCF31">(31)</a></h5>
<p>These statements mask a great deal of oversimplification.
For instance, if we count process steps as &#8220;machine operations&#8221; we are making
the assumption that the number of machine operations needed to perform, say, a
multiplication is independent of the size of the numbers to be multiplied,
which is false if the numbers are sufficiently large.  Similar remarks hold for
the estimates of space.  Like the design and description of a process, the
analysis of a process can be carried out at various levels of abstraction.</p>
<h5 class="footnote-body-heading"><a id="FOOT32" href="#DOCF32">(32)</a></h5>
<p>More precisely, the number of
multiplications required is equal to 1 less than the log base 2 of <em class="math">n</em> plus
the number of ones in the binary representation of <em class="math">n</em>.  This total is always
less than twice the log base 2 of <em class="math">n</em>.  The arbitrary constants <em class="math">k_1</em> and
<em class="math">k_2</em> in the definition of order notation imply that, for a logarithmic
process, the base to which logarithms are taken does not matter, so all such
processes are described as <em class="math">{\Theta(\log n)}</em>.</p>
<h5 class="footnote-body-heading"><a id="FOOT33" href="#DOCF33">(33)</a></h5>
<p>You may wonder why anyone would care about raising
numbers to the 1000th power.  See <a class="ref" href="#g_t1_002e2_002e6">Example: Testing for Primality</a>.</p>
<h5 class="footnote-body-heading"><a id="FOOT34" href="#DOCF34">(34)</a></h5>
<p>This
iterative algorithm is ancient.  It appears in the <cite class="cite">Chandah-sutra</cite> by
&#193;ch&#225;rya Pingala, written before 200 <abbr class="abbr">B.C.</abbr> See <a class="ref" href="References.xhtml#Knuth-1981">Knuth 1981</a>, section
4.6.3, for a full discussion and analysis of this and other methods of
exponentiation.</p>
<h5 class="footnote-body-heading"><a id="FOOT35" href="#DOCF35">(35)</a></h5>
<p>This algorithm,
which is sometimes known as the &#8220;Russian peasant method&#8221; of multiplication,
is ancient.  Examples of its use are found in the Rhind Papyrus, one of the two
oldest mathematical documents in existence, written about 1700 <abbr class="abbr">B.C.</abbr>
(and copied from an even older document) by an Egyptian scribe named A&#8217;h-mose.</p>
<h5 class="footnote-body-heading"><a id="FOOT36" href="#DOCF36">(36)</a></h5>
<p>This exercise was suggested to
us by Joe Stoy, based on an example in <a class="ref" href="References.xhtml#Kaldewaij-1990">Kaldewaij 1990</a>.</p>
<h5 class="footnote-body-heading"><a id="FOOT37" href="#DOCF37">(37)</a></h5>
<p>Euclid&#8217;s Algorithm is so called because 
it appears in Euclid&#8217;s <cite class="cite">Elements</cite> (Book 7, ca. 300 <abbr class="abbr">B.C.</abbr>).  
According to <a class="ref" href="References.xhtml#Knuth-_00281973_0029">Knuth (1973)</a>, it can be considered the oldest known nontrivial 
algorithm.  The ancient Egyptian method of multiplication (<a class="ref" href="#Exercise-1_002e18">Exercise 1.18</a>) is 
surely older, but, as Knuth explains, Euclid&#8217;s algorithm is the oldest known to 
have been presented as a general algorithm, rather than as a set of illustrative
examples.</p>
<h5 class="footnote-body-heading"><a id="FOOT38" href="#DOCF38">(38)</a></h5>
<p>This
theorem was proved in 1845 by Gabriel Lam&#233;, a French mathematician and
engineer known chiefly for his contributions to mathematical physics.  To prove
the theorem, we consider pairs <em class="math">{(a_k, b_k)}</em>, where <em class="math">{a_k \ge b_k}</em>, 
for which Euclid&#8217;s Algorithm terminates in <em class="math">k</em> steps.  The proof
is based on the claim that, if <em class="math">{(a_{k+1}, b_{k+1})} \to {(a_k, b_k)} \to {(a_{k-1}, b_{k-1})}</em> 
are three successive pairs 
in the reduction process, then we must have <em class="math">b_{k+1} \ge b_k + b_{k-1}</em>.  
To verify the claim, consider that a reduction step is defined by applying the 
transformation <em class="math">{a_{k-1} = b_k}</em>, <em class="math">{b_{k-1} =}</em> remainder of <em class="math">a_k</em> 
divided by <em class="math">b_k</em>.  The second equation means that <em class="math">a_k = {qb_k} + {b_{k-1}}</em> 
for some positive integer <em class="math">q</em>.  And since <em class="math">q</em> must be at least 1 we have 
<em class="math">a_k = {qb_k} + b_{k-1} \ge b_k + b_{k-1}</em>.  But in the previous reduction 
step we have <em class="math">b_{k+1} = a_k</em>.  Therefore, <em class="math">b_{k+1} = a_k \ge b_k + b_{k-1}</em>.
This verifies the claim.  Now we can prove the theorem by induction on <em class="math">k</em>, 
the number of steps that the algorithm requires to terminate.  The result is true for 
<em class="math">{k = 1}</em>, since this merely requires that <em class="math">b</em> be at least as large as <em class="math">{\text{Fib}(1) = 1}</em>.  
Now, assume that the result is true for all integers less than or equal
to <em class="math">k</em> and establish the result for <em class="math">{k + 1}</em>.  
Let <em class="math">{(a_{k+1}, b_{k+1})} \to {(a_k, b_k)} \to {(a_{k-1}, b_{k-1})}</em> 
be successive pairs in the reduction 
process.  By our induction hypotheses, we have <em class="math">b_{k-1} \ge {\text{Fib}(k - 1)}</em> 
and <em class="math">b_k \ge {\text{Fib}(k)}</em>.  Thus, applying the claim we just proved together with 
the definition of the Fibonacci numbers gives 
<em class="math">b_{k+1} \ge b_k + b_{k-1} \ge {\text{Fib}(k)} + {\text{Fib}(k-1)} = {\text{Fib}(k+1)}</em>, 
which completes the proof of Lam&#233;&#8217;s Theorem.</p>
<h5 class="footnote-body-heading"><a id="FOOT39" href="#DOCF39">(39)</a></h5>
<p>If <em class="math">d</em> is a divisor of <em class="math">n</em>, then so is
<em class="math">{n \,/\, d}</em>.  But <em class="math">d</em> and <em class="math">{n \,/\, d}</em> cannot both be greater than
<em class="math">\sqrt{n}</em>.</p>
<h5 class="footnote-body-heading"><a id="FOOT40" href="#DOCF40">(40)</a></h5>
<p>Pierre de Fermat
(1601-1665) is considered to be the founder of modern number theory.  He
obtained many important number-theoretic results, but he usually announced just
the results, without providing his proofs.  Fermat&#8217;s Little Theorem was stated
in a letter he wrote in 1640.  The first published proof was given by Euler in
1736 (and an earlier, identical proof was discovered in the unpublished
manuscripts of Leibniz).  The most famous of Fermat&#8217;s results&#8212;known as
Fermat&#8217;s Last Theorem&#8212;was jotted down in 1637 in his copy of the book
<cite class="cite">Arithmetic</cite> (by the third-century Greek mathematician Diophantus) with
the remark &#8220;I have discovered a truly remarkable proof, but this margin is too
small to contain it.&#8221;  Finding a proof of Fermat&#8217;s Last Theorem became one of
the most famous challenges in number theory.  A complete solution was finally
given in 1995 by Andrew Wiles of Princeton University.</p>
<h5 class="footnote-body-heading"><a id="FOOT41" href="#DOCF41">(41)</a></h5>
<p>The reduction steps in the cases where the exponent
<em class="math">e</em> is greater than 1 are based on the fact that, for any integers <em class="math">x</em>,
<em class="math">y</em>, and <em class="math">m</em>, we can find the remainder of <em class="math">x</em> times <em class="math">y</em> modulo <em class="math">m</em>
by computing separately the remainders of <em class="math">x</em> modulo <em class="math">m</em> and <em class="math">y</em> modulo
<em class="math">m</em>, multiplying these, and then taking the remainder of the result modulo
<em class="math">m</em>.  For instance, in the case where <em class="math">e</em> is even, we compute the remainder
of <em class="math">b^{e / 2}</em> modulo <em class="math">m</em>, square this, and take the remainder modulo
<em class="math">m</em>.  This technique is useful because it means we can perform our
computation without ever having to deal with numbers much larger than <em class="math">m</em>.
(Compare <a class="ref" href="#Exercise-1_002e25">Exercise 1.25</a>.)</p>
<h5 class="footnote-body-heading"><a id="FOOT42" href="#DOCF42">(42)</a></h5>
<a class="anchor" id="Footnote-47"></a><p>Numbers
that fool the Fermat test are called <a class="index-entry-id" id="index-Carmichael-numbers"></a>
<em class="dfn">Carmichael numbers</em>, and little
is known about them other than that they are extremely rare.  There are 255
Carmichael numbers below 100,000,000.  The smallest few are 561, 1105, 1729,
2465, 2821, and 6601.  In testing primality of very large numbers chosen at
random, the chance of stumbling upon a value that fools the Fermat test is less
than the chance that cosmic radiation will cause the computer to make an error
in carrying out a &#8220;correct&#8221; algorithm.  Considering an algorithm to be
inadequate for the first reason but not for the second illustrates the
difference between mathematics and engineering.</p>
<h5 class="footnote-body-heading"><a id="FOOT43" href="#DOCF43">(43)</a></h5>
<p>One of the most striking
applications of probabilistic prime testing has been to the field of
cryptography.  Although it is now computationally infeasible to factor an
arbitrary 200-digit number, the primality of such a number can be checked in a
few seconds with the Fermat test.  This fact forms the basis of a technique for
constructing &#8220;unbreakable codes&#8221; suggested by <a class="ref" href="References.xhtml#Rivest-et-al_002e-_00281977_0029">Rivest et al. (1977)</a>.  
The resulting <a class="index-entry-id" id="index-RSA-algorithm"></a>
<em class="dfn">RSA algorithm</em> has become a widely used
technique for enhancing the security of electronic communications.  Because of
this and related developments, the study of prime numbers, once considered the
epitome of a topic in &#8220;pure&#8221; mathematics to be studied only for its own sake,
now turns out to have important practical applications to cryptography,
electronic funds transfer, and information retrieval.</p>
</div>
<hr />
<div class="nav-panel">
<p>
Next: <a href="1_002e3.xhtml#g_t1_002e3" accesskey="n" rel="next">Formulating Abstractions with Higher-Order Procedures</a>, Previous: <a href="1_002e1.xhtml#g_t1_002e1" accesskey="p" rel="prev">The Elements of Programming</a>, Up: <a href="Chapter-1.xhtml" accesskey="u" rel="up">Building Abstractions with Procedures</a> &#160; [<a href="index.xhtml#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="Term-Index.xhtml" title="Index" rel="index">Index</a>]</p>
</div>



</body>
</html>
