<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<!-- Created by GNU Texinfo 7.1, https://www.gnu.org/software/texinfo/ -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>3.4 (Structure and Interpretation of Computer Programs, 2e)</title>

<meta name="description" content="3.4 (Structure and Interpretation of Computer Programs, 2e)" />
<meta name="keywords" content="3.4 (Structure and Interpretation of Computer Programs, 2e)" />
<meta name="resource-type" content="document" />
<meta name="distribution" content="global" />
<meta name="Generator" content="texi2any" />
<meta name="viewport" content="width=device-width,initial-scale=1" />

<link href="index.xhtml" rel="start" title="Top" />
<link href="Term-Index.xhtml" rel="index" title="Term Index" />
<link href="index.xhtml#SEC_Contents" rel="contents" title="Table of Contents" />
<link href="Chapter-3.xhtml" rel="up" title="Chapter 3" />
<link href="3_002e5.xhtml#g_t3_002e5" rel="next" title="3.5" />
<link href="3_002e3.xhtml#g_t3_002e3" rel="prev" title="3.3" />
<style type="text/css">
<!--
a.copiable-link {visibility: hidden; text-decoration: none; line-height: 0em}
div.example {margin-left: 3.2em}
span:hover a.copiable-link {visibility: visible}
ul.mark-bullet {list-style-type: disc}
-->
</style>
<link href="css/style.css" rel="stylesheet" type="text/css" />
<link href="css/prettify.css" rel="stylesheet" type="text/css" />
<script src="js/highlight/prettify.js" type="text/javascript"></script>
<script src="js/highlight/lang-lisp.js" type="text/javascript"></script>
<script src="js/highlight/lang-rust.js" type="text/javascript"></script>
</head>

<body lang="en">
<div class="section-level-extent" id="g_t3_002e4">
<div class="nav-panel">
<p>
Next: <a href="3_002e5.xhtml#g_t3_002e5" accesskey="n" rel="next">Streams</a>, Previous: <a href="3_002e3.xhtml#g_t3_002e3" accesskey="p" rel="prev">Modeling with Mutable Data</a>, Up: <a href="Chapter-3.xhtml" accesskey="u" rel="up">Modularity, Objects, and State</a> &#160; [<a href="index.xhtml#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="Term-Index.xhtml" title="Index" rel="index">Index</a>]</p>
</div>
<h3 class="section" id="Concurrency_003a-Time-Is-of-the-Essence"><span>3.4 Concurrency: Time Is of the Essence<a class="copiable-link" href="#Concurrency_003a-Time-Is-of-the-Essence"> &#182;</a></span></h3>

<p>We&#8217;ve seen the power of computational objects with local state as tools for
modeling.  Yet, as <a class="ref" href="3_002e1.xhtml#g_t3_002e1_002e3">The Costs of Introducing Assignment</a> warned, this power extracts a price: the
loss of referential transparency, giving rise to a thicket of questions about
sameness and change, and the need to abandon the substitution model of
evaluation in favor of the more intricate environment model.
</p>
<p>The central issue lurking beneath the complexity of state, sameness, and change
is that by introducing assignment we are forced to admit <a class="index-entry-id" id="index-time"></a>
<em class="dfn">time</em> into
our computational models.  Before we introduced assignment, all our programs
were timeless, in the sense that any expression that has a value always has the
same value.  In contrast, recall the example of modeling withdrawals from a
bank account and returning the resulting balance, introduced at the beginning
of <a class="ref" href="3_002e1.xhtml#g_t3_002e1_002e1">Local State Variables</a>:
</p>
<div class="example">
<pre class="example-preformatted">withdraw(25);
// =&gt; 75

withdraw(25);
// =&gt; 50
</pre></div>

<p>Here successive evaluations of the same expression yield different values.
This behavior arises from the fact that the execution of assignment statements
(in this case, assignments to the variable <code class="code">balance</code>) delineates
<a class="index-entry-id" id="index-moments-in-time"></a>
<em class="dfn">moments in time</em> when values change.  The result of evaluating an
expression depends not only on the expression itself, but also on whether the
evaluation occurs before or after these moments.  Building models in terms of
computational objects with local state forces us to confront time as an
essential concept in programming.
</p>
<p>We can go further in structuring computational models to match our perception
of the physical world.  Objects in the world do not change one at a time in
sequence.  Rather we perceive them as acting <a class="index-entry-id" id="index-concurrently"></a>
<em class="dfn">concurrently</em>&#8212;all at
once.  So it is often natural to model systems as collections of computational
processes that execute concurrently.  Just as we can make our programs modular
by organizing models in terms of objects with separate local state, it is often
appropriate to divide computational models into parts that evolve separately
and concurrently.  Even if the programs are to be executed on a sequential
computer, the practice of writing programs as if they were to be executed
concurrently forces the programmer to avoid inessential timing constraints and
thus makes programs more modular.
</p>
<p>In addition to making programs more modular, concurrent computation can provide
a speed advantage over sequential computation.  Sequential computers execute
only one operation at a time, so the amount of time it takes to perform a task
is proportional to the total number of operations performed.<a class="footnote" id="DOCF153" href="#FOOT153"><sup>153</sup></a>  However, if it is possible to decompose a problem
into pieces that are relatively independent and need to communicate only
rarely, it may be possible to allocate pieces to separate computing processors,
producing a speed advantage proportional to the number of processors available.
</p>
<p>Unfortunately, the complexities introduced by assignment become even more
problematic in the presence of concurrency.  The fact of concurrent execution,
either because the world operates in parallel or because our computers do,
entails additional complexity in our understanding of time.
</p>

<hr />
<div class="subsection-level-extent" id="g_t3_002e4_002e1">
<h4 class="subsection" id="The-Nature-of-Time-in-Concurrent-Systems"><span>3.4.1 The Nature of Time in Concurrent Systems<a class="copiable-link" href="#The-Nature-of-Time-in-Concurrent-Systems"> &#182;</a></span></h4>

<p>On the surface, time seems straightforward.  It is an ordering imposed on
events.<a class="footnote" id="DOCF154" href="#FOOT154"><sup>154</sup></a>  For any events <em class="math">A</em> and <em class="math">B</em>, either <em class="math">A</em> occurs before <em class="math">B</em>,
<em class="math">A</em> and <em class="math">B</em> are simultaneous, or <em class="math">A</em> occurs after <em class="math">B</em>.  For instance,
returning to the bank account example, suppose that Peter withdraws $10 and
Paul withdraws $25 from a joint account that initially contains $100, leaving
$65 in the account.  Depending on the order of the two withdrawals, the
sequence of balances in the account is either $100 <em class="math">→</em> $90 <em class="math">→</em> $65 or $100 <em class="math">→</em> $75
<em class="math">→</em> $65.  In a computer implementation of the banking system, this changing
sequence of balances could be modeled by successive assignments to a variable
<code class="code">balance</code>.
</p>
<p>In complex situations, however, such a view can be problematic.  Suppose that
Peter and Paul, and other people besides, are accessing the same bank account
through a network of banking machines distributed all over the world.  The
actual sequence of balances in the account will depend critically on the
detailed timing of the accesses and the details of the communication among the
machines.
</p>
<p>This indeterminacy in the order of events can pose serious problems in the
design of concurrent systems.  For instance, suppose that the withdrawals made
by Peter and Paul are implemented as two separate processes sharing a common
variable <code class="code">balance</code>, each process specified by the procedure given in
<a class="ref" href="3_002e1.xhtml#g_t3_002e1_002e1">Local State Variables</a>:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::Mutex;

static BALANCE: Mutex&lt;i64&gt; = Mutex::new(100);

fn withdraw(amount: i64) -&gt; Result&lt;i64, &amp;'static str&gt; {
    let mut balance = BALANCE.lock().unwrap();
    if *balance &gt;= amount {
        *balance -= amount;
        Ok(*balance)
    } else {
        Err(&quot;Insufficient funds&quot;)
    }
}
</pre></div>

<p>If the two processes operate independently, then Peter might test the
balance and attempt to withdraw a legitimate amount.  However, Paul
might withdraw some funds in between the time that Peter checks the
balance and the time Peter completes the withdrawal, thus invalidating
Peter&#8217;s test.
</p>
<p>Things can be worse still.  Consider the expression
</p>
<div class="example">
<pre class="example-preformatted">*balance -= amount;
</pre></div>

<p>executed as part of each withdrawal process.  This consists of three steps: (1)
accessing the value of the <code class="code">balance</code> variable; (2) computing the new
balance; (3) setting <code class="code">balance</code> to this new value.  If Peter and Paul&#8217;s
withdrawals execute this statement concurrently, then the two withdrawals might
interleave the order in which they access <code class="code">balance</code> and set it to the new
value.
</p>
<p>The timing diagram in <a class="ref" href="#Figure-3_002e29">Figure 3.29</a> depicts an order of events where
<code class="code">balance</code> starts at 100, Peter withdraws 10, Paul withdraws 25, and yet
the final value of <code class="code">balance</code> is 75.  As shown in the diagram, the reason
for this anomaly is that Paul&#8217;s assignment of 75 to <code class="code">balance</code> is made
under the assumption that the value of <code class="code">balance</code> to be decremented is 100.
That assumption, however, became invalid when Peter changed <code class="code">balance</code> to
90.  This is a catastrophic failure for the banking system, because the total
amount of money in the system is not conserved.  Before the transactions, the
total amount of money was $100.  Afterwards, Peter has $10, Paul has $25, and
the bank has $75.<a class="footnote" id="DOCF155" href="#FOOT155"><sup>155</sup></a>
</p>
<div class="float">
<a class="anchor" id="Figure-3_002e29"></a><img class="image" src="fig/chap3/Fig3.29b.std.svg" alt="fig/chap3/Fig3.29b" />
<div class="caption"><p><strong class="strong">Figure 3.29:</strong> Timing diagram showing how interleaving the order of events in two banking withdrawals can lead to an incorrect final balance.</p></div></div>
<p>The general phenomenon illustrated here is that several processes may share a
common state variable.  What makes this complicated is that more than one
process may be trying to manipulate the shared state at the same time.  For the
bank account example, during each transaction, each customer should be able to
act as if the other customers did not exist.  When a customer changes the
balance in a way that depends on the balance, he must be able to assume that,
just before the moment of change, the balance is still what he thought it was.
</p>
<h4 class="subsubheading" id="Correct-behavior-of-concurrent-programs"><span>Correct behavior of concurrent programs<a class="copiable-link" href="#Correct-behavior-of-concurrent-programs"> &#182;</a></span></h4>

<p>The above example typifies the subtle bugs that can creep into concurrent
programs.  The root of this complexity lies in the assignments to variables
that are shared among the different processes.  We already know that we must be
careful in writing programs that use <code class="code">set!</code>, because the results of a
computation depend on the order in which the assignments occur.<a class="footnote" id="DOCF156" href="#FOOT156"><sup>156</sup></a>  With concurrent processes we must be especially careful
about assignments, because we may not be able to control the order of the
assignments made by the different processes.  If several such changes might be
made concurrently (as with two depositors accessing a joint account) we need
some way to ensure that our system behaves correctly.  For example, in the case
of withdrawals from a joint bank account, we must ensure that money is
conserved.  To make concurrent programs behave correctly, we may have to place
some restrictions on concurrent execution.
</p>
<p>One possible restriction on concurrency would stipulate that no two operations
that change any shared state variables can occur at the same time.  This is an
extremely stringent requirement.  For distributed banking, it would require the
system designer to ensure that only one transaction could proceed at a time.
This would be both inefficient and overly conservative.  <a class="ref" href="#Figure-3_002e30">Figure 3.30</a>
shows Peter and Paul sharing a bank account, where Paul has a private account
as well.  The diagram illustrates two withdrawals from the shared account (one
by Peter and one by Paul) and a deposit to Paul&#8217;s private account.<a class="footnote" id="DOCF157" href="#FOOT157"><sup>157</sup></a>  The two withdrawals from the
shared account must not be concurrent (since both access and update the same
account), and Paul&#8217;s deposit and withdrawal must not be concurrent (since both
access and update the amount in Paul&#8217;s wallet).  But there should be no problem
permitting Paul&#8217;s deposit to his private account to proceed concurrently with
Peter&#8217;s withdrawal from the shared account.
</p>
<div class="float">
<a class="anchor" id="Figure-3_002e30"></a><img class="image" src="fig/chap3/Fig3.30c.std.svg" alt="fig/chap3/Fig3.30c" />
<div class="caption"><p><strong class="strong">Figure 3.30:</strong> Concurrent deposits and withdrawals from a joint account in Bank1 and a private account in Bank2.</p></div></div>
<p>A less stringent restriction on concurrency would ensure that a concurrent
system produces the same result as if the processes had run sequentially in
some order.  There are two important aspects to this requirement.  First, it
does not require the processes to actually run sequentially, but only to
produce results that are the same <em class="emph">as if</em> they had run sequentially.  For
the example in <a class="ref" href="#Figure-3_002e30">Figure 3.30</a>, the designer of the bank account system can
safely allow Paul&#8217;s deposit and Peter&#8217;s withdrawal to happen concurrently,
because the net result will be the same as if the two operations had happened
sequentially.  Second, there may be more than one possible &#8220;correct&#8221; result
produced by a concurrent program, because we require only that the result be
the same as for <em class="emph">some</em> sequential order.  For example, suppose that Peter
and Paul&#8217;s joint account starts out with $100, and Peter deposits $40 while
Paul concurrently withdraws half the money in the account.  Then sequential
execution could result in the account balance being either $70 or $90 (see
<a class="ref" href="#Exercise-3_002e38">Exercise 3.38</a>).<a class="footnote" id="DOCF158" href="#FOOT158"><sup>158</sup></a>
</p>
<p>There are still weaker requirements for correct execution of concurrent
programs.  A program for simulating diffusion (say, the flow of heat in an
object) might consist of a large number of processes, each one representing a
small volume of space, that update their values concurrently.  Each process
repeatedly changes its value to the average of its own value and its neighbors&#8217;
values.  This algorithm converges to the right answer independent of the order
in which the operations are done; there is no need for any restrictions on
concurrent use of the shared values.
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e38"></a>Exercise 3.38:</strong> Suppose that Peter, Paul, and
Mary share a joint bank account that initially contains $100.  Concurrently,
Peter deposits $10, Paul withdraws $20, and Mary withdraws half the money in
the account, by executing the following commands:
</p>
<div class="example">
<pre class="example-preformatted">// Using AtomicI64 for shared mutable state:
Peter: balance.fetch_add(10, Ordering::SeqCst);
Paul:  balance.fetch_sub(20, Ordering::SeqCst);
Mary:  let cur = balance.load(Ordering::SeqCst);
       balance.store(cur - cur / 2, Ordering::SeqCst);
</pre></div>

<ol class="enumerate" type="a" start="1">
<li> List all the different possible values for <code class="code">balance</code> after these three
transactions have been completed, assuming that the banking system forces the
three processes to run sequentially in some order.

</li><li> What are some other values that could be produced if the system allows the
processes to be interleaved?  Draw timing diagrams like the one in <a class="ref" href="#Figure-3_002e29">Figure 3.29</a>
to explain how these values can occur.

</li></ol>
</blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t3_002e4_002e2">
<h4 class="subsection" id="Mechanisms-for-Controlling-Concurrency"><span>3.4.2 Mechanisms for Controlling Concurrency<a class="copiable-link" href="#Mechanisms-for-Controlling-Concurrency"> &#182;</a></span></h4>

<p>We&#8217;ve seen that the difficulty in dealing with concurrent processes is rooted
in the need to consider the interleaving of the order of events in the
different processes.  For example, suppose we have two processes, one with
three ordered events <em class="math">{(a, b, c)}</em> and one with three ordered events
<em class="math">{(x, y, z)}</em>.  If the two processes run concurrently, with no
constraints on how their execution is interleaved, then there are 20 different
possible orderings for the events that are consistent with the individual
orderings for the two processes:
</p>
<div class="example">
<pre class="example-preformatted">(a,b,c,x,y,z)  (a,x,b,y,c,z)  (x,a,b,c,y,z)
(x,a,y,z,b,c)  (a,b,x,c,y,z)  (a,x,b,y,z,c)
(x,a,b,y,c,z)  (x,y,a,b,c,z)  (a,b,x,y,c,z)
(a,x,y,b,c,z)  (x,a,b,y,z,c)  (x,y,a,b,z,c)
(a,b,x,y,z,c)  (a,x,y,b,z,c)  (x,a,y,b,c,z)
(x,y,a,z,b,c)  (a,x,b,c,y,z)  (a,x,y,z,b,c)
(x,a,y,b,z,c)  (x,y,z,a,b,c)
</pre></div>

<p>As programmers designing this system, we would have to consider the effects of
each of these 20 orderings and check that each behavior is acceptable.  Such an
approach rapidly becomes unwieldy as the numbers of processes and events
increase.
</p>
<p>A more practical approach to the design of concurrent systems is to devise
general mechanisms that allow us to constrain the interleaving of concurrent
processes so that we can be sure that the program behavior is correct.  Many
mechanisms have been developed for this purpose.  In this section, we describe
one of them, the <a class="index-entry-id" id="index-serializer"></a>
<em class="dfn">serializer</em>.
</p>
<h4 class="subsubheading" id="Serializing-access-to-shared-state"><span>Serializing access to shared state<a class="copiable-link" href="#Serializing-access-to-shared-state"> &#182;</a></span></h4>

<p>Serialization implements the following idea: Processes will execute
concurrently, but there will be certain collections of procedures that cannot
be executed concurrently.  More precisely, serialization creates distinguished
sets of procedures such that only one execution of a procedure in each
serialized set is permitted to happen at a time.  If some procedure in the set
is being executed, then a process that attempts to execute any procedure in the
set will be forced to wait until the first execution has finished.
</p>
<p>We can use serialization to control access to shared variables.  For example,
if we want to update a shared variable based on the previous value of that
variable, we put the access to the previous value of the variable and the
assignment of the new value to the variable in the same procedure.  We then
ensure that no other procedure that assigns to the variable can run
concurrently with this procedure by serializing all of these procedures with
the same serializer.  This guarantees that the value of the variable cannot be
changed between an access and the corresponding assignment.
</p>
<h4 class="subsubheading" id="Serializers-in-Scheme"><span>Serializers in Scheme<a class="copiable-link" href="#Serializers-in-Scheme"> &#182;</a></span></h4>

<p>To make the above mechanism more concrete, suppose that we have extended Scheme
to include a procedure parallel execution:
</p>
<div class="example">
<pre class="example-preformatted">// Rust's std::thread provides parallel execution:
use std::thread;

let handles: Vec&lt;_&gt; = vec![p1, p2, /* ... */ pk]
    .into_iter()
    .map(|p| thread::spawn(p))
    .collect();

for h in handles {
    h.join().unwrap();
}
</pre></div>

<p>Each <code class="code">⟨</code><var class="var">p</var><code class="code">⟩</code> must be a procedure of no arguments.  <code class="code">Parallel-execute</code>
creates a separate process for each <code class="code">⟨</code><var class="var">p</var><code class="code">⟩</code>, which applies
<code class="code">⟨</code><var class="var">p</var><code class="code">⟩</code> (to no arguments).  These processes all run
concurrently.<a class="footnote" id="DOCF159" href="#FOOT159"><sup>159</sup></a>
</p>
<p>As an example of how this is used, consider
</p>
<p>Rust&#8217;s type system prevents data races at compile time:
</p><div class="example">
<pre class="example-preformatted">use std::sync::{Arc, Mutex};
use std::thread;

let x = Arc::new(Mutex::new(10_i64));
let x1 = Arc::clone(&amp;x);
let x2 = Arc::clone(&amp;x);

let h1 = thread::spawn(move || {
    let mut val = x1.lock().unwrap();
    *val = *val * *val;  // Mutex ensures exclusive access
});

let h2 = thread::spawn(move || {
    let mut val = x2.lock().unwrap();
    *val += 1;
});

h1.join().unwrap();
h2.join().unwrap();
// Result is always 101 or 121 - no race conditions!
</pre></div>

<p>This creates two concurrent processes&#8212;<em class="math">P_1</em>, which sets <code class="code">x</code> to
<code class="code">x</code> times <code class="code">x</code>, and <em class="math">P_2</em>, which increments <code class="code">x</code>.  After
execution is complete, <code class="code">x</code> will be left with one of five possible values,
depending on the interleaving of the events of <em class="math">P_1</em> and <em class="math">P_2</em>:
</p>
<div class="example">
<pre class="example-preformatted">101: <em class="math">P_1</em> sets <code class="code">x</code> to 100 and then <em class="math">P_2</em> increments
     <code class="code">x</code> to 101.
121: <em class="math">P_2</em> increments <code class="code">x</code> to 11 and then <em class="math">P_1</em> sets
     <code class="code">x</code> to <code class="code">x</code> times <code class="code">x</code>.
110: <em class="math">P_2</em> changes <code class="code">x</code> from 10 to 11 between the
     two times that <em class="math">P_1</em> accesses the value of
     <code class="code">x</code> during the evaluation of <code class="code">(* x x)</code>.
 11: <em class="math">P_2</em> accesses <code class="code">x</code>, then <em class="math">P_1</em> sets <code class="code">x</code> to 100,
     then <em class="math">P_2</em> sets <code class="code">x</code>.
100: <em class="math">P_1</em> accesses <code class="code">x</code> (twice), then <em class="math">P_2</em> sets
     <code class="code">x</code> to 11, then <em class="math">P_1</em> sets <code class="code">x</code>.
</pre></div>

<p>We can constrain the concurrency by using serialized procedures, which are
created by <a class="index-entry-id" id="index-serializers"></a>
<em class="dfn">serializers</em>. Serializers are constructed by
<code class="code">Mutex::new</code>, whose implementation is given below.  A serializer
takes a procedure as argument and returns a serialized procedure that behaves
like the original procedure.  All calls to a given serializer return serialized
procedures in the same set.
</p>
<p>Thus, in contrast to the example above, executing
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::{Arc, Mutex};
use std::thread;

let x = Arc::new(Mutex::new(10_i64));
let x1 = Arc::clone(&amp;x);
let x2 = Arc::clone(&amp;x);

// The Mutex acts as the serializer
let h1 = thread::spawn(move || {
    let mut val = x1.lock().unwrap();
    *val = *val * *val;
});

let h2 = thread::spawn(move || {
    let mut val = x2.lock().unwrap();
    *val += 1;
});

h1.join().unwrap();
h2.join().unwrap();
// Only 101 or 121 possible - serialized access!
</pre></div>

<p>can produce only two possible values for <code class="code">x</code>, 101 or 121.  The other
possibilities are eliminated, because the execution of <em class="math">P_1</em> and <em class="math">P_2</em>
cannot be interleaved.
</p>
<p>Here is a version of the <code class="code">Account::new</code> procedure from
<a class="ref" href="3_002e1.xhtml#g_t3_002e1_002e1">Local State Variables</a>, where the deposits and withdrawals have been serialized:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::Mutex;

struct Account {
    balance: Mutex&lt;i64&gt;,
}

impl Account {
    fn new(initial_balance: i64) -&gt; Self {
        Account { balance: Mutex::new(initial_balance) }
    }

    fn withdraw(&amp;self, amount: i64) -&gt; Result&lt;i64, &amp;'static str&gt; {
        let mut balance = self.balance.lock().unwrap();
        if *balance &gt;= amount {
            *balance -= amount;
            Ok(*balance)
        } else {
            Err(&quot;Insufficient funds&quot;)
        }
    }

    fn deposit(&amp;self, amount: i64) -&gt; i64 {
        let mut balance = self.balance.lock().unwrap();
        *balance += amount;
        *balance
    }

    fn get_balance(&amp;self) -&gt; i64 {
        *self.balance.lock().unwrap()
    }
}
</pre></div>

<p>With this implementation, two processes cannot be withdrawing from or
depositing into a single account concurrently.  This eliminates the source of
the error illustrated in <a class="ref" href="#Figure-3_002e29">Figure 3.29</a>, where Peter changes the account
balance between the times when Paul accesses the balance to compute the new
value and when Paul actually performs the assignment.  On the other hand, each
account has its own serializer, so that deposits and withdrawals for different
accounts can proceed concurrently.
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e39"></a>Exercise 3.39:</strong> Which of the five possibilities
in the parallel execution shown above remain if we instead serialize execution
as follows:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::{Arc, Mutex};
use std::thread;

let x = Arc::new(Mutex::new(10_i64));
let x1 = Arc::clone(&amp;x);
let x2 = Arc::clone(&amp;x);

// Only the inner computation is serialized for thread 1
let h1 = thread::spawn(move || {
    let square = {
        let val = x1.lock().unwrap();
        *val * *val
    };  // Lock released here
    let mut val = x1.lock().unwrap();
    *val = square;
});

// Thread 2 is fully serialized
let h2 = thread::spawn(move || {
    let mut val = x2.lock().unwrap();
    *val += 1;
});
</pre></div>
</blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e40"></a>Exercise 3.40:</strong> Give all possible values of
<code class="code">x</code> that can result from executing
</p>
<div class="example">
<pre class="example-preformatted">// Without serialization - many possible outcomes
let x = Arc::new(AtomicI64::new(10));
let x1 = Arc::clone(&amp;x);
let x2 = Arc::clone(&amp;x);

thread::spawn(move || {
    let val = x1.load(Ordering::SeqCst);
    x1.store(val * val, Ordering::SeqCst);
});

thread::spawn(move || {
    let val = x2.load(Ordering::SeqCst);
    x2.store(val * val * val, Ordering::SeqCst);
});
</pre></div>

<p>Which of these possibilities remain if we instead use serialized procedures:
</p>
<div class="example">
<pre class="example-preformatted">// With Mutex serialization - only 100 or 1000000 possible
let x = Arc::new(Mutex::new(10_i64));
let x1 = Arc::clone(&amp;x);
let x2 = Arc::clone(&amp;x);

thread::spawn(move || {
    let mut val = x1.lock().unwrap();
    *val = *val * *val;
});

thread::spawn(move || {
    let mut val = x2.lock().unwrap();
    *val = *val * *val * *val;
});
</pre></div>
</blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e41"></a>Exercise 3.41:</strong> Ben Bitdiddle worries that it
would be better to implement the bank account as follows (where the commented
line has been changed):
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::Mutex;

struct Account {
    balance: Mutex&lt;i64&gt;,
}

impl Account {
    fn new(initial: i64) -&gt; Self {
        Account { balance: Mutex::new(initial) }
    }

    fn withdraw(&amp;self, amount: i64) -&gt; Result&lt;i64, &amp;'static str&gt; {
        let mut balance = self.balance.lock().unwrap();
        if *balance &gt;= amount {
            *balance -= amount;
            Ok(*balance)
        } else {
            Err(&quot;Insufficient funds&quot;)
        }
    }

    fn deposit(&amp;self, amount: i64) -&gt; i64 {
        let mut balance = self.balance.lock().unwrap();
        *balance += amount;
        *balance
    }

    fn get_balance(&amp;self) -&gt; i64 {
        // Also serialized - locks the mutex
        *self.balance.lock().unwrap()
    }
}
</pre></div>

<p>because allowing unserialized access to the bank balance can result in
anomalous behavior.  Do you agree?  Is there any scenario that demonstrates
Ben&#8217;s concern?
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e42"></a>Exercise 3.42:</strong> Ben Bitdiddle suggests that it&#8217;s
a waste of time to create a new serialized procedure in response to every
<code class="code">withdraw</code> and <code class="code">deposit</code> message.  He says that <code class="code">Account::new</code>
could be changed so that the calls to <code class="code">protected</code> are done outside the
<code class="code">dispatch</code> procedure.  That is, an account would return the same
serialized procedure (which was created at the same time as the account) each
time it is asked for a withdrawal procedure.
</p>
<div class="example">
<pre class="example-preformatted">// In Rust, the Mutex itself provides serialization.
// There's no difference between creating wrapper methods
// at construction time vs on demand - both access the
// same Mutex which serializes all operations.

struct Account {
    balance: Mutex&lt;i64&gt;,
}

impl Account {
    fn new(initial: i64) -&gt; Self {
        Account { balance: Mutex::new(initial) }
    }

    // These methods don't need explicit serialization -
    // the Mutex lock provides it automatically
    fn withdraw(&amp;self, amount: i64) -&gt; Result&lt;i64, &amp;'static str&gt; {
        let mut balance = self.balance.lock().unwrap();
        if *balance &gt;= amount {
            *balance -= amount;
            Ok(*balance)
        } else {
            Err(&quot;Insufficient funds&quot;)
        }
    }

    fn deposit(&amp;self, amount: i64) -&gt; i64 {
        let mut balance = self.balance.lock().unwrap();
        *balance += amount;
        *balance
    }
}
</pre></div>

<p>Is this a safe change to make?  In particular, is there any difference in what
concurrency is allowed by these two versions of <code class="code">Account::new</code>?
</p></blockquote>

<h4 class="subsubheading" id="Complexity-of-using-multiple-shared-resources"><span>Complexity of using multiple shared resources<a class="copiable-link" href="#Complexity-of-using-multiple-shared-resources"> &#182;</a></span></h4>

<p>Serializers provide a powerful abstraction that helps isolate the complexities
of concurrent programs so that they can be dealt with carefully and (hopefully)
correctly.  However, while using serializers is relatively straightforward when
there is only a single shared resource (such as a single bank account),
concurrent programming can be treacherously difficult when there are multiple
shared resources.
</p>
<p>To illustrate one of the difficulties that can arise, suppose we wish to swap
the balances in two bank accounts.  We access each account to find the balance,
compute the difference between the balances, withdraw this difference from one
account, and deposit it in the other account.  We could implement this as
follows:<a class="footnote" id="DOCF160" href="#FOOT160"><sup>160</sup></a>
</p>
<div class="example">
<pre class="example-preformatted">fn exchange(account1: &amp;Account, account2: &amp;Account) {
    let difference = account1.get_balance() - account2.get_balance();
    account1.withdraw(difference).ok();
    account2.deposit(difference);
}
</pre></div>

<p>This procedure works well when only a single process is trying to do the
exchange.  Suppose, however, that Peter and Paul both have access to accounts
<em class="math">{a1}</em>, <em class="math">{a2}</em>, and <em class="math">{a3}</em>, and that Peter exchanges <em class="math">{a1}</em> and <em class="math">{a2}</em> while
Paul concurrently exchanges <em class="math">{a1}</em> and <em class="math">{a3}</em>.  Even with account deposits and
withdrawals serialized for individual accounts (as in the <code class="code">Account::new</code>
procedure shown above in this section), <code class="code">exchange</code> can still produce
incorrect results.  For example, Peter might compute the difference in the
balances for <em class="math">{a1}</em> and <em class="math">{a2}</em>, but then Paul might change the balance in
<em class="math">{a1}</em> before Peter is able to complete the exchange.<a class="footnote" id="DOCF161" href="#FOOT161"><sup>161</sup></a>  For correct behavior, we must arrange for the
<code class="code">exchange</code> procedure to lock out any other concurrent accesses to the
accounts during the entire time of the exchange.
</p>
<p>One way we can accomplish this is by using both accounts&#8217; serializers to
serialize the entire <code class="code">exchange</code> procedure.  To do this, we will arrange
for access to an account&#8217;s serializer.  Note that we are deliberately breaking
the modularity of the bank-account object by exposing the serializer.  The
following version of <code class="code">Account::new</code> is identical to the original version
given in <a class="ref" href="3_002e1.xhtml#g_t3_002e1_002e1">Local State Variables</a>, except that a serializer is provided to protect
the balance variable, and the serializer is exported via message passing:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::{Arc, Mutex};

// Account that exposes its lock for external serialization
struct AccountWithSerializer {
    balance: Arc&lt;Mutex&lt;i64&gt;&gt;,
}

impl AccountWithSerializer {
    fn new(initial_balance: i64) -&gt; Self {
        AccountWithSerializer {
            balance: Arc::new(Mutex::new(initial_balance)),
        }
    }

    fn withdraw(&amp;self, amount: i64) -&gt; Result&lt;i64, &amp;'static str&gt; {
        let mut balance = self.balance.lock().unwrap();
        if *balance &gt;= amount {
            *balance -= amount;
            Ok(*balance)
        } else {
            Err(&quot;Insufficient funds&quot;)
        }
    }

    fn deposit(&amp;self, amount: i64) -&gt; i64 {
        let mut balance = self.balance.lock().unwrap();
        *balance += amount;
        *balance
    }

    fn get_balance(&amp;self) -&gt; i64 {
        *self.balance.lock().unwrap()
    }

    // Expose the lock for external serialization
    fn serializer(&amp;self) -&gt; Arc&lt;Mutex&lt;i64&gt;&gt; {
        Arc::clone(&amp;self.balance)
    }
}
</pre></div>

<p>We can use this to do serialized deposits and withdrawals.  However, unlike our
earlier serialized account, it is now the responsibility of each user of
bank-account objects to explicitly manage the serialization, for example as
follows:<a class="footnote" id="DOCF162" href="#FOOT162"><sup>162</sup></a>
</p>
<div class="example">
<pre class="example-preformatted">// In Rust, the Mutex already serializes access, so deposit
// automatically waits for any concurrent operation to complete
fn deposit(account: &amp;AccountWithSerializer, amount: i64) -&gt; i64 {
    account.deposit(amount)  // Lock acquired internally
}
</pre></div>

<p>Exporting the serializer in this way gives us enough flexibility to implement a
serialized exchange program.  We simply serialize the original <code class="code">exchange</code>
procedure with the serializers for both accounts:
</p>
<div class="example">
<pre class="example-preformatted">// Serialized exchange: acquire both locks before exchanging
fn serialized_exchange(
    account1: &amp;AccountWithSerializer,
    account2: &amp;AccountWithSerializer,
) {
    // Lock both accounts (careful ordering to avoid deadlock)
    let (lock1, lock2) = if Arc::as_ptr(&amp;account1.balance)
        &lt; Arc::as_ptr(&amp;account2.balance)
    {
        (account1.balance.lock().unwrap(),
         account2.balance.lock().unwrap())
    } else {
        let l2 = account2.balance.lock().unwrap();
        let l1 = account1.balance.lock().unwrap();
        (l1, l2)
    };

    // Perform exchange with both locks held
    let difference = *lock1 - *lock2;
    drop(lock1);
    drop(lock2);

    account1.withdraw(difference).ok();
    account2.deposit(difference);
}
</pre></div>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e43"></a>Exercise 3.43:</strong> Suppose that the balances in
three accounts start out as $10, $20, and $30, and that multiple processes run,
exchanging the balances in the accounts.  Argue that if the processes are run
sequentially, after any number of concurrent exchanges, the account balances
should be $10, $20, and $30 in some order.  Draw a timing diagram like the one
in <a class="ref" href="#Figure-3_002e29">Figure 3.29</a> to show how this condition can be violated if the
exchanges are implemented using the first version of the account-exchange
program in this section.  On the other hand, argue that even with this
<code class="code">exchange</code> program, the sum of the balances in the accounts will be
preserved.  Draw a timing diagram to show how even this condition would be
violated if we did not serialize the transactions on individual accounts.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e44"></a>Exercise 3.44:</strong> Consider the problem of
transferring an amount from one account to another.  Ben Bitdiddle claims that
this can be accomplished with the following procedure, even if there are
multiple people concurrently transferring money among multiple accounts, using
any account mechanism that serializes deposit and withdrawal transactions, for
example, the version of <code class="code">Account::new</code> in the text above.
</p>
<div class="example">
<pre class="example-preformatted">fn transfer(
    from_account: &amp;AccountWithSerializer,
    to_account: &amp;AccountWithSerializer,
    amount: i64,
) {
    from_account.withdraw(amount).unwrap();
    to_account.deposit(amount);
}
</pre></div>

<p>Louis Reasoner claims that there is a problem here, and that we need to use a
more sophisticated method, such as the one required for dealing with the
exchange problem.  Is Louis right?  If not, what is the essential difference
between the transfer problem and the exchange problem?  (You should assume that
the balance in <code class="code">from-account</code> is at least <code class="code">amount</code>.)
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e45"></a>Exercise 3.45:</strong> Louis Reasoner thinks our
bank-account system is unnecessarily complex and error-prone now that deposits
and withdrawals aren&#8217;t automatically serialized.  He suggests that
<code class="code">make-account-and-serializer</code> should have exported the serializer (for use
by such procedures as <code class="code">serialized-exchange</code>) in addition to (rather than
instead of) using it to serialize accounts and deposits as <code class="code">Account::new</code>
did.  He proposes to redefine accounts as follows:
</p>
<div class="example">
<pre class="example-preformatted">// Louis's proposed design: expose serializer AND use it internally
// This causes deadlock because withdraw/deposit already hold the lock
// when serialized_exchange tries to acquire it again
struct LouisAccount {
    balance: Arc&lt;Mutex&lt;i64&gt;&gt;,
}

impl LouisAccount {
    fn new(initial_balance: i64) -&gt; Self {
        LouisAccount {
            balance: Arc::new(Mutex::new(initial_balance)),
        }
    }

    // Problem: these methods lock internally...
    fn withdraw(&amp;self, amount: i64) -&gt; Result&lt;i64, &amp;'static str&gt; {
        let mut balance = self.balance.lock().unwrap();
        if *balance &gt;= amount {
            *balance -= amount;
            Ok(*balance)
        } else {
            Err(&quot;Insufficient funds&quot;)
        }
    }

    fn deposit(&amp;self, amount: i64) -&gt; i64 {
        let mut balance = self.balance.lock().unwrap();
        *balance += amount;
        *balance
    }

    // ...but we also expose the lock for external serialization
    fn serializer(&amp;self) -&gt; Arc&lt;Mutex&lt;i64&gt;&gt; {
        Arc::clone(&amp;self.balance)
    }
    // If serialized_exchange locks, then calls withdraw,
    // withdraw tries to lock again -&gt; DEADLOCK!
}
</pre></div>

<p>Then deposits are handled as with the original <code class="code">Account::new</code>:
</p>
<div class="example">
<pre class="example-preformatted">fn deposit(account: &amp;LouisAccount, amount: i64) -&gt; i64 {
    account.deposit(amount)
}
</pre></div>

<p>Explain what is wrong with Louis&#8217;s reasoning.  In particular, consider what
happens when <code class="code">serialized-exchange</code> is called.
</p></blockquote>

<h4 class="subsubheading" id="Implementing-serializers"><span>Implementing serializers<a class="copiable-link" href="#Implementing-serializers"> &#182;</a></span></h4>

<p>We implement serializers in terms of a more primitive synchronization mechanism
called a <a class="index-entry-id" id="index-mutex"></a>
<em class="dfn">mutex</em>.  A mutex is an object that supports two
operations&#8212;the mutex can be <a class="index-entry-id" id="index-acquired"></a>
<em class="dfn">acquired</em>, and the mutex can be
<a class="index-entry-id" id="index-released"></a>
<em class="dfn">released</em>.  Once a mutex has been acquired, no other acquire
operations on that mutex may proceed until the mutex is released.<a class="footnote" id="DOCF163" href="#FOOT163"><sup>163</sup></a> In our implementation, each
serializer has an associated mutex.  Given a procedure <code class="code">p</code>, the serializer
returns a procedure that acquires the mutex, runs <code class="code">p</code>, and then releases
the mutex.  This ensures that only one of the procedures produced by the
serializer can be running at once, which is precisely the serialization
property that we need to guarantee.
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::Mutex;

// In Rust, Mutex&lt;T&gt; IS the serializer - it wraps the protected data
// and provides exclusive access via lock()
fn make_serializer&lt;T&gt;() -&gt; Mutex&lt;T&gt;
where
    T: Default,
{
    Mutex::new(T::default())
}

// To serialize a procedure in Rust, we use MutexGuard's RAII:
fn serialized_operation&lt;T, F, R&gt;(mutex: &amp;Mutex&lt;T&gt;, f: F) -&gt; R
where
    F: FnOnce(&amp;mut T) -&gt; R,
{
    let mut guard = mutex.lock().unwrap();  // Acquire
    let result = f(&amp;mut *guard);             // Run procedure
    result                                    // Release on drop
}
</pre></div>

<p>The mutex is a mutable object (here we&#8217;ll use a one-element list, which we&#8217;ll
refer to as a <a class="index-entry-id" id="index-cell"></a>
<em class="dfn">cell</em>) that can hold the value true or false.  When the
value is false, the mutex is available to be acquired.  When the value is true,
the mutex is unavailable, and any process that attempts to acquire the mutex
must wait.
</p>
<p>Our mutex constructor <code class="code">make-mutex</code> begins by initializing the cell
contents to false.  To acquire the mutex, we test the cell.  If the mutex is
available, we set the cell contents to true and proceed.  Otherwise, we wait in
a loop, attempting to acquire over and over again, until we find that the mutex
is available.<a class="footnote" id="DOCF164" href="#FOOT164"><sup>164</sup></a>  To release the
mutex, we set the cell contents to false.
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::atomic::{AtomicBool, Ordering};

// Low-level mutex using atomic operations (spinlock)
struct SpinLock {
    locked: AtomicBool,
}

impl SpinLock {
    fn new() -&gt; Self {
        SpinLock {
            locked: AtomicBool::new(false),
        }
    }

    fn acquire(&amp;self) {
        // Spin until we acquire the lock
        while self.test_and_set() {
            std::hint::spin_loop();  // CPU hint for spin-waiting
        }
    }

    fn release(&amp;self) {
        self.locked.store(false, Ordering::Release);
    }

    fn test_and_set(&amp;self) -&gt; bool {
        // Atomically: read old value, write true, return old
        self.locked.swap(true, Ordering::Acquire)
    }
}
</pre></div>

<p><code class="code">Test-and-set!</code> tests the cell and returns the result of the test.  In
addition, if the test was false, <code class="code">test-and-set!</code> sets the cell contents to
true before returning false.  We can express this behavior as the following
procedure:
</p>
<div class="example">
<pre class="example-preformatted">// NON-ATOMIC version - INCORRECT for concurrent use!
// This demonstrates the race condition problem
fn test_and_set_broken(cell: &amp;Cell&lt;bool&gt;) -&gt; bool {
    if cell.get() {
        true           // Already locked
    } else {
        cell.set(true);  // &lt;-- Race condition here!
        false          // We &quot;acquired&quot; it
    }
}
// Between the read and write, another thread could also
// see false and think it acquired the lock
</pre></div>

<p>However, this implementation of <code class="code">test-and-set!</code> does not suffice as it
stands.  There is a crucial subtlety here, which is the essential place where
concurrency control enters the system: The <code class="code">test-and-set!</code> operation must
be performed <a class="index-entry-id" id="index-atomically"></a>
<em class="dfn">atomically</em>.  That is, we must guarantee that, once a
process has tested the cell and found it to be false, the cell contents will
actually be set to true before any other process can test the cell.  If we do
not make this guarantee, then the mutex can fail in a way similar to the
bank-account failure in <a class="ref" href="#Figure-3_002e29">Figure 3.29</a>.  (See <a class="ref" href="#Exercise-3_002e46">Exercise 3.46</a>.)
</p>
<p>The actual implementation of <code class="code">test-and-set!</code> depends on the details of how
our system runs concurrent processes.  For example, we might be executing
concurrent processes on a sequential processor using a time-slicing mechanism
that cycles through the processes, permitting each process to run for a short
time before interrupting it and moving on to the next process.  In that case,
<code class="code">test-and-set!</code>  can work by disabling time slicing during the testing and
setting.<a class="footnote" id="DOCF165" href="#FOOT165"><sup>165</sup></a>  Alternatively, multiprocessing computers provide
instructions that support atomic operations directly in
hardware.<a class="footnote" id="DOCF166" href="#FOOT166"><sup>166</sup></a>
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e46"></a>Exercise 3.46:</strong> Suppose that we implement
<code class="code">test-and-set!</code>  using an ordinary procedure as shown in the text, without
attempting to make the operation atomic.  Draw a timing diagram like the one in
<a class="ref" href="#Figure-3_002e29">Figure 3.29</a> to demonstrate how the mutex implementation can fail by
allowing two processes to acquire the mutex at the same time.
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e47"></a>Exercise 3.47:</strong> A semaphore (of size <em class="math">n</em>) is a
generalization of a mutex.  Like a mutex, a semaphore supports acquire and
release operations, but it is more general in that up to <em class="math">n</em> processes can
acquire it concurrently.  Additional processes that attempt to acquire the
semaphore must wait for release operations.  Give implementations of semaphores
</p>
<ol class="enumerate" type="a" start="1">
<li> in terms of mutexes

</li><li> in terms of atomic <code class="code">test-and-set!</code> operations.

</li></ol>
</blockquote>

<h4 class="subsubheading" id="Deadlock"><span>Deadlock<a class="copiable-link" href="#Deadlock"> &#182;</a></span></h4>

<p>Now that we have seen how to implement serializers, we can see that account
exchanging still has a problem, even with the <code class="code">serialized-exchange</code>
procedure above.  Imagine that Peter attempts to exchange <em class="math">{a1}</em> with <em class="math">{a2}</em>
while Paul concurrently attempts to exchange <em class="math">{a2}</em> with <em class="math">{a1}</em>.  Suppose that
Peter&#8217;s process reaches the point where it has entered a serialized procedure
protecting <em class="math">{a1}</em> and, just after that, Paul&#8217;s process enters a serialized
procedure protecting <em class="math">{a2}</em>.  Now Peter cannot proceed (to enter a serialized
procedure protecting <em class="math">{a2}</em>) until Paul exits the serialized procedure
protecting <em class="math">{a2}</em>.  Similarly, Paul cannot proceed until Peter exits the
serialized procedure protecting <em class="math">{a1}</em>.  Each process is stalled forever,
waiting for the other.  This situation is called a <a class="index-entry-id" id="index-deadlock"></a>
<em class="dfn">deadlock</em>.
Deadlock is always a danger in systems that provide concurrent access to
multiple shared resources.
</p>
<p>One way to avoid the deadlock in this situation is to give each account a
unique identification number and rewrite <code class="code">serialized-exchange</code> so that a
process will always attempt to enter a procedure protecting the lowest-numbered
account first.  Although this method works well for the exchange problem, there
are other situations that require more sophisticated deadlock-avoidance
techniques, or where deadlock cannot be avoided at all.  (See <a class="ref" href="#Exercise-3_002e48">Exercise 3.48</a>
and <a class="ref" href="#Exercise-3_002e49">Exercise 3.49</a>.)<a class="footnote" id="DOCF167" href="#FOOT167"><sup>167</sup></a>
</p>
<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e48"></a>Exercise 3.48:</strong> Explain in detail why the
deadlock-avoidance method described above, (i.e., the accounts are numbered,
and each process attempts to acquire the smaller-numbered account first) avoids
deadlock in the exchange problem.  Rewrite <code class="code">serialized-exchange</code> to
incorporate this idea.  (You will also need to modify <code class="code">Account::new</code> so
that each account is created with a number, which can be accessed by sending an
appropriate message.)
</p></blockquote>

<blockquote class="quotation">
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e49"></a>Exercise 3.49:</strong> Give a scenario where the
deadlock-avoidance mechanism described above does not work.  (Hint: In the
exchange problem, each process knows in advance which accounts it will need to
get access to.  Consider a situation where a process must get access to some
shared resources before it can know which additional shared resources it will
require.)
</p></blockquote>

<h4 class="subsubheading" id="Concurrency_002c-time_002c-and-communication"><span>Concurrency, time, and communication<a class="copiable-link" href="#Concurrency_002c-time_002c-and-communication"> &#182;</a></span></h4>

<p>We&#8217;ve seen how programming concurrent systems requires controlling the ordering
of events when different processes access shared state, and we&#8217;ve seen how to
achieve this control through judicious use of serializers.  But the problems of
concurrency lie deeper than this, because, from a fundamental point of view,
it&#8217;s not always clear what is meant by &#8220;shared state.&#8221;
</p>
<p>Mechanisms such as <code class="code">test-and-set!</code> require processes to examine a global
shared flag at arbitrary times.  This is problematic and inefficient to
implement in modern high-speed processors, where due to optimization techniques
such as pipelining and cached memory, the contents of memory may not be in a
consistent state at every instant.  In contemporary multiprocessing systems,
therefore, the serializer paradigm is being supplanted by new approaches to
concurrency control.<a class="footnote" id="DOCF168" href="#FOOT168"><sup>168</sup></a>
</p>
<p>The problematic aspects of shared state also arise in large, distributed
systems.  For instance, imagine a distributed banking system where individual
branch banks maintain local values for bank balances and periodically compare
these with values maintained by other branches.  In such a system the value of
&#8220;the account balance&#8221; would be undetermined, except right after
synchronization.  If Peter deposits money in an account he holds jointly with
Paul, when should we say that the account balance has changed&#8212;when the
balance in the local branch changes, or not until after the synchronization?
And if Paul accesses the account from a different branch, what are the
reasonable constraints to place on the banking system such that the behavior is
&#8220;correct&#8221;?  The only thing that might matter for correctness is the behavior
observed by Peter and Paul individually and the &#8220;state&#8221; of the account
immediately after synchronization.  Questions about the &#8220;real&#8221; account
balance or the order of events between synchronizations may be irrelevant or
meaningless.<a class="footnote" id="DOCF169" href="#FOOT169"><sup>169</sup></a>
</p>
<p>The basic phenomenon here is that synchronizing different processes,
establishing shared state, or imposing an order on events requires
communication among the processes.  In essence, any notion of time in
concurrency control must be intimately tied to communication.<a class="footnote" id="DOCF170" href="#FOOT170"><sup>170</sup></a>  It is intriguing that a
similar connection between time and communication also arises in the Theory of
Relativity, where the speed of light (the fastest signal that can be used to
synchronize events) is a fundamental constant relating time and space.  The
complexities we encounter in dealing with time and state in our computational
models may in fact mirror a fundamental complexity of the physical universe.
</p>
<hr />
</div>
<div class="subsection-level-extent" id="g_t3_002e4_002e3">
<h4 class="subsection" id="Fearless-Concurrency"><span>3.4.3 Fearless Concurrency<a class="copiable-link" href="#Fearless-Concurrency"> &#182;</a></span></h4>
<a class="index-entry-id" id="index-fearless-concurrency"></a>
<a class="index-entry-id" id="index-data-races-1"></a>
<a class="index-entry-id" id="index-Send-trait"></a>
<a class="index-entry-id" id="index-Sync-trait"></a>

<p>In the previous sections, we explored mechanisms for controlling
concurrency through serialization&#8212;ensuring that certain procedures
cannot execute simultaneously. We saw how the <code class="code">make-serializer</code>
abstraction in Scheme requires discipline from the programmer to avoid
race conditions. A forgotten serialization call can lead to subtle,
non-deterministic bugs that appear only under specific timing conditions.
</p>
<p>Rust takes a radically different approach: <em class="emph">the compiler enforces
concurrency safety at compile time</em>. Through its ownership system and
two special traits&#8212;<code class="code">Send</code> and <code class="code">Sync</code>&#8212;Rust makes data races
impossible in safe code. This is what we call <em class="dfn">fearless concurrency</em>:
<a class="index-entry-id" id="index-fearless-concurrency_002c-definition"></a>
the ability to write concurrent programs without fear of data races,
because the type system prevents them from compiling.
</p>
<h4 class="subsubheading" id="The-Problem_003a-Data-Races"><span>The Problem: Data Races<a class="copiable-link" href="#The-Problem_003a-Data-Races"> &#182;</a></span></h4>

<p>A <em class="dfn">data race</em> occurs when:
<a class="index-entry-id" id="index-data-race_002c-definition"></a>
</p><ol class="enumerate">
<li> Two or more threads access the same memory location
</li><li> At least one access is a write
</li><li> The accesses are not synchronized
</li></ol>

<p>Consider this hypothetical unsafe Rust code (which won&#8217;t compile):
</p>
<div class="example">
<pre class="example-preformatted">let mut counter = 0;

// Spawn two threads that both increment the counter
let handle1 = std::thread::spawn(|| {
    for _ in 0..1000 {
        counter += 1;  // Error: cannot capture `counter`
    }
});

let handle2 = std::thread::spawn(|| {
    for _ in 0..1000 {
        counter += 1;  // Error: cannot capture `counter`
    }
});

handle1.join().unwrap();
handle2.join().unwrap();
</pre></div>

<p>This code has a data race: both threads read and write <code class="code">counter</code>
without synchronization. The final value is unpredictable&#8212;it might
be 2000, or any value less than that, depending on how the increments
interleave. Worse, on modern processors, the behavior might be undefined
due to memory model violations.
</p>
<p><strong class="strong">Rust refuses to compile this code.</strong> The error message points
directly at the problem: <code class="code">counter</code> is being accessed from multiple
threads without proper synchronization.
</p>
<h4 class="subsubheading" id="The-Send-Trait_003a-Safe-Transfer"><span>The Send Trait: Safe Transfer<a class="copiable-link" href="#The-Send-Trait_003a-Safe-Transfer"> &#182;</a></span></h4>

<p>The <code class="code">Send</code> trait marks types that are safe to <em class="emph">transfer ownership</em>
<a class="index-entry-id" id="index-Send-trait-1"></a>
across thread boundaries. Most types are <code class="code">Send</code>: integers, strings,
vectors, and user-defined structs composed of <code class="code">Send</code> types.
</p>
<div class="example">
<pre class="example-preformatted">pub unsafe auto trait Send {}
</pre></div>

<p>The <code class="code">auto</code> keyword means the compiler automatically implements
<code class="code">Send</code> for types composed only of <code class="code">Send</code> components. The
<code class="code">unsafe</code> keyword means that implementing <code class="code">Send</code> manually
requires an <code class="code">unsafe impl</code> block&#8212;a signal that you&#8217;re making
guarantees the compiler cannot verify.
</p>
<p>Types that are <em class="emph">not</em> <code class="code">Send</code> include:
</p><ul class="itemize mark-bullet">
<li><code class="code">Rc&lt;T&gt;</code>: uses non-atomic reference counting, unsafe to send
</li><li><code class="code">*mut T</code> and <code class="code">*const T</code>: raw pointers have no ownership semantics
</li><li>Types containing non-<code class="code">Send</code> components
</li></ul>

<p>When you call <code class="code">std::thread::spawn</code>, the closure must be <code class="code">Send</code>:
</p>
<div class="example">
<pre class="example-preformatted">use std::thread;

let data = vec![1, 2, 3];
let handle = thread::spawn(move || {
    println!(&quot;Data: {:?}&quot;, data);  // `data` is moved into the thread
});
handle.join().unwrap();
</pre></div>

<p>The <code class="code">move</code> keyword transfers ownership of <code class="code">data</code> to the new
thread. Since <code class="code">Vec&lt;i32&gt;</code> is <code class="code">Send</code>, this compiles successfully.
</p>
<h4 class="subsubheading" id="The-Sync-Trait_003a-Safe-Sharing"><span>The Sync Trait: Safe Sharing<a class="copiable-link" href="#The-Sync-Trait_003a-Safe-Sharing"> &#182;</a></span></h4>

<p>The <code class="code">Sync</code> trait marks types that are safe to <em class="emph">share references</em>
<a class="index-entry-id" id="index-Sync-trait-1"></a>
across thread boundaries. A type <code class="code">T</code> is <code class="code">Sync</code> if <code class="code">&amp;T</code>
(an immutable reference to <code class="code">T</code>) is <code class="code">Send</code>.
</p>
<div class="example">
<pre class="example-preformatted">pub unsafe auto trait Sync {}
</pre></div>

<p>Intuitively:
</p><ul class="itemize mark-bullet">
<li><code class="code">Send</code>: &#8220;I can be moved to another thread&#8221;
</li><li><code class="code">Sync</code>: &#8220;I can be shared (via <code class="code">&amp;T</code>) between threads&#8221;
</li></ul>

<p>Primitive types like <code class="code">i32</code>, <code class="code">bool</code>, and <code class="code">f64</code> are <code class="code">Sync</code>
because immutable references to them can safely be shared. However:
</p><ul class="itemize mark-bullet">
<li><code class="code">Cell&lt;T&gt;</code> and <code class="code">RefCell&lt;T&gt;</code> are <em class="emph">not</em> <code class="code">Sync</code> because
they provide interior mutability without synchronization
</li><li><code class="code">Mutex&lt;T&gt;</code> <em class="emph">is</em> <code class="code">Sync</code> (if <code class="code">T</code> is <code class="code">Send</code>) because
it provides synchronized interior mutability
</li></ul>

<h4 class="subsubheading" id="Spawning-Threads"><span>Spawning Threads<a class="copiable-link" href="#Spawning-Threads"> &#182;</a></span></h4>

<p>The <code class="code">std::thread::spawn</code> function creates a new OS thread:
<a class="index-entry-id" id="index-thread-spawning"></a>
</p>
<div class="example">
<pre class="example-preformatted">use std::thread;
use std::time::Duration;

let handle = thread::spawn(|| {
    for i in 1..10 {
        println!(&quot;Thread: {}&quot;, i);
        thread::sleep(Duration::from_millis(1));
    }
});

for i in 1..5 {
    println!(&quot;Main: {}&quot;, i);
    thread::sleep(Duration::from_millis(1));
}

handle.join().unwrap();  // Wait for thread to finish
</pre></div>

<p>The closure passed to <code class="code">spawn</code> must satisfy two constraints:
</p><ol class="enumerate">
<li> It must be <code class="code">Send</code> (can move to another thread)
</li><li> It must have a <code class="code">'static</code> lifetime (cannot borrow local data)
</li></ol>

<p>The <code class="code">'static</code> requirement is crucial. Consider:
</p>
<div class="example">
<pre class="example-preformatted">let v = vec![1, 2, 3];

let handle = thread::spawn(|| {
    println!(&quot;{:?}&quot;, v);  // Error: `v` does not live long enough
});
</pre></div>

<p>The spawned thread might outlive the current scope, but <code class="code">v</code> will
be dropped when the scope ends. Rust prevents this use-after-free by
requiring <code class="code">'static</code> bounds. The solution is to <code class="code">move</code> ownership:
</p>
<div class="example">
<pre class="example-preformatted">let v = vec![1, 2, 3];

let handle = thread::spawn(move || {
    println!(&quot;{:?}&quot;, v);  // OK: ownership transferred
});
</pre></div>

<h4 class="subsubheading" id="Scoped-Threads"><span>Scoped Threads<a class="copiable-link" href="#Scoped-Threads"> &#182;</a></span></h4>

<p>The <code class="code">'static</code> requirement is sometimes too restrictive. The
<code class="code">crossbeam</code> crate provides <em class="dfn">scoped threads</em> that can safely
<a class="index-entry-id" id="index-scoped-threads"></a>
borrow local data:
</p>
<div class="example">
<pre class="example-preformatted">use crossbeam::thread;

let mut arr = [1, 2, 3];

thread::scope(|s| {
    s.spawn(|_| {
        arr[0] += 1;  // Error: cannot mutably borrow
    });

    s.spawn(|_| {
        println!(&quot;{:?}&quot;, arr);  // OK: immutable borrow
    });
}).unwrap();
</pre></div>

<p>The <code class="code">scope</code> function guarantees that all spawned threads complete
before returning. This allows the threads to borrow from the parent scope.
However, Rust still prevents data races: you cannot have multiple mutable
borrows or a mutable borrow alongside immutable ones.
</p>
<h4 class="subsubheading" id="Message-Passing"><span>Message Passing<a class="copiable-link" href="#Message-Passing"> &#182;</a></span></h4>

<p>One approach to sharing data between threads is <em class="emph">not to share at all</em>.
<a class="index-entry-id" id="index-message-passing-2"></a>
Instead, threads communicate by sending messages through channels. Rust&#8217;s
standard library provides <code class="code">std::sync::mpsc</code> (multi-producer,
single-consumer) channels:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::mpsc;
use std::thread;

let (tx, rx) = mpsc::channel();

thread::spawn(move || {
    let values = vec![
        String::from(&quot;hello&quot;),
        String::from(&quot;from&quot;),
        String::from(&quot;the&quot;),
        String::from(&quot;thread&quot;),
    ];

    for val in values {
        tx.send(val).unwrap();
    }
});

for received in rx {
    println!(&quot;Received: {}&quot;, received);
}
</pre></div>

<p>The <code class="code">channel</code> function returns a tuple: <code class="code">tx</code> (transmitter) and
<code class="code">rx</code> (receiver). The transmitter is <code class="code">Send</code> but not <code class="code">Sync</code>,
allowing multiple threads to send messages. The receiver is neither
<code class="code">Send</code> nor <code class="code">Sync</code>, ensuring only one thread receives messages.
</p>
<p>When <code class="code">tx.send(val)</code> executes, ownership of <code class="code">val</code> transfers to
the channel. The receiving thread takes ownership when it calls <code class="code">rx.recv()</code>.
This <em class="emph">ownership transfer</em> prevents data races: only one thread owns
the data at any time.
</p>
<p>The <code class="code">crossbeam::channel</code> crate provides more advanced channels:
<a class="index-entry-id" id="index-crossbeam-channels"></a>
</p>
<div class="example">
<pre class="example-preformatted">use crossbeam::channel;
use std::thread;
use std::time::Duration;

let (tx, rx) = channel::unbounded();

// Producer thread
thread::spawn(move || {
    for i in 0..10 {
        tx.send(i).unwrap();
        thread::sleep(Duration::from_millis(100));
    }
});

// Select between channel and timeout
loop {
    select! {
        recv(rx) -&gt; msg =&gt; {
            match msg {
                Ok(i) =&gt; println!(&quot;Received: {}&quot;, i),
                Err(_) =&gt; break,
            }
        }
        default(Duration::from_millis(500)) =&gt; {
            println!(&quot;Timeout - no message received&quot;);
        }
    }
}
</pre></div>

<p>The <code class="code">select!</code> macro allows waiting on multiple channel operations,
similar to the serializer&#8217;s constraint that only one operation proceeds.
</p>
<h4 class="subsubheading" id="Shared-State-with-Arc-and-Mutex"><span>Shared State with Arc and Mutex<a class="copiable-link" href="#Shared-State-with-Arc-and-Mutex"> &#182;</a></span></h4>

<p>While message passing is idiomatic, sometimes shared mutable state is
<a class="index-entry-id" id="index-Arc"></a>
<a class="index-entry-id" id="index-Mutex"></a>
necessary. Rust provides two tools:
</p>
<ol class="enumerate">
<li> <code class="code">Arc&lt;T&gt;</code> (Atomic Reference Counted): thread-safe shared ownership
</li><li> <code class="code">Mutex&lt;T&gt;</code>: mutual exclusion for synchronized access
</li></ol>

<p>Here&#8217;s our counter example, written correctly:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::{Arc, Mutex};
use std::thread;

let counter = Arc::new(Mutex::new(0));
let mut handles = vec![];

for _ in 0..10 {
    let counter_clone = Arc::clone(&amp;counter);
    let handle = thread::spawn(move || {
        let mut num = counter_clone.lock().unwrap();
        *num += 1;
    });
    handles.push(handle);
}

for handle in handles {
    handle.join().unwrap();
}

println!(&quot;Result: {}&quot;, *counter.lock().unwrap());  // Result: 10
</pre></div>

<p>Let&#8217;s dissect this:
</p>
<ol class="enumerate">
<li> <code class="code">Arc::new(Mutex::new(0))</code>: Creates a reference-counted pointer to
a mutex protecting the integer 0
</li><li> <code class="code">Arc::clone(&amp;counter)</code>: Increments the reference count, creating
a new pointer to the same data
</li><li> <code class="code">counter_clone.lock().unwrap()</code>: Acquires the mutex, returning
a <code class="code">MutexGuard</code>
</li><li> <code class="code">*num += 1</code>: The guard dereferences to <code class="code">&amp;mut i32</code>, allowing
mutation
</li><li> When the guard goes out of scope, the mutex is <em class="emph">automatically released</em>
</li></ol>

<p>The type signature reveals the safety:
</p><div class="example">
<pre class="example-preformatted">impl&lt;T: ?Sized + Send&gt; Send for Arc&lt;T&gt;
impl&lt;T: ?Sized + Send + Sync&gt; Sync for Arc&lt;T&gt;
impl&lt;T: ?Sized + Send&gt; Send for Mutex&lt;T&gt;
impl&lt;T: ?Sized + Send&gt; Sync for Mutex&lt;T&gt;
</pre></div>

<p><code class="code">Arc&lt;Mutex&lt;T&gt;&gt;</code> is <code class="code">Send</code> and <code class="code">Sync</code> (if <code class="code">T</code> is <code class="code">Send</code>),
allowing it to be shared across threads. The <code class="code">Mutex</code> ensures that only
one thread can access the data at a time, preventing data races.
</p>
<h4 class="subsubheading" id="Message-Passing-vs_002e-Shared-State"><span>Message Passing vs. Shared State<a class="copiable-link" href="#Message-Passing-vs_002e-Shared-State"> &#182;</a></span></h4>

<p>These two approaches represent different concurrency philosophies:
<a class="index-entry-id" id="index-concurrency-patterns"></a>
</p>
<p><strong class="strong">Message Passing</strong> (&#8220;Share memory by communicating&#8221;):
</p><ul class="itemize mark-bullet">
<li><strong class="strong">Pros</strong>: Clear ownership, fewer synchronization bugs, composable
</li><li><strong class="strong">Cons</strong>: Copying overhead, less intuitive for shared resources
</li><li><strong class="strong">Use when</strong>: Tasks are independent, data flow is unidirectional
</li></ul>

<p><strong class="strong">Shared State</strong> (&#8220;Communicate by sharing memory&#8221;):
</p><ul class="itemize mark-bullet">
<li><strong class="strong">Pros</strong>: Zero-copy access, familiar to systems programmers
</li><li><strong class="strong">Cons</strong>: Deadlock risk, harder to reason about, contention issues
</li><li><strong class="strong">Use when</strong>: High-frequency updates, complex shared data structures
</li></ul>

<p>Consider a web server maintaining request statistics. Message passing works well:
</p>
<div class="example">
<pre class="example-preformatted">struct Stats {
    requests: u64,
    errors: u64,
}

enum Message {
    Request,
    Error,
    GetStats(mpsc::Sender&lt;Stats&gt;),
}

fn stats_actor(rx: mpsc::Receiver&lt;Message&gt;) {
    let mut stats = Stats { requests: 0, errors: 0 };

    for msg in rx {
        match msg {
            Message::Request =&gt; stats.requests += 1,
            Message::Error =&gt; stats.errors += 1,
            Message::GetStats(reply) =&gt; {
                reply.send(stats.clone()).unwrap();
            }
        }
    }
}
</pre></div>

<p>The <code class="code">stats_actor</code> owns the statistics, and other threads send messages
to update or query them. No locks required.
</p>
<p>Conversely, a cache shared by multiple workers benefits from shared state:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::{Arc, RwLock};
use std::collections::HashMap;

type Cache = Arc&lt;RwLock&lt;HashMap&lt;String, Vec&lt;u8&gt;&gt;&gt;&gt;;

fn worker(cache: Cache, key: String) -&gt; Option&lt;Vec&lt;u8&gt;&gt; {
    // Try read lock first
    {
        let read_guard = cache.read().unwrap();
        if let Some(value) = read_guard.get(&amp;key) {
            return Some(value.clone());
        }
    }  // Read lock released

    // Expensive computation
    let value = compute_value(&amp;key);

    // Write lock to update cache
    let mut write_guard = cache.write().unwrap();
    write_guard.insert(key, value.clone());
    Some(value)
}
</pre></div>

<p>The <code class="code">RwLock</code> (read-write lock) allows multiple readers or one writer,
<a class="index-entry-id" id="index-RwLock"></a>
reducing contention compared to <code class="code">Mutex</code>.
</p>
<h4 class="subsubheading" id="The-Compiler-as-Guardian"><span>The Compiler as Guardian<a class="copiable-link" href="#The-Compiler-as-Guardian"> &#182;</a></span></h4>

<p>The key insight is that Rust&#8217;s type system <em class="emph">encodes concurrency invariants</em>.
<a class="index-entry-id" id="index-type-system_002c-concurrency-safety"></a>
You cannot write a data race in safe Rust because:
</p>
<ol class="enumerate">
<li> Mutable references are exclusive (<code class="code">&amp;mut T</code>)
</li><li> Shared references are immutable (<code class="code">&amp;T</code>)
</li><li> <code class="code">Send</code> and <code class="code">Sync</code> control thread boundaries
</li><li> Lifetimes prevent use-after-free
</li></ol>

<p>Compare this to Section 3.4&#8217;s serializers in Scheme, which rely on programmer
discipline. In Rust, if your program compiles, you have a <em class="emph">proof</em> that
it is free of data races.
</p>
<p>This doesn&#8217;t prevent <em class="emph">all</em> concurrency bugs. Deadlocks are still possible:
</p>
<div class="example">
<pre class="example-preformatted">let m1 = Arc::new(Mutex::new(0));
let m2 = Arc::new(Mutex::new(0));

let m1_clone = Arc::clone(&amp;m1);
let m2_clone = Arc::clone(&amp;m2);

let t1 = thread::spawn(move || {
    let _g1 = m1_clone.lock().unwrap();  // Acquire m1
    thread::sleep(Duration::from_millis(10));
    let _g2 = m2_clone.lock().unwrap();  // Wait for m2 (deadlock!)
});

let t2 = thread::spawn(move || {
    let _g2 = m2.lock().unwrap();        // Acquire m2
    thread::sleep(Duration::from_millis(10));
    let _g1 = m1.lock().unwrap();        // Wait for m1 (deadlock!)
});
</pre></div>

<p>Both threads wait forever. The solution is <em class="emph">lock ordering</em>: always
acquire locks in the same order. This is a runtime protocol, not enforced
by the type system.
</p>
<p>However, data races&#8212;the most insidious class of concurrency bugs&#8212;are
<em class="emph">impossible</em> in safe Rust. This is fearless concurrency: the confidence
to parallelize code without fear of undefined behavior.
</p>
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e45a"></a>Exercise 3.45a:</strong>
</p><blockquote class="quotation">
<p>The following code attempts to compute the sum of an array in parallel:
</p>
<div class="example">
<pre class="example-preformatted">fn parallel_sum(data: &amp;[i32]) -&gt; i32 {
    let sum = 0;
    let handles: Vec&lt;_&gt; = data.chunks(100)
        .map(|chunk| {
            thread::spawn(move || {
                chunk.iter().sum::&lt;i32&gt;()
            })
        })
        .collect();

    for handle in handles {
        sum += handle.join().unwrap();
    }
    sum
}
</pre></div>

<p>This code doesn&#8217;t compile. Identify the two errors and fix them. Then,
rewrite the function using message passing instead of shared state.
</p></blockquote>

<p><strong class="strong"><a class="anchor" id="Exercise-3_002e46a"></a>Exercise 3.46a:</strong>
</p><blockquote class="quotation">
<p>Explain why <code class="code">Rc&lt;T&gt;</code> is not <code class="code">Send</code>, but <code class="code">Arc&lt;T&gt;</code> is.
What operations does <code class="code">Arc&lt;T&gt;</code> perform differently to ensure
thread safety? What is the performance cost?
</p>
<p>Implement a simplified version of <code class="code">Arc&lt;T&gt;</code> using <code class="code">AtomicUsize</code>
for the reference count:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::atomic::{AtomicUsize, Ordering};
use std::ops::Deref;

struct SimpleArc&lt;T&gt; {
    ptr: *const ArcInner&lt;T&gt;,
}

struct ArcInner&lt;T&gt; {
    ref_count: AtomicUsize,
    data: T,
}

impl&lt;T&gt; SimpleArc&lt;T&gt; {
    fn new(data: T) -&gt; Self {
        // Your implementation here
    }
}

impl&lt;T&gt; Clone for SimpleArc&lt;T&gt; {
    fn clone(&amp;self) -&gt; Self {
        // Your implementation here
    }
}

impl&lt;T&gt; Drop for SimpleArc&lt;T&gt; {
    fn drop(&amp;mut self) {
        // Your implementation here
    }
}

impl&lt;T&gt; Deref for SimpleArc&lt;T&gt; {
    type Target = T;
    fn deref(&amp;self) -&gt; &amp;T {
        // Your implementation here
    }
}
</pre></div>

<p><em class="emph">Hint</em>: Use <code class="code">Ordering::Relaxed</code> for increments,
<code class="code">Ordering::Release</code> for decrements, and <code class="code">Ordering::Acquire</code>
for the final check before deallocation.
</p></blockquote>

<p><strong class="strong"><a class="anchor" id="Exercise-3_002e47a"></a>Exercise 3.47a:</strong>
</p><blockquote class="quotation">
<p>Design and implement a <em class="dfn">reader-writer lock</em> using <code class="code">Mutex</code> and
condition variables (or channels). The lock should allow:
</p><ul class="itemize mark-bullet">
<li>Multiple readers simultaneously (if no writer)
</li><li>Exactly one writer (if no readers or other writers)
</li><li>Writers have priority over readers (prevent writer starvation)
</li></ul>

<div class="example">
<pre class="example-preformatted">struct RwLock&lt;T&gt; {
    // Your fields here
}

impl&lt;T&gt; RwLock&lt;T&gt; {
    fn new(data: T) -&gt; Self {
        // Your implementation
    }

    fn read(&amp;self) -&gt; ReadGuard&lt;T&gt; {
        // Acquire read access
    }

    fn write(&amp;self) -&gt; WriteGuard&lt;T&gt; {
        // Acquire write access
    }
}

struct ReadGuard&lt;'a, T&gt; {
    // Your fields
}

struct WriteGuard&lt;'a, T&gt; {
    // Your fields
}
</pre></div>

<p>Test your implementation with multiple threads performing mixed read/write
operations. Verify that:
</p><ol class="enumerate">
<li> Multiple reads can occur simultaneously
</li><li> Writes are exclusive
</li><li> No data races occur (use <code class="code">Arc&lt;RwLock&lt;Vec&lt;i32&gt;&gt;&gt;</code> and verify final state)
</li></ol>

<p><em class="emph">Bonus</em>: Instrument your lock to track metrics (total reads, writes,
contentions, wait times). How does performance compare to <code class="code">std::sync::RwLock</code>?
</p></blockquote>

<hr />
</div>
<div class="subsection-level-extent" id="g_t3_002e4_002e5">
<h4 class="subsection" id="Async_002fAwait_003a-Cooperative-Concurrency"><span>3.4.4 Async/Await: Cooperative Concurrency<a class="copiable-link" href="#Async_002fAwait_003a-Cooperative-Concurrency"> &#182;</a></span></h4>
<a class="index-entry-id" id="index-async_002fawait"></a>
<a class="index-entry-id" id="index-futures"></a>
<a class="index-entry-id" id="index-cooperative-concurrency"></a>

<p>In the previous section, we explored thread-based concurrency, where the
operating system schedules multiple threads of execution. Each thread has
its own stack and can be preempted at any time. This <em class="dfn">preemptive
multitasking</em> works well for CPU-bound tasks, but creates overhead for
I/O-bound workloads.
</p>
<p>Consider a web server handling 10,000 simultaneous connections. Creating
10,000 OS threads is expensive:
</p><ul class="itemize mark-bullet">
<li>Each thread requires stack space (typically 2 MB on Linux)
</li><li>Context switching between thousands of threads degrades performance
</li><li>Most threads spend time waiting for I/O, not computing
</li></ul>

<p>Rust&#8217;s <code class="code">async/await</code> syntax provides an alternative: <em class="dfn">cooperative
<a class="index-entry-id" id="index-cooperative-concurrency_002c-definition"></a>
concurrency</em>, where tasks voluntarily yield control at specific points.
This allows thousands or millions of concurrent tasks on a single thread,
with minimal overhead.
</p>
<h4 class="subsubheading" id="Concurrency-vs_002e-Parallelism"><span>Concurrency vs. Parallelism<a class="copiable-link" href="#Concurrency-vs_002e-Parallelism"> &#182;</a></span></h4>

<p>Before diving into async, let&#8217;s clarify terminology:
<a class="index-entry-id" id="index-concurrency-vs-parallelism"></a>
</p>
<ul class="itemize mark-bullet">
<li><strong class="strong">Concurrency</strong>: Managing multiple tasks that make progress over
overlapping time periods (&#8220;dealing with lots of things at once&#8221;)
</li><li><strong class="strong">Parallelism</strong>: Executing multiple tasks simultaneously on different
CPU cores (&#8220;doing lots of things at once&#8221;)
</li></ul>

<p>Threads can provide both: the OS may run threads in parallel on multiple
cores, or interleave them on a single core. Async provides <em class="emph">concurrency
without parallelism</em>&#8212;tasks cooperatively share a single thread (though
async runtimes may use multiple threads internally).
</p>
<p><strong class="strong">Use threads when</strong>:
</p><ul class="itemize mark-bullet">
<li>Tasks are CPU-bound (image processing, cryptography, simulations)
</li><li>You need to utilize multiple CPU cores
</li><li>Interfacing with blocking system calls or C libraries
</li></ul>

<p><strong class="strong">Use async when</strong>:
</p><ul class="itemize mark-bullet">
<li>Tasks are I/O-bound (network servers, databases, file systems)
</li><li>Managing thousands of concurrent connections
</li><li>Low latency is critical (microsecond-scale task switching)
</li></ul>

<h4 class="subsubheading" id="Futures_003a-Lazy-Computations"><span>Futures: Lazy Computations<a class="copiable-link" href="#Futures_003a-Lazy-Computations"> &#182;</a></span></h4>

<p>At the heart of Rust&#8217;s async system is the <code class="code">Future</code> trait:
<a class="index-entry-id" id="index-Future-trait"></a>
</p>
<div class="example">
<pre class="example-preformatted">pub trait Future {
    type Output;

    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;)
        -&gt; Poll&lt;Self::Output&gt;;
}

pub enum Poll&lt;T&gt; {
    Ready(T),
    Pending,
}
</pre></div>

<p>A <code class="code">Future</code> represents a value that may not be available yet. The
<code class="code">poll</code> method asks: &#8220;Are you done?&#8221; It returns either:
</p><ul class="itemize mark-bullet">
<li><code class="code">Poll::Ready(value)</code>: The computation completed with <code class="code">value</code>
</li><li><code class="code">Poll::Pending</code>: Still working; poll again later
</li></ul>

<p>Crucially, futures are <em class="dfn">lazy</em>: they do nothing until polled. Creating
<a class="index-entry-id" id="index-lazy-evaluation-1"></a>
a future merely constructs a state machine; no work happens until an
executor calls <code class="code">poll</code>.
</p>
<p>Here&#8217;s a simple future that completes after being polled three times:
</p>
<div class="example">
<pre class="example-preformatted">use std::future::Future;
use std::pin::Pin;
use std::task::{Context, Poll};

struct CountdownFuture {
    count: u32,
}

impl Future for CountdownFuture {
    type Output = ();

    fn poll(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;)
        -&gt; Poll&lt;Self::Output&gt;
    {
        if self.count == 0 {
            Poll::Ready(())
        } else {
            self.count -= 1;
            cx.waker().wake_by_ref();  // Schedule another poll
            Poll::Pending
        }
    }
}
</pre></div>

<p>Each <code class="code">poll</code> decrements the counter. When it reaches zero, the future
completes. The <code class="code">cx.waker().wake_by_ref()</code> call tells the executor
to poll this future again.
</p>
<h4 class="subsubheading" id="Async-Functions-and-Await"><span>Async Functions and Await<a class="copiable-link" href="#Async-Functions-and-Await"> &#182;</a></span></h4>

<p>Manually implementing <code class="code">Future</code> is tedious. The <code class="code">async fn</code> syntax
<a class="index-entry-id" id="index-async-fn"></a>
generates futures automatically:
</p>
<div class="example">
<pre class="example-preformatted">async fn fetch_url(url: &amp;str) -&gt; Result&lt;String, reqwest::Error&gt; {
    let response = reqwest::get(url).await?;
    let body = response.text().await?;
    Ok(body)
}
</pre></div>

<p>This simple function hides sophisticated machinery:
</p><ol class="enumerate">
<li> <code class="code">async fn</code> transforms the function into a state machine implementing
<code class="code">Future</code>
</li><li> Each <code class="code">.await</code> is a potential suspension point
</li><li> When a future returns <code class="code">Poll::Pending</code>, the state machine saves
local variables and yields control
</li><li> When polled again, it resumes from the last <code class="code">.await</code>
</li></ol>

<p>The <code class="code">.await</code> keyword is postfix (like <code class="code">?</code>) to emphasize that
<a class="index-entry-id" id="index-_002eawait-keyword"></a>
it&#8217;s an operation on a future, not a function call. Compare:
</p>
<div class="example">
<pre class="example-preformatted">// Synchronous (blocking)
fn fetch_sync(url: &amp;str) -&gt; String {
    let response = blocking_get(url);  // Thread sleeps here
    response.text()
}

// Asynchronous (non-blocking)
async fn fetch_async(url: &amp;str) -&gt; String {
    let response = async_get(url).await;  // Task yields here
    response.text().await
}
</pre></div>

<p>In the synchronous version, <code class="code">blocking_get</code> halts the thread until
the network responds. In the async version, <code class="code">.await</code> yields control
to the executor, which can run other tasks while waiting.
</p>
<h4 class="subsubheading" id="Async-Runtimes"><span>Async Runtimes<a class="copiable-link" href="#Async-Runtimes"> &#182;</a></span></h4>

<p>Futures need an <em class="dfn">executor</em> to drive them to completion. Rust&#8217;s standard
<a class="index-entry-id" id="index-async-runtime"></a>
<a class="index-entry-id" id="index-executor"></a>
library defines the <code class="code">Future</code> trait but <em class="emph">does not provide an
executor</em>. This allows different runtimes to specialize:
</p>
<ul class="itemize mark-bullet">
<li><strong class="strong">Tokio</strong>: Full-featured, multi-threaded work-stealing scheduler
</li><li><strong class="strong">async-std</strong>: Mirrors std library APIs, single- or multi-threaded
</li><li><strong class="strong">smol</strong>: Minimal, single-threaded, embeddable
</li><li><strong class="strong">embassy</strong>: Embedded systems, <code class="code">no_std</code> compatible
</li></ul>

<p>Here&#8217;s a Tokio example:
</p>
<div class="example">
<pre class="example-preformatted">use tokio;

#[tokio::main]
async fn main() {
    let result = fetch_url(&quot;https://example.com&quot;).await;
    println!(&quot;Fetched: {:?}&quot;, result);
}
</pre></div>

<p>The <code class="code">#[tokio::main]</code> macro expands to:
</p>
<div class="example">
<pre class="example-preformatted">fn main() {
    tokio::runtime::Runtime::new()
        .unwrap()
        .block_on(async {
            let result = fetch_url(&quot;https://example.com&quot;).await;
            println!(&quot;Fetched: {:?}&quot;, result);
        })
}
</pre></div>

<p><code class="code">block_on</code> is the bridge between sync and async: it blocks the current
thread until the future completes. Inside the async block, you can use
<code class="code">.await</code> freely.
</p>
<h4 class="subsubheading" id="Async-Blocks-and-Async-Move"><span>Async Blocks and Async Move<a class="copiable-link" href="#Async-Blocks-and-Async-Move"> &#182;</a></span></h4>

<p>Just as closures capture their environment, <code class="code">async</code> blocks can too:
<a class="index-entry-id" id="index-async-blocks"></a>
</p>
<div class="example">
<pre class="example-preformatted">let url = String::from(&quot;https://example.com&quot;);

let future = async {
    fetch_url(&amp;url).await
};

// `future` borrows `url`
</pre></div>

<p>To transfer ownership, use <code class="code">async move</code>:
<a class="index-entry-id" id="index-async-move"></a>
</p>
<div class="example">
<pre class="example-preformatted">let url = String::from(&quot;https://example.com&quot;);

let future = async move {
    fetch_url(&amp;url).await  // `url` is moved into the future
};

// `url` is no longer accessible here
</pre></div>

<p>This is essential when spawning tasks:
</p>
<div class="example">
<pre class="example-preformatted">use tokio;

#[tokio::main]
async fn main() {
    let url = String::from(&quot;https://example.com&quot;);

    let handle = tokio::spawn(async move {
        fetch_url(&amp;url).await
    });

    let result = handle.await.unwrap();
    println!(&quot;{:?}&quot;, result);
}
</pre></div>

<p><code class="code">tokio::spawn</code> is the async equivalent of <code class="code">std::thread::spawn</code>:
it runs a future on the runtime&#8217;s thread pool. The future must be
<code class="code">'static</code>, so we use <code class="code">async move</code> to capture ownership.
</p>
<h4 class="subsubheading" id="Pin_003a-Preventing-Self_002dReferential-Moves"><span>Pin: Preventing Self-Referential Moves<a class="copiable-link" href="#Pin_003a-Preventing-Self_002dReferential-Moves"> &#182;</a></span></h4>

<p>One of async Rust&#8217;s thorniest concepts is <code class="code">Pin</code>. To understand why
<a class="index-entry-id" id="index-Pin"></a>
<a class="index-entry-id" id="index-self_002dreferential-structs"></a>
it&#8217;s necessary, consider how async functions are compiled.
</p>
<p>This async function:
</p>
<div class="example">
<pre class="example-preformatted">async fn example() {
    let x = String::from(&quot;hello&quot;);
    let y = &amp;x;
    some_async_call().await;
    println!(&quot;{}&quot;, y);
}
</pre></div>

<p>Becomes a state machine like:
</p>
<div class="example">
<pre class="example-preformatted">enum ExampleFuture {
    Start,
    AwaitingCall {
        x: String,
        y: *const String,  // Pointer to x!
    },
    Done,
}
</pre></div>

<p>The problem: <code class="code">y</code> points to <code class="code">x</code>, which is stored <em class="emph">inside
the same struct</em>. If we move <code class="code">ExampleFuture</code> in memory, <code class="code">y</code>
becomes a dangling pointer.
</p>
<p><code class="code">Pin&lt;&amp;mut T&gt;</code> is Rust&#8217;s solution: it guarantees that <code class="code">T</code> will
not move in memory. The <code class="code">poll</code> method takes <code class="code">Pin&lt;&amp;mut Self&gt;</code>,
ensuring the future stays put:
</p>
<div class="example">
<pre class="example-preformatted">fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;)
    -&gt; Poll&lt;Self::Output&gt;;
</pre></div>

<p>For most users, <code class="code">Pin</code> is transparent: the <code class="code">async fn</code> syntax
handles it automatically. You only encounter <code class="code">Pin</code> when:
</p><ul class="itemize mark-bullet">
<li>Manually implementing <code class="code">Future</code>
</li><li>Working with low-level async primitives
</li><li>Creating self-referential types
</li></ul>

<p>The key insight: <code class="code">Pin</code> is a <em class="emph">compile-time proof</em> that a value
won&#8217;t move. This allows safe self-references in generated state machines.
</p>
<h4 class="subsubheading" id="Structured-Concurrency"><span>Structured Concurrency<a class="copiable-link" href="#Structured-Concurrency"> &#182;</a></span></h4>

<p>Just as spawning threads creates the risk of orphaned tasks, spawning
<a class="index-entry-id" id="index-structured-concurrency"></a>
async tasks requires careful management. Structured concurrency ensures
tasks are properly joined before their scope ends.
</p>
<p>The <code class="code">tokio::join!</code> macro waits for multiple futures concurrently:
<a class="index-entry-id" id="index-tokio_003a_003ajoin_0021"></a>
</p>
<div class="example">
<pre class="example-preformatted">use tokio;

async fn fetch_many() -&gt; (String, String, String) {
    let (a, b, c) = tokio::join!(
        fetch_url(&quot;https://example.com/a&quot;),
        fetch_url(&quot;https://example.com/b&quot;),
        fetch_url(&quot;https://example.com/c&quot;),
    );

    (a.unwrap(), b.unwrap(), c.unwrap())
}
</pre></div>

<p>All three fetches run concurrently (not sequentially!). If one completes,
the others continue. <code class="code">join!</code> waits for all to finish before returning.
</p>
<p>For dynamic collections, use <code class="code">JoinSet</code>:
</p>
<div class="example">
<pre class="example-preformatted">use tokio::task::JoinSet;

async fn fetch_urls(urls: Vec&lt;String&gt;) -&gt; Vec&lt;String&gt; {
    let mut set = JoinSet::new();

    for url in urls {
        set.spawn(async move {
            fetch_url(&amp;url).await.unwrap()
        });
    }

    let mut results = Vec::new();
    while let Some(res) = set.join_next().await {
        results.push(res.unwrap());
    }
    results
}
</pre></div>

<p>The <code class="code">tokio::select!</code> macro is the async equivalent of the <code class="code">select!</code>
<a class="index-entry-id" id="index-tokio_003a_003aselect_0021"></a>
we saw for channels:
</p>
<div class="example">
<pre class="example-preformatted">use tokio::time::{sleep, Duration};

async fn timeout_example() {
    tokio::select! {
        result = fetch_url(&quot;https://slow-server.com&quot;) =&gt; {
            println!(&quot;Fetched: {:?}&quot;, result);
        }
        _ = sleep(Duration::from_secs(5)) =&gt; {
            println!(&quot;Timeout!&quot;);
        }
    }
}
</pre></div>

<p>If the fetch completes first, the timeout is cancelled. If the timeout
fires, the fetch is cancelled. Only one branch executes.
</p>
<p><strong class="strong">Caution</strong>: <code class="code">select!</code> is not deterministic if multiple branches
are ready. For fairness, use <code class="code">tokio::select! { biased; ... }</code> to
prioritize branches in order.
</p>
<h4 class="subsubheading" id="When-to-Use-Async-vs_002e-Threads"><span>When to Use Async vs. Threads<a class="copiable-link" href="#When-to-Use-Async-vs_002e-Threads"> &#182;</a></span></h4>

<p>Async is not always the right choice. Consider:
</p>
<p><strong class="strong">Async Wins</strong>:
</p><div class="example">
<pre class="example-preformatted">// Web server handling 100,000 concurrent connections
use tokio::net::TcpListener;

#[tokio::main]
async fn main() {
    let listener = TcpListener::bind(&quot;127.0.0.1:8080&quot;)
        .await.unwrap();

    loop {
        let (socket, _) = listener.accept().await.unwrap();
        tokio::spawn(async move {
            handle_connection(socket).await;
        });
    }
}
</pre></div>

<p>Each connection is a lightweight task. Spawning 100,000 threads would
exhaust memory; spawning 100,000 async tasks uses a few megabytes.
</p>
<p><strong class="strong">Threads Win</strong>:
</p><div class="example">
<pre class="example-preformatted">// CPU-bound parallel computation
use rayon::prelude::*;

fn parallel_sum(data: &amp;[i32]) -&gt; i32 {
    data.par_iter().sum()
}
</pre></div>

<p>Async provides no benefit here&#8212;the task is CPU-bound, not I/O-bound.
Rayon&#8217;s thread pool efficiently parallelizes across cores.
</p>
<p><strong class="strong">Hybrid Approach</strong>:
</p><div class="example">
<pre class="example-preformatted">use tokio;

#[tokio::main]
async fn main() {
    let data = load_data().await;

    let result = tokio::task::spawn_blocking(|| {
        expensive_computation(data)  // Runs on dedicated thread pool
    }).await.unwrap();

    save_result(result).await;
}
</pre></div>

<p><code class="code">spawn_blocking</code> runs the synchronous, CPU-bound computation on a
separate thread pool, preventing it from blocking the async executor.
</p>
<h4 class="subsubheading" id="Zero_002dCost-Abstraction_003a-State-Machines"><span>Zero-Cost Abstraction: State Machines<a class="copiable-link" href="#Zero_002dCost-Abstraction_003a-State-Machines"> &#182;</a></span></h4>

<p>Async functions compile to state machines with zero runtime overhead
<a class="index-entry-id" id="index-zero_002dcost-abstraction-3"></a>
<a class="index-entry-id" id="index-state-machines"></a>
beyond what you&#8217;d write manually. Consider:
</p>
<div class="example">
<pre class="example-preformatted">async fn process() -&gt; i32 {
    let a = async_op_1().await;
    let b = async_op_2(a).await;
    let c = async_op_3(b).await;
    c
}
</pre></div>

<p>The compiler generates (conceptually):
</p>
<div class="example">
<pre class="example-preformatted">enum ProcessFuture {
    Start,
    AwaitingOp1 { fut1: impl Future&lt;Output = i32&gt; },
    AwaitingOp2 { a: i32, fut2: impl Future&lt;Output = i32&gt; },
    AwaitingOp3 { fut3: impl Future&lt;Output = i32&gt; },
    Done,
}

impl Future for ProcessFuture {
    type Output = i32;

    fn poll(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;)
        -&gt; Poll&lt;i32&gt;
    {
        loop {
            match &amp;mut *self {
                ProcessFuture::Start =&gt; {
                    let fut1 = async_op_1();
                    *self = ProcessFuture::AwaitingOp1 { fut1 };
                }
                ProcessFuture::AwaitingOp1 { fut1 } =&gt; {
                    match fut1.poll(cx) {
                        Poll::Ready(a) =&gt; {
                            let fut2 = async_op_2(a);
                            *self = ProcessFuture::AwaitingOp2 {
                                a, fut2
                            };
                        }
                        Poll::Pending =&gt; return Poll::Pending,
                    }
                }
                ProcessFuture::AwaitingOp2 { a, fut2 } =&gt; {
                    match fut2.poll(cx) {
                        Poll::Ready(b) =&gt; {
                            let fut3 = async_op_3(b);
                            *self = ProcessFuture::AwaitingOp3 { fut3 };
                        }
                        Poll::Pending =&gt; return Poll::Pending,
                    }
                }
                ProcessFuture::AwaitingOp3 { fut3 } =&gt; {
                    match fut3.poll(cx) {
                        Poll::Ready(c) =&gt; {
                            *self = ProcessFuture::Done;
                            return Poll::Ready(c);
                        }
                        Poll::Pending =&gt; return Poll::Pending,
                    }
                }
                ProcessFuture::Done =&gt; panic!(&quot;polled after completion&quot;),
            }
        }
    }
}
</pre></div>

<p>This is a <em class="emph">zero-cost abstraction</em>: the generated code is as efficient
as if you wrote the state machine by hand. Each <code class="code">.await</code> becomes a
state transition. Local variables are stored in the enum.
</p>
<p>The size of the future is the size of the largest variant (plus a
discriminant tag). Async code has minimal heap allocation&#8212;most futures
live on the stack or inline in their parent future.
</p>
<h4 class="subsubheading" id="Async-and-Cancellation-Safety"><span>Async and Cancellation Safety<a class="copiable-link" href="#Async-and-Cancellation-Safety"> &#182;</a></span></h4>

<p>Unlike preemptive threads, async tasks can be <em class="emph">cancelled</em> at any
<a class="index-entry-id" id="index-cancellation-safety"></a>
<code class="code">.await</code> point. This has subtle implications:
</p>
<div class="example">
<pre class="example-preformatted">async fn transfer_money(from: &amp;Account, to: &amp;Account, amount: u64) {
    from.withdraw(amount).await;  // Cancellation here is a problem!
    to.deposit(amount).await;
}
</pre></div>

<p>If this task is cancelled after the first <code class="code">.await</code>, money disappears.
The solution is to make critical sections atomic:
</p>
<div class="example">
<pre class="example-preformatted">async fn transfer_money(from: &amp;Account, to: &amp;Account, amount: u64) {
    let transaction = {
        let tx = Transaction::new();
        tx.withdraw(from, amount);
        tx.deposit(to, amount);
        tx
    };  // No .await in this block---cannot be cancelled mid-transaction

    transaction.commit().await;  // Atomic commit
}
</pre></div>

<p>Alternatively, use a <code class="code">tokio::sync::Mutex</code> guard:
</p>
<div class="example">
<pre class="example-preformatted">async fn critical_section(data: &amp;Mutex&lt;Vec&lt;i32&gt;&gt;) {
    let mut guard = data.lock().await;
    guard.push(1);
    guard.push(2);
    // If cancelled here, guard drops and releases lock
}
</pre></div>

<p>The <code class="code">MutexGuard</code> ensures the lock is released even if the task is
cancelled. Async code must be <em class="dfn">cancellation-safe</em>: safe to abort at
any <code class="code">.await</code>.
</p>
<h4 class="subsubheading" id="Comparison-to-SICP_0027s-Streams"><span>Comparison to SICP&#8217;s Streams<a class="copiable-link" href="#Comparison-to-SICP_0027s-Streams"> &#182;</a></span></h4>

<p>Async programming shares deep connections with Chapter 3&#8217;s streams. Recall
<a class="index-entry-id" id="index-streams-1"></a>
the stream paradigm: lazy sequences computed on demand. The <code class="code">cons-stream</code>
construct delays evaluation:
</p>
<div class="example">
<pre class="example-preformatted">(define (integers-starting-from n)
  (cons-stream n (integers-starting-from (+ n 1))))
</pre></div>

<p>In Rust, async functions and <code class="code">Stream</code> (the async version of <code class="code">Iterator</code>)
<a class="index-entry-id" id="index-Stream-trait"></a>
provide similar lazy evaluation:
</p>
<div class="example">
<pre class="example-preformatted">use async_stream::stream;

fn integers_starting_from(n: i32) -&gt; impl Stream&lt;Item = i32&gt; {
    stream! {
        let mut i = n;
        loop {
            yield i;
            i += 1;
        }
    }
}
</pre></div>

<p>The <code class="code">yield</code> keyword produces values lazily, just like <code class="code">cons-stream</code>.
The key difference: async streams can perform I/O or await other futures
between yields:
</p>
<div class="example">
<pre class="example-preformatted">async fn fetch_pages(urls: Vec&lt;String&gt;) -&gt; impl Stream&lt;Item = String&gt; {
    stream! {
        for url in urls {
            let page = fetch_url(&amp;url).await;
            yield page.unwrap();
        }
    }
}
</pre></div>

<p>This combines the laziness of streams with the concurrency of async/await.
You can process results as they arrive, without waiting for all fetches
to complete.
</p>
<p><strong class="strong"><a class="anchor" id="Exercise-3_002e48a"></a>Exercise 3.48a:</strong>
</p><blockquote class="quotation">
<p>Implement an async function <code class="code">parallel_map</code> that applies an async
function to every element of a vector, with a concurrency limit:
</p>
<div class="example">
<pre class="example-preformatted">async fn parallel_map&lt;T, U, F&gt;(
    items: Vec&lt;T&gt;,
    f: F,
    concurrency: usize,
) -&gt; Vec&lt;U&gt;
where
    T: Send + 'static,
    U: Send + 'static,
    F: Fn(T) -&gt; Pin&lt;Box&lt;dyn Future&lt;Output = U&gt; + Send&gt;&gt; + Send + Sync,
{
    // Your implementation here
}
</pre></div>

<p>For example, <code class="code">parallel_map(urls, fetch_url, 10)</code> should fetch URLs
with at most 10 concurrent requests. Test your implementation with a
simulated async operation:
</p>
<div class="example">
<pre class="example-preformatted">async fn simulate_work(id: usize) -&gt; usize {
    tokio::time::sleep(Duration::from_millis(100)).await;
    id * 2
}

#[tokio::main]
async fn main() {
    let items: Vec&lt;usize&gt; = (0..20).collect();
    let results = parallel_map(
        items,
        |x| Box::pin(simulate_work(x)),
        5,
    ).await;
    assert_eq!(results, (0..20).map(|x| x * 2).collect::&lt;Vec&lt;_&gt;&gt;());
}
</pre></div>

<p><em class="emph">Hint</em>: Use <code class="code">futures::stream::FuturesUnordered</code> or a <code class="code">JoinSet</code>
with manual buffering.
</p>
<p><strong class="strong">Bonus</strong>: Modify your implementation to preserve input order in
the output, even though tasks may complete out of order.
</p></blockquote>

<p><strong class="strong"><a class="anchor" id="Exercise-3_002e49a"></a>Exercise 3.49a:</strong>
</p><blockquote class="quotation">
<p>The following code has a subtle bug related to cancellation safety:
</p>
<div class="example">
<pre class="example-preformatted">use tokio::fs::File;
use tokio::io::{AsyncReadExt, AsyncWriteExt};

async fn copy_file(src: &amp;str, dst: &amp;str) -&gt; std::io::Result&lt;()&gt; {
    let mut file = File::open(src).await?;
    let mut buffer = Vec::new();
    file.read_to_end(&amp;mut buffer).await?;

    let mut out = File::create(dst).await?;
    out.write_all(&amp;buffer).await?;
    out.sync_all().await?;
    Ok(())
}

async fn process_files(files: Vec&lt;(&amp;str, &amp;str)&gt;) {
    for (src, dst) in files {
        tokio::select! {
            result = copy_file(src, dst) =&gt; {
                println!(&quot;Copied: {:?}&quot;, result);
            }
            _ = tokio::time::sleep(Duration::from_secs(5)) =&gt; {
                println!(&quot;Timeout on {} -&gt; {}&quot;, src, dst);
            }
        }
    }
}
</pre></div>

<ol class="enumerate">
<li> Identify the cancellation safety issue. What can go wrong if a timeout occurs?
</li><li> Fix the bug by making <code class="code">copy_file</code> cancellation-safe. Ensure that
if the operation is cancelled, no partial file is left on disk.
</li><li> Explain why <code class="code">tokio::fs::File</code> doesn&#8217;t implement <code class="code">Drop</code> to
delete the file on cancellation. What would be the downsides?
</li></ol>

<p><em class="emph">Hint</em>: Consider using a temporary file and atomic rename, or
checking for partial writes and cleaning up explicitly.
</p></blockquote>

</div>
</div>
<div class="footnotes-segment">
<hr />
<h4 class="footnotes-heading">Footnotes</h4>

<h5 class="footnote-body-heading"><a id="FOOT153" href="#DOCF153">(153)</a></h5>
<p>Most real
processors actually execute a few operations at a time, following a strategy
called <a class="index-entry-id" id="index-pipelining"></a>
<em class="dfn">pipelining</em>.  Although this technique greatly improves the
effective utilization of the hardware, it is used only to speed up the
execution of a sequential instruction stream, while retaining the behavior of
the sequential program.</p>
<h5 class="footnote-body-heading"><a id="FOOT154" href="#DOCF154">(154)</a></h5>
<p>To quote some graffiti seen on a Cambridge building wall:
&#8220;Time is a device that was invented to keep everything from happening at
once.&#8221;</p>
<h5 class="footnote-body-heading"><a id="FOOT155" href="#DOCF155">(155)</a></h5>
<p>An even worse failure for this system could occur if
the two <code class="code">set!</code> operations attempt to change the balance simultaneously, in
which case the actual data appearing in memory might end up being a random
combination of the information being written by the two processes.  Most
computers have interlocks on the primitive memory-write operations, which
protect against such simultaneous access.  Even this seemingly simple kind of
protection, however, raises implementation challenges in the design of
multiprocessing computers, where elaborate <a class="index-entry-id" id="index-cache_002dcoherence"></a>
<em class="dfn">cache-coherence</em> protocols
are required to ensure that the various processors will maintain a consistent
view of memory contents, despite the fact that data may be replicated
(&#8220;cached&#8221;) among the different processors to increase the speed of memory
access.</p>
<h5 class="footnote-body-heading"><a id="FOOT156" href="#DOCF156">(156)</a></h5>
<p>The
factorial program in <a class="ref" href="3_002e1.xhtml#g_t3_002e1_002e3">The Costs of Introducing Assignment</a> illustrates this for a single
sequential process.</p>
<h5 class="footnote-body-heading"><a id="FOOT157" href="#DOCF157">(157)</a></h5>
<p>The
columns show the contents of Peter&#8217;s wallet, the joint account (in Bank1),
Paul&#8217;s wallet, and Paul&#8217;s private account (in Bank2), before and after each
withdrawal (W) and deposit (D).  Peter withdraws $10 from Bank1; Paul deposits
$5 in Bank2, then withdraws $25 from Bank1.</p>
<h5 class="footnote-body-heading"><a id="FOOT158" href="#DOCF158">(158)</a></h5>
<a class="anchor" id="Footnote-167"></a><p>A more formal
way to express this idea is to say that concurrent programs are inherently
<a class="index-entry-id" id="index-nondeterministic"></a>
<em class="dfn">nondeterministic</em>. That is, they are described not by single-valued
functions, but by functions whose results are sets of possible values.  In
<a class="ref" href="4_002e3.xhtml#g_t4_002e3">Variations on a Scheme &#8212; Nondeterministic Computing</a> we will study a language for expressing nondeterministic
computations.</p>
<h5 class="footnote-body-heading"><a id="FOOT159" href="#DOCF159">(159)</a></h5>
<p><code class="code">Parallel-execute</code> is not part of standard Scheme,
but it can be implemented in <abbr class="abbr">MIT</abbr> Scheme.  In our implementation, the
new concurrent processes also run concurrently with the original Scheme
process.  Also, in our implementation, the value returned by
<code class="code">parallel-execute</code> is a special control object that can be used to halt
the newly created processes.</p>
<h5 class="footnote-body-heading"><a id="FOOT160" href="#DOCF160">(160)</a></h5>
<p>We have simplified <code class="code">exchange</code> by exploiting the fact
that our <code class="code">deposit</code> message accepts negative amounts.  (This is a serious
bug in our banking system!)</p>
<h5 class="footnote-body-heading"><a id="FOOT161" href="#DOCF161">(161)</a></h5>
<p>If the account
balances start out as $10, $20, and $30, then after any number of concurrent
exchanges, the balances should still be $10, $20, and $30 in some order.
Serializing the deposits to individual accounts is not sufficient to guarantee
this.  See <a class="ref" href="#Exercise-3_002e43">Exercise 3.43</a>.</p>
<h5 class="footnote-body-heading"><a id="FOOT162" href="#DOCF162">(162)</a></h5>
<p><a class="ref" href="#Exercise-3_002e45">Exercise 3.45</a> investigates why deposits and withdrawals
are no longer automatically serialized by the account.</p>
<h5 class="footnote-body-heading"><a id="FOOT163" href="#DOCF163">(163)</a></h5>
<p>The
term &#8220;mutex&#8221; is an abbreviation for <a class="index-entry-id" id="index-mutual-exclusion"></a>
<em class="dfn">mutual exclusion</em>.  The general
problem of arranging a mechanism that permits concurrent processes to safely
share resources is called the mutual exclusion problem.  Our mutex is a simple
variant of the <a class="index-entry-id" id="index-semaphore"></a>
<em class="dfn">semaphore</em> mechanism (see <a class="ref" href="#Exercise-3_002e47">Exercise 3.47</a>), which
was introduced in the &#8220;THE&#8221; Multiprogramming System developed at the
Technological University of Eindhoven and named for the university&#8217;s initials
in Dutch (<a class="ref" href="References.xhtml#Dijkstra-1968a">Dijkstra 1968a</a>).  The acquire and release operations were originally
called P and V, from the Dutch words <em class="emph">passeren</em> (to pass) and
<em class="emph">vrijgeven</em> (to release), in reference to the semaphores used on railroad
systems.  Dijkstra&#8217;s classic exposition (<a class="ref" href="References.xhtml#g_t1968b">1968b</a>) was one of the first to clearly
present the issues of concurrency control, and showed how to use semaphores to
handle a variety of concurrency problems.</p>
<h5 class="footnote-body-heading"><a id="FOOT164" href="#DOCF164">(164)</a></h5>
<p>In most time-shared operating systems, processes that
are blocked by a mutex do not waste time &#8220;busy-waiting&#8221; as above.  Instead,
the system schedules another process to run while the first is waiting, and the
blocked process is awakened when the mutex becomes available.</p>
<h5 class="footnote-body-heading"><a id="FOOT165" href="#DOCF165">(165)</a></h5>
<p>In <abbr class="abbr">MIT</abbr> Scheme for a single processor, which uses a
time-slicing model, <code class="code">test-and-set!</code> can be implemented as follows:
</p>
<div class="example">
<pre class="example-preformatted">use std::sync::atomic::{AtomicBool, Ordering};

// ATOMIC version using hardware support
fn test_and_set_atomic(cell: &amp;AtomicBool) -&gt; bool {
    // compare_exchange atomically:
    // 1. Checks if current value equals `false`
    // 2. If so, sets to `true` and returns Ok(false)
    // 3. If not, returns Err(true)
    cell.compare_exchange(
        false,            // expected
        true,             // new value if expected matches
        Ordering::Acquire,
        Ordering::Relaxed,
    )
    .is_err()  // true if lock was already held
}
</pre></div>

<p><code class="code">Without-interrupts</code> disables time-slicing interrupts while its procedure
argument is being executed.</p>
<h5 class="footnote-body-heading"><a id="FOOT166" href="#DOCF166">(166)</a></h5>
<p>There are many variants of such instructions&#8212;including
test-and-set, test-and-clear, swap, compare-and-exchange, load-reserve, and
store-conditional&#8212;whose design must be carefully matched to the machine&#8217;s
processor-memory interface.  One issue that arises here is to determine what
happens if two processes attempt to acquire the same resource at exactly the
same time by using such an instruction.  This requires some mechanism for
making a decision about which process gets control.  Such a mechanism is called
an <a class="index-entry-id" id="index-arbiter"></a>
<em class="dfn">arbiter</em>.  Arbiters usually boil down to some sort of hardware
device.  Unfortunately, it is possible to prove that one cannot physically
construct a fair arbiter that works 100% of the time unless one allows the
arbiter an arbitrarily long time to make its decision.  The fundamental
phenomenon here was originally observed by the fourteenth-century French
philosopher Jean Buridan in his commentary on Aristotle&#8217;s <i class="i">De caelo</i>.
Buridan argued that a perfectly rational dog placed between two equally
attractive sources of food will starve to death, because it is incapable of
deciding which to go to first.</p>
<h5 class="footnote-body-heading"><a id="FOOT167" href="#DOCF167">(167)</a></h5>
<p>The general technique for avoiding
deadlock by numbering the shared resources and acquiring them in order is due
to <a class="ref" href="References.xhtml#Havender-_00281968_0029">Havender (1968)</a>.  Situations where deadlock cannot be avoided require
<a class="index-entry-id" id="index-deadlock_002drecovery"></a>
<em class="dfn">deadlock-recovery</em> methods, which entail having processes &#8220;back out&#8221;
of the deadlocked state and try again.  Deadlock-recovery mechanisms are widely
used in database management systems, a topic that is treated in detail in
<a class="ref" href="References.xhtml#Gray-and-Reuter-1993">Gray and Reuter 1993</a>.</p>
<h5 class="footnote-body-heading"><a id="FOOT168" href="#DOCF168">(168)</a></h5>
<p>One such alternative to serialization is called
<a class="index-entry-id" id="index-barrier-synchronization"></a>
<em class="dfn">barrier synchronization</em>.  The programmer permits concurrent processes
to execute as they please, but establishes certain synchronization points
(&#8220;barriers&#8221;) through which no process can proceed until all the processes
have reached the barrier.  Modern processors provide machine instructions that
permit programmers to establish synchronization points at places where
consistency is required.  The <abbr class="abbr">PowerPC</abbr>, for example, includes for
this purpose two instructions called <abbr class="abbr">SYNC</abbr> and <abbr class="abbr">EIEIO</abbr>
(Enforced In-order Execution of Input/Output).</p>
<h5 class="footnote-body-heading"><a id="FOOT169" href="#DOCF169">(169)</a></h5>
<p>This may seem like a strange point of view, but there are
systems that work this way.  International charges to credit-card accounts, for
example, are normally cleared on a per-country basis, and the charges made in
different countries are periodically reconciled.  Thus the account balance may
be different in different countries.</p>
<h5 class="footnote-body-heading"><a id="FOOT170" href="#DOCF170">(170)</a></h5>
<p>For
distributed systems, this perspective was pursued by <a class="ref" href="References.xhtml#Lamport-_00281978_0029">Lamport (1978)</a>, who showed
how to use communication to establish &#8220;global clocks&#8221; that can be used to
establish orderings on events in distributed systems.</p>
</div>
<hr />
<div class="nav-panel">
<p>
Next: <a href="3_002e5.xhtml#g_t3_002e5" accesskey="n" rel="next">Streams</a>, Previous: <a href="3_002e3.xhtml#g_t3_002e3" accesskey="p" rel="prev">Modeling with Mutable Data</a>, Up: <a href="Chapter-3.xhtml" accesskey="u" rel="up">Modularity, Objects, and State</a> &#160; [<a href="index.xhtml#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="Term-Index.xhtml" title="Index" rel="index">Index</a>]</p>
</div>



</body>
</html>
